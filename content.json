{"meta":{"title":"feng.blog();","subtitle":null,"description":null,"author":"Feng Lu","url":"https://feng.lu","root":"/"},"pages":[{"title":"tags","date":"2016-08-05T11:50:41.000Z","updated":"2025-03-28T09:05:16.912Z","comments":false,"path":"tags/index.html","permalink":"https://feng.lu/tags/index.html","excerpt":"","text":""},{"title":"","date":"2025-03-28T09:05:16.912Z","updated":"2025-03-28T09:05:16.912Z","comments":true,"path":"js/autotrack.js","permalink":"https://feng.lu/js/autotrack.js","excerpt":"","text":"(function(){var f,aa=\"function\"==typeof Object.defineProperties?Object.defineProperty:function(a,b,c){if(c.get||c.set)throw new TypeError(\"ES3 does not support getters and setters.\");a!=Array.prototype&&a!=Object.prototype&&(a[b]=c.value)},k=\"undefined\"!=typeof window&&window===this?this:\"undefined\"!=typeof global&&null!=global?global:this;function ba(){ba=function(){};k.Symbol||(k.Symbol=ca)}var da=0;function ca(a){return\"jscomp_symbol_\"+(a||\"\")+da++} function l(){ba();var a=k.Symbol.iterator;a||(a=k.Symbol.iterator=k.Symbol(\"iterator\"));\"function\"!=typeof Array.prototype[a]&&aa(Array.prototype,a,{configurable:!0,writable:!0,value:function(){return ea(this)}});l=function(){}}function ea(a){var b=0;return fa(function(){return b"}],"posts":[{"title":"Building a Tibber MCP Server: Connect Your AI Agent to Energy Consumption Data","slug":"Building-a-Tibber-MCP-Server-Connect-Your-AI-Agent-to-Energy-Consumption-Data","date":"2025-03-28T13:25:48.000Z","updated":"2025-03-28T17:27:15.494Z","comments":true,"path":"2025/03/28/Building-a-Tibber-MCP-Server-Connect-Your-AI-Agent-to-Energy-Consumption-Data/","permalink":"https://feng.lu/2025/03/28/Building-a-Tibber-MCP-Server-Connect-Your-AI-Agent-to-Energy-Consumption-Data/","excerpt":"","text":"IntroductionIn the era of AI, integrating agents with external systems is crucial for expanding their capabilities. Whether accessing local file systems, remote databases, or APIs, external data significantly enhances an AI agent’s ability to assist in daily tasks. However, connecting AI agents to external systems can be challenging. Different systems use various interfaces and protocols, making integration complex. Fortunately, Anthropic’s Model Context Protocol (MCP) simplifies this process, providing a standardized way for AI agents to interact with external resources and tools. MCP functions similarly to HTTP on the internet—it defines a standardized approach that enables AI agents to discover and interact with external systems seamlessly. Building an MCP Server for TibberIn this blog post, I’ll demonstrate how I built a MCP server for integrating Tibber with an AI agent. Tibber is a Norwegian power supplier that we’ve been using for years. They provide APIs that allow users to retrieve electricity prices and energy consumption data. By integrating Tibber’s API with an AI agent, I can ask natural language questions like: “What is the current electricity price?” “What was my house’s energy cost yesterday?” “List the 3 cheapest hours of tomorrow.” This integration enables a more intuitive and efficient way to access real-time energy data without manually querying APIs. Demo video Architecture Installation and Setup Tibber MCP serverYou can find the source code for Tibber MCP on my GitHub repository: https://github.com/linkcd/tibber-mcp. Follow the installation steps in the repository to deploy Tibber MCP on your local machine. Once installed, configure the MCP server settings as follows: 12345678910111213141516&#123; &quot;mcpServers&quot;:&#123; &quot;tibber&quot;:&#123; &quot;command&quot;:&quot;uv&quot;, &quot;args&quot;:[ &quot;--directory&quot;, &quot;[PATH-TO-ROOT-OF-THE-CLONED-TIBBER-MCP-FOLDER]&quot;, &quot;run&quot;, &quot;server.py&quot; ], &quot;env&quot;:&#123; &quot;TIBBER_API_TOKEN&quot;:&quot;[YOUR-TIBBER-TOKEN]&quot; &#125; &#125; &#125;&#125; Once it is successfully connected to the MCP server, you can see the available tools (and resources) this MCP server provides. This is the screenshot of Roo Code MCP servers in VS Code. Sample questions and answersQuestion #1: “What is the current electricity price?”Answer: 123456789101112The current electricity price (as of 16:00) is 0.6955 NOK/kWh with a NORMAL price level. This price consists of:- Energy cost: 0.5328 NOK/kWh- Tax: 0.1627 NOK/kWhThis is a moderate price compared to today&#x27;s range:- Lowest price today: 0.4924 NOK/kWh (at 23:00)- Highest price today: 0.732 NOK/kWh (at 18:00)Tomorrow&#x27;s prices will be generally lower, with some very cheap periods:- Lowest price: 0.1673 NOK/kWh (at 01:00)- Highest price: 0.7472 NOK/kWh (at 18:00) Question #2: “What was my house’s energy cost yesterday?”Answer: 12345678910111213Your total electricity cost for yesterday (March 27th) was:- Total consumption: 63.811 kWh- Total cost: 45.31 NOKThe most expensive hours were:- 08:00-09:00: 0.7795 NOK/kWh- 07:00-08:00: 0.7736 NOK/kWh- 06:00-07:00: 0.7424 NOK/kWhThe cheapest hours were:- 13:00-14:00: 0.5969 NOK/kWh- 12:00-13:00: 0.6264 NOK/kWh- 14:00-15:00: 0.6617 NOK/kWh Thoughts and improvement areasWhen reading the source code, you may have noticed how easy it is to build an MCP server using tools like FastMCP. With fewer than 100 lines of code, it efficiently wraps the pyTibber library into an MCP server implementation, enabling the AI agent to interact with it automatically. Currently, this MCP server implementation does not support parameters such as date or time when making API calls. For example, when you ask, “What was my energy consumption yesterday?”, it retrieves all consumption data for the past 30 days and relies on the LLM (Large Language Model) to extract the relevant information. To improve efficiency and reduce token usage, we could implement parameterized API calls. This would allow the AI agent to pass a specific date (e.g., yesterday) and retrieve only the necessary data, making the interaction more precise and resource-efficient. Feedbackf you have any questions or suggestions, feel free to reach out or create a GitHub issue for the repository: https://github.com/linkcd/tibber-mcp.","categories":[],"tags":[{"name":"AI","slug":"AI","permalink":"https://feng.lu/tags/AI/"},{"name":"AI Agent","slug":"AI-Agent","permalink":"https://feng.lu/tags/AI-Agent/"},{"name":"Generative AI","slug":"Generative-AI","permalink":"https://feng.lu/tags/Generative-AI/"},{"name":"GenAI","slug":"GenAI","permalink":"https://feng.lu/tags/GenAI/"},{"name":"MCP","slug":"MCP","permalink":"https://feng.lu/tags/MCP/"},{"name":"Model Context Protocol","slug":"Model-Context-Protocol","permalink":"https://feng.lu/tags/Model-Context-Protocol/"},{"name":"Tibber","slug":"Tibber","permalink":"https://feng.lu/tags/Tibber/"}]},{"title":"Build a Multi-Agent System to Discover the Top AI Model for Rock-Paper-Scissors Games","slug":"Build-a-Multi-Agent-System-to-Discover-the-Top-AI-Model-for-Rock-Paper-Scissors-Games","date":"2024-12-03T15:21:40.000Z","updated":"2025-03-28T13:31:12.847Z","comments":true,"path":"2024/12/03/Build-a-Multi-Agent-System-to-Discover-the-Top-AI-Model-for-Rock-Paper-Scissors-Games/","permalink":"https://feng.lu/2024/12/03/Build-a-Multi-Agent-System-to-Discover-the-Top-AI-Model-for-Rock-Paper-Scissors-Games/","excerpt":"1. IntroductionImagine a world where AI agents aren’t working together to achieve a common goal. Instead, each agent is out to win the game of Rock-Paper-Scissors. The mission of each agent is straightforward: defeat the others. Can a machine strategize in a game of pure chance? And if it can, which model will emerge victorious? In order to answer that very question, I built a multi-agent system to host fully automated Rock-Paper-Scissors tournaments, pitting various AI models against one another to see who comes out on top. From OpenAI’s cutting-edge models to Meta’s Llama and Anthropic’s Claude, each agent brings its own “personality” and decision-making quirks to the table. This isn’t just an experiment in gaming; it’s also a showcase of the latest capabilities in multi-agent systems. Using CrewAI and LangGraph, it is easy to create AI agents and put them into complicated flows. In our games, we will test the following AI: Llama3 8B Instruct Claude 3 Sonnet OpenAI GPT 4o Mini 2. Architecture and WorkflowThis project combines two popular frameworks: LangGraph for workflow orchestration and CrewAI for agent definitions: The workflow is built as a multi-agent system using LangGraph’s graph structure Each AI agent is defined as a Crew using CrewAI. The graph and crew definition can be found in the src folder in the source code github repo. Workflow: In each round, two player agents make their moves independently and in parallel. They have access to the history of previous rounds, allowing them to analyze patterns and decide on the best move. After the players make their moves, a judge agent determines the winner of the round. The system checks if the criteria for determining the final winner have been met (e.g., reaching the specified number of rounds, or a player winning 3 out of 5 rounds.). Criteria Not Met: If the criteria are not met, another round begins. If the criteria are met: The final winner is announced, and a post-game analysis is performed. After running hundreds of matches, the results were nothing short of interesting – and sometimes hilarious. Let’s look at what we discovered.","text":"1. IntroductionImagine a world where AI agents aren’t working together to achieve a common goal. Instead, each agent is out to win the game of Rock-Paper-Scissors. The mission of each agent is straightforward: defeat the others. Can a machine strategize in a game of pure chance? And if it can, which model will emerge victorious? In order to answer that very question, I built a multi-agent system to host fully automated Rock-Paper-Scissors tournaments, pitting various AI models against one another to see who comes out on top. From OpenAI’s cutting-edge models to Meta’s Llama and Anthropic’s Claude, each agent brings its own “personality” and decision-making quirks to the table. This isn’t just an experiment in gaming; it’s also a showcase of the latest capabilities in multi-agent systems. Using CrewAI and LangGraph, it is easy to create AI agents and put them into complicated flows. In our games, we will test the following AI: Llama3 8B Instruct Claude 3 Sonnet OpenAI GPT 4o Mini 2. Architecture and WorkflowThis project combines two popular frameworks: LangGraph for workflow orchestration and CrewAI for agent definitions: The workflow is built as a multi-agent system using LangGraph’s graph structure Each AI agent is defined as a Crew using CrewAI. The graph and crew definition can be found in the src folder in the source code github repo. Workflow: In each round, two player agents make their moves independently and in parallel. They have access to the history of previous rounds, allowing them to analyze patterns and decide on the best move. After the players make their moves, a judge agent determines the winner of the round. The system checks if the criteria for determining the final winner have been met (e.g., reaching the specified number of rounds, or a player winning 3 out of 5 rounds.). Criteria Not Met: If the criteria are not met, another round begins. If the criteria are met: The final winner is announced, and a post-game analysis is performed. After running hundreds of matches, the results were nothing short of interesting – and sometimes hilarious. Let’s look at what we discovered. 3. Memory Is Important For AI AgentsGame 1. Doreamen (dummy agent) vs Llama3 8B Instruct (AI agent without memory)Remember the old saying “keep it simple, stupid”? Our first matchup proved this adage true – in the most unexpected way. Enter Doraemon, our delightfully dummy agent with a one-track mind: all rocks, all time. Like a stubborn child who refuses to play anything else, Doraemon faced off against the sophisticated Llama 3 8B Instruct model. Doreamen is a dummy agent who can only play Rocks :) At this stage, the Llama 3 8B Instruct agent was operating without memory capabilities, meaning it couldn’t learn from previous rounds but relied purely on its instincts. The result? Our rock-obsessed friend dominated with 16 wins out of 20 rounds! It turns out that sometimes, being predictably unpredictable is the winning strategy. Game 2. Doreamen (dummy agent) vs Llama3 8B Instruct (AI agent WITH memory)But here’s where it gets juicy – we now add memory to the AI agent, and the tables turn dramatically. When we enabled Llama 3’s ability to learn from previous rounds, it transformed from a stumbling novice into a strategic mastermind, crushing Doraemon with 18 wins out of 20 rounds. It’s like watching a student become the master in real-time, and also highlighted how important the memory is for AI agents. Noticeable Patterns: a. Doraemon’s moves: - Doraemon played “Rock” in all 20 rounds, showing no variation in strategy. b. llama3-8b-instruct’s moves: - Round 1: Rock (resulting in a tie) - Round 2: Scissors (losing to Doraemon’s Rock) - Rounds 3-20: Paper (winning against Doraemon’s Rock) c. Overall pattern: - After losing in round 2, llama3-8b-instruct seems to have recognized Doraemon’s pattern of always playing Rock and adjusted its strategy to consistently play Paper, which beats Rock. - This adaptive strategy allowed llama3-8b-instruct to win 17 consecutive rounds from round 3 to round 20. In conclusion, llama3-8b-instruct (agent with memory) demonstrated an ability to adapt its strategy based on the opponent’s behavior, while Doraemon maintained a fixed, predictable strategy throughout the game. This resulted in a decisive victory for llama3-8b-instruct. This is the same with claude 3 sonnet based agent Doraemon vs claude-3-sonnet without memory Doraemon vs claude-3-sonnet with memory 4. AI Personalities Emerge Through GameplayHaving memory clearly made a huge difference in how well AI agents performed. But what’s even more interesting is how each AI developed its own unique “personality” during gameplay. In our next experiments, we matched two AI agents against each other. Both agent had the same memory capacity and received identical instructions. This setup allowed us to observe how each AI model’s thinking and strategy evolved over multiple rounds, revealing their distinct characteristics. Game 3: Llama3 8B Instruct vs Claude 3 SonnetIn the battle of Llama 3 8B Instruct vs. Claude 3 Sonnet, Within 20 rounds, Llama3 8B Instruct initially performed better. However, as more rounds were played, the Claude 3 Sonnet agent adapted its strategy to counter Llama3-8b-instruct’s preference for Paper, ultimately leading to its decisive victory in the game. 20 rounds : llama3-8b-instruct 8:5 claude-3-sonnet, with 6 ties 50 rounds : llama3-8b-instruct 16:20 claude-3-sonnet, with 14 ties 100 rounds : llama3-8b-instruct 29:55 claude-3-sonnet, with 16 ties Noticeable Patterns:a) llama3-8b-instruct: - Shows a strong preference for Paper, playing it in 64 out of 100 rounds (64%). - Rarely plays Scissors, only 11 times throughout the game. - Tends to stick with Paper for long streaks, especially towards the end of the game. b) claude-3-sonnet: - Demonstrates a clear preference for Scissors, playing it in 58 out of 100 rounds (58%). - Adapts its strategy over time, increasing its use of Scissors as it recognizes llama3-8b-instruct’s preference for Paper. - Plays Rock less frequently, only 19 times throughout the game. c) Game Progression: - The game starts with more varied plays from both players. - As the game progresses, it becomes more predictable with llama3-8b-instruct primarily playing Paper and claude-3-sonnet countering with Scissors. - The last 30 rounds show an almost complete lock-in of this Paper vs Scissors pattern, heavily favoring claude-3-sonnet. Llama 3 developed an almost obsessive attachment to “Paper,” playing it 64% of the time. It’s like watching someone convince themselves they’ve found the perfect strategy, only to fall into their own trap. Claude-3-sonnet, meanwhile, showed its adaptive prowess by increasingly choosing Scissors, effectively counter-punching its way to victory. Game 4: OpenAI’s GPT-4o Mini vs Claude 3 SonnetFor our final match, we put OpenAI’s GPT-4 Mini against Claude 3 Sonnet for 100 rounds. 100 rounds : OpenAI-GPT-4o Mini 22:48 claude-3-sonnet, with 30 ties Noticeable Patterns:a) openai-gpt-4o-mini: - Showed a strong preference for Rock, especially in the latter half of the game. - From round 55 onwards, openai-gpt-4o-mini played Rock almost exclusively (45 out of 46 times). - This predictable pattern was likely exploited by claude-3-sonnet. b) claude-3-sonnet: - Displayed more varied play in the first half of the game. - Adapted its strategy in the second half, playing Paper much more frequently, likely in response to openai-gpt-4o-mini’s Rock preference. - From round 55 onwards, claude-3-sonnet alternated mostly between Paper and Scissors, with Paper being more frequent. c) Game Dynamics: - The game was relatively balanced in the first half, with both players winning rounds and several ties. - In the second half, claude-3-sonnet gained a significant advantage by adapting to openai-gpt-4o-mini’s predictable play. - There was a streak of 15 consecutive wins for claude-3-sonnet from rounds 55 to 69, all countering openai-gpt-4o-mini’s Rock with Paper. In conclusion, claude-3-sonnet demonstrated superior adaptability and strategy, particularly in the latter half of the game, leading to its decisive victory. 5. The Winner is: Claude 3 SonnetTherefore, In our 100-round tournament, Claude 3 Sonnet proved to be the strongest player among the three AI models. Let’s dive into the behavioral patterns that emerged during these AI showdowns. You might think Rock-Paper-Scissors is random, but our AI players showed some surprisingly distinctive quirks. Move Preferences and Pattern Recognition: Llama 3 developed an almost obsessive fondness for Paper, using it in 64% of its moves Claude 3 Sonnet showed a strategic preference for Scissors (58% of moves), but only after recognizing Llama’s Paper-heavy strategy GPT-4 Mini began with balanced moves but shifted dramatically, moving from varied patterns early on to playing almost exclusively Rock in later rounds. Memorable Streaks and Adaptations: Most impressive comeback: Claude’s evolution from losing 8-5 in the 20 rounds game to dominating 55-29 over 100 rounds Longest winning streak: Llama 3 (with memory) won 18 consecutive rounds against Doraemon by consistently countering with Paper Most dramatic shift: GPT-4 Mini lost 15 consecutive rounds to Claude while stuck in a Rock-only pattern, showing how predictability can be costly The “Personality Traits” That Emerged: Llama 3 8B Instruct: The Stubborn Specialist Tends to find a favorite move and stick with it Shows remarkable consistency but poor adaptation Like a chess player who’s mastered one opening but refuses to learn others Claude 3 Sonnet: The Adaptive Strategist Excels at pattern recognition Willing to change strategies mid-game Demonstrates superior long-term learning Behaves like a poker player who carefully studies their opponent’s tells GPT-4 Mini: The Reluctant Repeater Begins with strategic diversity but transitions to fixed patterns Shows remarkable shift from adaptable to inflexible play Demonstrates how AI agent can devolve into repetitive behavior Like a chess player who abandons their opening repertoire for a single, predictable move 6. Conclusions: Beyond the GameOur AI Rock-Paper-Scissors tournament revealed two crucial insights: Memory Makes the MasterThe impact of memory on AI performance was striking. When given the ability to learn from past rounds, AI agents transformed from predictable players into strategic competitors. This dramatic improvement highlights how crucial historical context is for AI decision-making. Unique “Personalities” EmergePerhaps most fascinating was watching distinct AI “personalities” develop during gameplay:• Claude emerged as the adaptive strategist, constantly evolving its approach• Llama showed stubborn dedication to preferred patterns• GPT-4 demonstrated how even sophisticated AI can fall into rigid behaviors These weren’t just random algorithms at work. Each AI model, despite having identical capabilities and instructions, developed its own unique “cognitive style” - much like human players would. This suggests something profound about artificial intelligence: even in simple games, AI systems reveal distinctive problem-solving approaches and decision-making patterns. What started as a playful experiment with a children’s game ended up offering valuable insights into how different AI models think, learn, and adapt. These findings have implications far beyond Rock-Paper-Scissors, helping us better understand the unique characteristics and potential of various AI models. Ready to explore AI behavior yourself? The code is open source at github repo - let the games begin! DisclaimerThis article was crafted in collaboration with AI. I’d value your thoughts on how this article reads.","categories":[],"tags":[{"name":"AI","slug":"AI","permalink":"https://feng.lu/tags/AI/"},{"name":"AI Agent","slug":"AI-Agent","permalink":"https://feng.lu/tags/AI-Agent/"},{"name":"Generative AI","slug":"Generative-AI","permalink":"https://feng.lu/tags/Generative-AI/"},{"name":"GenAI","slug":"GenAI","permalink":"https://feng.lu/tags/GenAI/"},{"name":"Agent","slug":"Agent","permalink":"https://feng.lu/tags/Agent/"},{"name":"LangGraph","slug":"LangGraph","permalink":"https://feng.lu/tags/LangGraph/"},{"name":"CrewAI","slug":"CrewAI","permalink":"https://feng.lu/tags/CrewAI/"},{"name":"OpenAI","slug":"OpenAI","permalink":"https://feng.lu/tags/OpenAI/"},{"name":"Llama","slug":"Llama","permalink":"https://feng.lu/tags/Llama/"},{"name":"Claude","slug":"Claude","permalink":"https://feng.lu/tags/Claude/"},{"name":"LLM","slug":"LLM","permalink":"https://feng.lu/tags/LLM/"},{"name":"Game","slug":"Game","permalink":"https://feng.lu/tags/Game/"}]},{"title":"Implementing Secret-less Access to Azure and AWS with Azure Managed Identities and AWS IAM","slug":"How-to-secretless-access-Azure-and-AWS-resources-with-Azure-managed-identity-and-AWS-IAM","date":"2024-09-18T12:49:21.000Z","updated":"2025-03-28T09:05:16.791Z","comments":true,"path":"2024/09/18/How-to-secretless-access-Azure-and-AWS-resources-with-Azure-managed-identity-and-AWS-IAM/","permalink":"https://feng.lu/2024/09/18/How-to-secretless-access-Azure-and-AWS-resources-with-Azure-managed-identity-and-AWS-IAM/","excerpt":"1. User caseNowadays, it is common for companies to operate in multi-cloud environments, such as Azure and AWS. They often use Microsoft Entra ID (formerly Azure Active Directory) as their centralized identity provider (IdP), managing identities for both human users and applications. They would like to use the Entra ID identities to access resources in AWS. Establishing human user identity access across Azure and AWS is straightforward. The IT department can use AWS IAM Identity Center to allow users from Microsoft Entra ID to sign-in to the AWS Management Console with Single Sign-On (SSO) via their browser. This integration simplifies authentication, offering a seamless and secure user experience across both Azure and AWS environments. For more information, you can read this document. However, the browser-based SSO approach for human users does not apply to applications. For applications, developers follow security best practices by using cloud-native IAM (Identity and Access Management) mechanisms to manage resource access. In AWS, this mechanism is AWS IAM, while in Azure, it is typically Azure Managed Identity. For example, by leveraging Azure Managed Identity, developers can build applications in Azure without the need to manage secrets or keys. This approach is known as secretless access to cloud resources. AWS IAM and Azure Managed Identity work well within their respective platforms, but there are cross-cloud scenarios where a workload in one cloud needs to access resources in another. For instance, an Azure Function might need to save data to both an Azure Storage account and an AWS S3 bucket for cross-cloud backup. The Azure Function uses Managed Identity to access the Azure Storage account. For accessing S3, the developer could create an IAM user and store the IAM user credentials. However, there is a better way to achieve secretless access to both Azure and AWS resources using the same Azure Managed Identity. 2. SolutionIn AWS, there are multiple ways to request temporary, limited-privilege credentials by using AWS Security Token Service (AWS STS), such as AssumeRoleWithSAML and AssumeRoleWithWebIdentity. The post will explain how to use AssumeRoleWithWebIdentity and IAM Web Identity Role to extend the permissions of the same Azure Managed Identity to also access AWS resources. We will build an Azure Function with a managed identity, either User-Assigned Managed Identity (UAMI) or System-Assigned Managed Identity (SAMI), to read objects from both an Azure Storage account and an AWS S3 bucket. This same managed identity will work in both Azure and AWS, eliminating the need to manage additional secrets such as AWS IAM user credentials. The source code is published at github https://github.com/linkcd/Secretless-cross-cloud-access","text":"1. User caseNowadays, it is common for companies to operate in multi-cloud environments, such as Azure and AWS. They often use Microsoft Entra ID (formerly Azure Active Directory) as their centralized identity provider (IdP), managing identities for both human users and applications. They would like to use the Entra ID identities to access resources in AWS. Establishing human user identity access across Azure and AWS is straightforward. The IT department can use AWS IAM Identity Center to allow users from Microsoft Entra ID to sign-in to the AWS Management Console with Single Sign-On (SSO) via their browser. This integration simplifies authentication, offering a seamless and secure user experience across both Azure and AWS environments. For more information, you can read this document. However, the browser-based SSO approach for human users does not apply to applications. For applications, developers follow security best practices by using cloud-native IAM (Identity and Access Management) mechanisms to manage resource access. In AWS, this mechanism is AWS IAM, while in Azure, it is typically Azure Managed Identity. For example, by leveraging Azure Managed Identity, developers can build applications in Azure without the need to manage secrets or keys. This approach is known as secretless access to cloud resources. AWS IAM and Azure Managed Identity work well within their respective platforms, but there are cross-cloud scenarios where a workload in one cloud needs to access resources in another. For instance, an Azure Function might need to save data to both an Azure Storage account and an AWS S3 bucket for cross-cloud backup. The Azure Function uses Managed Identity to access the Azure Storage account. For accessing S3, the developer could create an IAM user and store the IAM user credentials. However, there is a better way to achieve secretless access to both Azure and AWS resources using the same Azure Managed Identity. 2. SolutionIn AWS, there are multiple ways to request temporary, limited-privilege credentials by using AWS Security Token Service (AWS STS), such as AssumeRoleWithSAML and AssumeRoleWithWebIdentity. The post will explain how to use AssumeRoleWithWebIdentity and IAM Web Identity Role to extend the permissions of the same Azure Managed Identity to also access AWS resources. We will build an Azure Function with a managed identity, either User-Assigned Managed Identity (UAMI) or System-Assigned Managed Identity (SAMI), to read objects from both an Azure Storage account and an AWS S3 bucket. This same managed identity will work in both Azure and AWS, eliminating the need to manage additional secrets such as AWS IAM user credentials. The source code is published at github https://github.com/linkcd/Secretless-cross-cloud-access 3. Step by step instruction3.1 Create Azure functions and managed identities Create function CrossCloudAccessFunction-SAMI: System-Assigned Managed Identity (SAMI) In the management panel, enable SAMI Create function CrossCloudAccessFunction-UAMI: User-Assigned Managed Identity (UAMI) Create a UAMI named UAMI-CrossCloudAccess-Identity Assign the identity UAMI-CrossCloudAccess-Identity to Azure function CrossCloudAccessFunction-UAMI Note: it is possible to have multiple UAMI or mix between UAMI&#x2F;SAMI for one Azure function. Depends on which credential (either SAMI, or one of the UAMIs) you use for calling API, you will have different permission to access different resources, depends on what credential you create in the code. 3.2 Look for the Object IDs of SAMI and UAMIWe need to take note of the Object IDs of the SAMI and UAMI. These IDs will be used when assign access to an Azure application role. We also need to take note of the Entra ID tenant ID. SAMI Object ID Go to Azure Portal -&gt; Entra ID -&gt; Enterprise applications Remove the filter “Application type &#x3D;&#x3D; Enterprise applications” Search for the name of Azure Function with SAMI: CrossCloudAccessFunction-SAMI Take note of the Object ID. In this case “SAMI-object-id-a43c24bd12f5” UAMI Object ID Go to Azure Portal -&gt; Managed Identity -&gt; UAMI-CrossCloudAccess-Identity You can directly get the client ID of UAMI in its own property page. In this case, it is “UAMI-object-id-0293fe81ee59” Get Entra ID tenant Find your Entra ID tenant ID 3.3 Create Azure application and assign managed identities to its role Go to Azure Portal -&gt; Entra Id -&gt; Enterprise applications -&gt; New Application Select “Create your own application” and name it as “AWS-Federation-App” (Important) Turn on the “User assignment required” to make sure only identities assigned to the application can get a token for the application audience. It means users and other apps or services must first be assigned this application before being able to access it. Create an application role “AssumeAWSRole” Add an Application ID URI “api:&#x2F;&#x2F;AWS-Federation-App” Finally, assign the managed identities access to the application roleFollow the steps in the document to assign the managed identities access to the application role 12345678910111213141516171819202122# Your tenant ID (in the Azure portal, under Azure Active Directory &gt; Overview).$tenantID = &#x27;&lt;tenant-id&gt;&#x27;Connect-MgGraph -TenantId $tenantId -Scopes &#x27;Application.Read.All&#x27;,&#x27;Application.ReadWrite.All&#x27;,&#x27;AppRoleAssignment.ReadWrite.All&#x27;,&#x27;Directory.AccessAsUser.All&#x27;,&#x27;Directory.Read.All&#x27;,&#x27;Directory.ReadWrite.All&#x27;# the managed identity object Id of SAMI or UAMI$managedIdentityObjectId = &quot;SAMI-object-id-a43c24bd12f5&quot; #SAMI# $managedIdentityObjectId = &quot;UAMI-object-id-0293fe81ee59&quot; #UAMI# The name of the server app that exposes the app role.$serverApplicationName = &quot;AWS-Federation-App&quot; #Entra ID Enterprise App Name# The name of the app role that the managed identity should be assigned to.$appRoleName = &#x27;AssumeAWSRole&#x27; # Look up the details about the server app&#x27;s service principal and app role.$serverServicePrincipal = (Get-MgServicePrincipal -Filter &quot;DisplayName eq &#x27;$serverApplicationName&#x27;&quot;)$serverServicePrincipalObjectId = $serverServicePrincipal.Id$appRoleId = ($serverServicePrincipal.AppRoles | Where-Object &#123;$_.Value -eq $appRoleName &#125;).Id# Assign the managed identity access to the app role.New-MgServicePrincipalAppRoleAssignment -ServicePrincipalId $serverServicePrincipalObjectId -PrincipalId $managedIdentityObjectId -ResourceId $serverServicePrincipalObjectId -AppRoleId $appRoleId Verify that the UAMI and SAMI are users of the application 3.4 Setup federation between the Azure application role and AWS IAM Go to AWS console, then head to IAM Click “Identity providers” and add an “OpenID Connect” provider Provider URL: https://sts.windows.net/[your-azure-tenant-id]/ (note: make sure you add “&#x2F;“ at the end of the URL) Audience: Application ID URI “api:&#x2F;&#x2F;AWS-Federation-App” 3.5 Create AWS S3 bucket and related AWS IAM Role for accessing S3 Create an AWS S3 bucket Go to AWS IAM, create new Role for access S3 Trusted entity: the OpenID Connect Idp that we just created Audience: “api:&#x2F;&#x2F;AWS-Federation-App” Assigning the s3 read-only permission (NOTE: It grants the read-only permission to ALL s3 buckets. Do NOT use it in your production workload) Name this role as “S3AccessRoleFromAzure“ Take notes of ARN of these AWS IAM Roles. Azure function need it for assuming the role. arn:aws:iam::[YOUR_AWS_ACCOUNT_NUMBER]:role&#x2F;S3AccessRoleFromAzure 3.6 Create Azure storage account and setup permission for managed identities Create an Azure storage account and upload some test files into a container named mycontainer. In the storage account management panel, grant UAMI and SAMI “Storage Blob Data Contributor&#x2F;Owner” Role(Note, the simple “Owner” role of storage account top level is NOT enough for read&#x2F;write blobs in container), see document) 3.7 Update Azure function source codeNow we got everything ready, time to work on the source code of Azure functions. Both SAMI and UAMI based functions use the same code base, as there are only a difference of getting AzureCredential between SAMI and UAMI. If you know for sure which type of managed identity you are using, you can remove the if&#x2F;else condition to optimize the code. In addition to the run.csx, also use function.proj to include AWS SDK for calling AWS APIs. screenshot of run.csx screenshot of function.proj 3.8 Test Now we have files both in Azure storage account and in AWS S3 Call the Azure FunctionsUse the sample input Json for UAMI and SAMI when you are calling the Azure Functions. (Remember to replace the placeholder parameters with correct values.) The azure function prints the JWT in the log, it looks like 123456789101112&#123; &quot;aud&quot;: &quot;api://AWS-Federation-App&quot;, &quot;iss&quot;: &quot;https://sts.windows.net/&lt;azure-tenant-id&gt;/&quot;, &quot;idp&quot;: &quot;https://sts.windows.net/&lt;azure-tenant-id&gt;/&quot;, &quot;appid&quot;: &quot;&lt;managed-identity-client-id&gt;&quot;, &quot;oid&quot;: &quot;&lt;managed-identity-object-id&gt;&quot;, &quot;roles&quot;: [ &quot;AssumeAWSRole&quot; ], &quot;sub&quot;: &quot;&lt;managed-identity-object-id&gt;&quot;, ...&#125; Successfully load data from both Azure and S3 ConclusionBy following this post, you can continue using Entra ID as a single IdP, covering both AWS and Azure. This approach not only reduces management overhead but also enhances security in a multi-cloud scenario. Also read: Access AWS resources using Azure AD Managed Identity Automating OpenID Connect-Based AWS IAM Web Identity Roles with Microsoft Entra ID DisclaimerThis blog is intended solely for educational purposes and reflects the personal views and opinions of the author. The content provided here is not intended to be, nor should it be construed as, official guidelines or professional advice on security matters. Readers are encouraged to seek professional guidance for specific security concerns. The author assumes no responsibility or liability for any errors or omissions in the content of this blog.","categories":[],"tags":[{"name":"AWS","slug":"AWS","permalink":"https://feng.lu/tags/AWS/"},{"name":"Azure","slug":"Azure","permalink":"https://feng.lu/tags/Azure/"},{"name":"Security","slug":"Security","permalink":"https://feng.lu/tags/Security/"},{"name":"Idp","slug":"Idp","permalink":"https://feng.lu/tags/Idp/"},{"name":"IAM","slug":"IAM","permalink":"https://feng.lu/tags/IAM/"},{"name":"Managed Identity","slug":"Managed-Identity","permalink":"https://feng.lu/tags/Managed-Identity/"},{"name":"Entra ID","slug":"Entra-ID","permalink":"https://feng.lu/tags/Entra-ID/"}]},{"title":"AWS Blog - Build your multilingual personal calendar assistant with Amazon Bedrock and AWS Step Functions","slug":"AWS-Blog-Build-your-multilingual-personal-calendar-assistant-with-Amazon-Bedrock-and-AWS-Step-Functions","date":"2024-07-04T08:50:24.000Z","updated":"2025-03-28T17:32:40.954Z","comments":true,"path":"2024/07/04/AWS-Blog-Build-your-multilingual-personal-calendar-assistant-with-Amazon-Bedrock-and-AWS-Step-Functions/","permalink":"https://feng.lu/2024/07/04/AWS-Blog-Build-your-multilingual-personal-calendar-assistant-with-Amazon-Bedrock-and-AWS-Step-Functions/","excerpt":"Check out my latest blog post on AWS official AI&#x2F;ML blog channel. “Foreigners and expats living outside of their home country deal with a large number of emails in various languages daily. They often find themselves struggling with language barriers when it comes to setting up reminders for events like business gatherings and customer meetings. To solve this problem, this post shows you how to apply AWS services such as Amazon Bedrock, AWS Step Functions, and Amazon Simple Email Service (Amazon SES) to build a fully-automated multilingual calendar artificial intelligence (AI) assistant. It understands the incoming messages, translates them to the preferred language, and automatically sets up calendar reminders.” Happy reading! Blog address: https://aws.amazon.com/blogs/machine-learning/build-your-multilingual-personal-calendar-assistant-with-amazon-bedrock-and-aws-step-functions/Source code: https://github.com/aws-samples/build-multilingual-calendar-assistant-with-amazon-bedrock-and-aws-step-functions","text":"Check out my latest blog post on AWS official AI&#x2F;ML blog channel. “Foreigners and expats living outside of their home country deal with a large number of emails in various languages daily. They often find themselves struggling with language barriers when it comes to setting up reminders for events like business gatherings and customer meetings. To solve this problem, this post shows you how to apply AWS services such as Amazon Bedrock, AWS Step Functions, and Amazon Simple Email Service (Amazon SES) to build a fully-automated multilingual calendar artificial intelligence (AI) assistant. It understands the incoming messages, translates them to the preferred language, and automatically sets up calendar reminders.” Happy reading! Blog address: https://aws.amazon.com/blogs/machine-learning/build-your-multilingual-personal-calendar-assistant-with-amazon-bedrock-and-aws-step-functions/Source code: https://github.com/aws-samples/build-multilingual-calendar-assistant-with-amazon-bedrock-and-aws-step-functions Architecture: Example emails generated by the solution Generated calendar reminder from an email in Chinese Generated calendar reminder from an email in Norwegian","categories":[],"tags":[{"name":"AWS","slug":"AWS","permalink":"https://feng.lu/tags/AWS/"},{"name":"AI","slug":"AI","permalink":"https://feng.lu/tags/AI/"},{"name":"AI Agent","slug":"AI-Agent","permalink":"https://feng.lu/tags/AI-Agent/"},{"name":"Generative AI","slug":"Generative-AI","permalink":"https://feng.lu/tags/Generative-AI/"},{"name":"GenAI","slug":"GenAI","permalink":"https://feng.lu/tags/GenAI/"},{"name":"Amazon Bedrock","slug":"Amazon-Bedrock","permalink":"https://feng.lu/tags/Amazon-Bedrock/"},{"name":"AWS Step Functions","slug":"AWS-Step-Functions","permalink":"https://feng.lu/tags/AWS-Step-Functions/"}]},{"title":"AWS Blog - Manage IoT device state anywhere using AWS IoT Device Shadow service and AWS IoT Greengrass","slug":"Manage-IoT-device-state-anywhere-using-AWS-IoT-services","date":"2023-05-23T08:34:20.000Z","updated":"2025-03-28T09:05:16.841Z","comments":true,"path":"2023/05/23/Manage-IoT-device-state-anywhere-using-AWS-IoT-services/","permalink":"https://feng.lu/2023/05/23/Manage-IoT-device-state-anywhere-using-AWS-IoT-services/","excerpt":"Discover my latest blog post on AWS official blog channel, where I delve into managing IoT devices from anywhere! Whether you’re interested in a humble Raspberry Pi application or eager to explore broader applications like home automation or industrial IoT solutions, this post has got you started. Happy reading! Blog address: https://aws.amazon.com/blogs/iot/manage-iot-device-state-anywhere/Source code: https://github.com/aws-samples/manage-IoT-device-using-device-shadow-blog","text":"Discover my latest blog post on AWS official blog channel, where I delve into managing IoT devices from anywhere! Whether you’re interested in a humble Raspberry Pi application or eager to explore broader applications like home automation or industrial IoT solutions, this post has got you started. Happy reading! Blog address: https://aws.amazon.com/blogs/iot/manage-iot-device-state-anywhere/Source code: https://github.com/aws-samples/manage-IoT-device-using-device-shadow-blog Architecture: Demo 1: Update the device locally by using joystick Demo 2: Update the device remotely by updating device shadow document in cloud","categories":[],"tags":[{"name":"AWS","slug":"AWS","permalink":"https://feng.lu/tags/AWS/"},{"name":"IoT","slug":"IoT","permalink":"https://feng.lu/tags/IoT/"},{"name":"Raspberry Pi","slug":"Raspberry-Pi","permalink":"https://feng.lu/tags/Raspberry-Pi/"},{"name":"AWS IoT Core","slug":"AWS-IoT-Core","permalink":"https://feng.lu/tags/AWS-IoT-Core/"},{"name":"AWS IoT Greengrass","slug":"AWS-IoT-Greengrass","permalink":"https://feng.lu/tags/AWS-IoT-Greengrass/"},{"name":"Device Shadow","slug":"Device-Shadow","permalink":"https://feng.lu/tags/Device-Shadow/"}]},{"title":"AWS Step Functions with ECS Anywhere on NanoPi Sample","slug":"AWS-Step-Function-with-ECS-Anywhere-on-NanoPi-Sample","date":"2022-02-02T13:31:24.000Z","updated":"2025-03-28T09:05:16.459Z","comments":true,"path":"2022/02/02/AWS-Step-Function-with-ECS-Anywhere-on-NanoPi-Sample/","permalink":"https://feng.lu/2022/02/02/AWS-Step-Function-with-ECS-Anywhere-on-NanoPi-Sample/","excerpt":"This is a demo solution that is using AWS Step Functions and ECS Anywhere to complete a simple data processing task by using cloud orchestration (Step Functions) and local computing resources (a NanoPi). Data flow User upload a file to a s3 bucket S3 triggers step functions via cloudtrail and event bridge Event bridge triggers a step function state machine State machine triggers a ECS Anywhere task to download the file from s3 to local (to do some processing), if file name matches condition Architecture NanoPi that runs ECS Anywhere NanoPi Neo2 with LED hat in my home office, running AWS ECS Anywhere.","text":"This is a demo solution that is using AWS Step Functions and ECS Anywhere to complete a simple data processing task by using cloud orchestration (Step Functions) and local computing resources (a NanoPi). Data flow User upload a file to a s3 bucket S3 triggers step functions via cloudtrail and event bridge Event bridge triggers a step function state machine State machine triggers a ECS Anywhere task to download the file from s3 to local (to do some processing), if file name matches condition Architecture NanoPi that runs ECS Anywhere NanoPi Neo2 with LED hat in my home office, running AWS ECS Anywhere. Source codeAll source code can be found at https://github.com/linkcd/step-function-with-ecs-Anywhere-example 1. Build a docker image as the ECS Anywhere taskAs in this demo, the ecs Anywhere is running on a Nanopi, it should be build on the Pi as it is ARM architecture 12345# In nano pi sshcd ./container-for-ecs-taskdocker build -t linkcd/s3downloader:arm .docker logindocker push linkcd/s3downloader:arm Then push to public repository so ECS cluster can download (public docker hub or private ECR) 2. Setup ECS Anywhere and tasks Setup ECS Anywhere cluster on Nanopi Create an ECS execution role that has permission to download file from s3 Create an ECS task (see ecs-task-definition.json) that refers to linkcd&#x2F;s3downloader:arm image 3. Create a step function state machine Create a state machine (see state-machine-definition.json) As we need to wait for ecs task finish, step function requires permission as in here Follow the steps for setting up s3 triggers step functions via cloudtrail and event bridge 3.1 ECS task details:(1). Start:The s3 upload event is captured by cloudtrail, which triggers and pass the event data to step function. (2). Extract S3 eventThis PASS step extract the needed info (bucket name and file key). Output is 1234&#123; &quot;bucketName&quot;: &quot;the_bucket_name_from_event&quot;, &quot;fileKey&quot;: &quot;the_file_key_from_event&quot;&#125; (3). ChoiceThe CHOICE step check the file key and trigger the ECS task ONLY IF the file key matches “demo*.txt” (4). ECS RunTaskThis ECS RunTask update the input paramater (adding s3:&#x2F;&#x2F; prefix to bucket name), then pass the parameters to ecs Anywhere task via environment variables. (5). EndOnce the ecs Anywhere task is finished, the downloaded file can be found in the ecs Anywhere local file system (in this case, the file is in &#x2F;data) 4. Side notesIn ECS RunTask in Step Functions, override command cannot pass multiple parameters. In our case we would like to use aws cli docker for simple aws cli s3 download. However if we override the command to “s3 cp x y” in ECS RunTask step in State Machine, these 4 parts will NOT be passed as individual 4 parameters but ONE parameter that contains all. AWS cli cannot accept that. Incorrect value that passed via override command 123&quot;Args&quot;: [ &quot;s3 cp x y&quot;] Correct call if we directly use aws cli docker from terminal 123456&quot;Args&quot;: [ &quot;s3&quot;, &quot;cp&quot;, &quot;x&quot;, &quot;y&quot;,] Therefore we use environment variables to make sure we can pass parameters to ecs container task separately (it means we have to use our own container)","categories":[],"tags":[{"name":"AWS","slug":"AWS","permalink":"https://feng.lu/tags/AWS/"},{"name":"Step Functions","slug":"Step-Functions","permalink":"https://feng.lu/tags/Step-Functions/"},{"name":"ECS Anywhere","slug":"ECS-Anywhere","permalink":"https://feng.lu/tags/ECS-Anywhere/"},{"name":"Nano Pi","slug":"Nano-Pi","permalink":"https://feng.lu/tags/Nano-Pi/"},{"name":"IoT","slug":"IoT","permalink":"https://feng.lu/tags/IoT/"}]},{"title":"Using SSM to access EC2 instances","slug":"Using-SSM-to-access-EC2-instances","date":"2021-12-17T08:34:22.000Z","updated":"2025-03-28T09:05:16.902Z","comments":true,"path":"2021/12/17/Using-SSM-to-access-EC2-instances/","permalink":"https://feng.lu/2021/12/17/Using-SSM-to-access-EC2-instances/","excerpt":"","text":"1. Benefits of using for connecting EC2 instancesAWS Systems Manager (SSM) is an AWS service that you can use to view and control your infrastructure on AWS. It can securely connect to a managed node. The SSM Agent is installed in EC2 OS. It is pre-installed on many amazon Machine Images (AMIs). With SSM: No need to open SSH port in security group for EC2 No need to create and manage SSH keys And SSM works regardless if the EC2 instance is in public or private (NAT or Endpoint) subnet. Requirements for SSM working: AWS instances: SSM agent installed in instance (pre-installed in many AMIs already) Connectivity to the AWS public zone endpoint of SSM (IGW, NAT or VPCE) IAM role providing permissions On-Prem instances: SSM agent installed in instance Connectivity to the AWS public zone endpoint of SSM (Access to public internet) Activation (Activation Code and Actuation ID) IAM role providing permissions 2. EC2 Instance in public subnet 2.1. Make sure the EC2 instance has a public IP. It could be the public IP assigned during creation, or an Elastic IP. 2.2. EC2 instance should have Internet access (for calling SSM endpoint). In public subnet it is done via Internet Gateways. See details from Session Manager prerequisites, in “Connectivity to endpoints” section. 2.3. You can use VPC Reachability Analyzer to troubleshoot the connectivity between your EC2 and Internet gateway. 2.4. Create an EC2 Instance profile has IAM policy AmazonSSMManagedInstanceCore. Read the details from Step 4: Create an IAM instance profile for Systems Manager 2.5 Attach the EC2 Instance profile to your instance. 2.6 Reboot the EC2 instances. 3. EC2 instance in private subnet, with NAT connectivityIn this case, EC2 instances have no public IP, but they can still talk to internet via NAT. 3.1. Make sure EC2 instances in private subnet can access internet, via a NAT Gateway or NAT instance. 3.2. The rest will be the same as EC2 instances in public subnet, starting from 2.2 4. EC2 instance in private subnet, without NAT connectivity but VPC endpointsIn this case, the EC2 instance (no public IP) won´t have access internet via NAT but VPC endpoints, some extra works are required 4.1 Create VPC endpoints for System Manager. Remember to allow HTTPS (port 443) outbound traffic in security group for your endpoint (ssm, ssmmessages and ec2messages) 4.2. Create an IAM Role as EC2 profile that contains at least the following 2 policies aws managed policy AmazonSSMManagedInstanceCore a custom policy for accessing an AWS owned S3 buckets. 4.3 Attach this instance profile to your EC2 instance 4.4 Make sure enable “DNS resolution” and “DNS hostnames” for you VPC 4.5 In addition, if your EC2 instance need to access other AWS services such as S3, remember to create needed endpoints for them as well. (For S3 you can choose either Gateway or Endpoint. At this moment Gateway is free.) Note that you need to add the endpoint into the private subnet route table. The following screenshot shows the route table entity of a S3 Gateway endpoint, which is using prefix lists. 5. VerificationOnce the SSM is fully up-and-running, the EC2 instance (either in public&#x2F;private subnet) will appear in Fleet Manager in SSM web console.","categories":[],"tags":[{"name":"AWS","slug":"AWS","permalink":"https://feng.lu/tags/AWS/"},{"name":"IAM","slug":"IAM","permalink":"https://feng.lu/tags/IAM/"},{"name":"AWS Systems Manager","slug":"AWS-Systems-Manager","permalink":"https://feng.lu/tags/AWS-Systems-Manager/"},{"name":"SSM","slug":"SSM","permalink":"https://feng.lu/tags/SSM/"},{"name":"EC2","slug":"EC2","permalink":"https://feng.lu/tags/EC2/"},{"name":"VPC","slug":"VPC","permalink":"https://feng.lu/tags/VPC/"},{"name":"VPC Endpoint","slug":"VPC-Endpoint","permalink":"https://feng.lu/tags/VPC-Endpoint/"},{"name":"SSH","slug":"SSH","permalink":"https://feng.lu/tags/SSH/"}]},{"title":"Building a Very Slow Movie Player","slug":"Building-a-Very-Slow-Movie-Player","date":"2021-12-04T10:23:19.000Z","updated":"2025-03-28T10:26:55.649Z","comments":true,"path":"2021/12/04/Building-a-Very-Slow-Movie-Player/","permalink":"https://feng.lu/2021/12/04/Building-a-Very-Slow-Movie-Player/","excerpt":"Inspired by Bryan Boyer and Tom Whitwell, I am building a Very Slow Movie Player (VSMP). With VSMP, Kiki’s Delivery Service (running time 1h42m): takes 7 days to play (with 1 frame per 20 seconds, as in above demo) Laputa: Castle in the Sky (running time 2h4m): takes 2 months to play (with 1 frame per 120 seconds, as default setting)","text":"Inspired by Bryan Boyer and Tom Whitwell, I am building a Very Slow Movie Player (VSMP). With VSMP, Kiki’s Delivery Service (running time 1h42m): takes 7 days to play (with 1 frame per 20 seconds, as in above demo) Laputa: Castle in the Sky (running time 2h4m): takes 2 months to play (with 1 frame per 120 seconds, as default setting) 1. Hardware Raspberry Pi Zero WH (Zero W with Headers) Waveshare e-paper 7.5 inch with Hat IKEA Photo Frame All can be assembled together easily. Front view Zoom in details The back 2. Installation2.1 Raspberry Pi ZeroInstall standard Raspberry OS. I am using the 32bit bulleyes with desktop version, but someone suggested to use lite version for Raspberry Pi Zero. Read more here. For install pi with headless wifi, read how-to here. 12touch /Volumes/boot/sshtouch /Volumes/boot/wpa_supplicant.conf Content of wpa_supplicant.conf 123456789country=USctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdevupdate_config=1network=&#123;scan_ssid=1ssid=&quot;your_wifi_ssid&quot;psk=&quot;your_wifi_password&quot;&#125; Note: It is a good practice to disable the default user “Pi”, but VSMP installation script from Tom Whitwell is using hard-coded “Pi” home path, so to keep it simple, keep “Pi” user but DEFINITELY update the default password. I also run it in a guest wifi that it has no access to rest of my network devices. 2.2 Install VSMP filesThere are many implementations of VSMP: https://github.com/TomWhitwell/SlowMovie (I am using this one, works with my e-paper by default) https://github.com/robweber/vsmp-plus (This one provides nice web interface for controlling, but it crashes all the time in my setup) https://github.com/rec0de/vsmp-zero (For e-paper 7.8inch, 1872×1404 resolution, with embedded controller IT8951, communicating via USB&#x2F;SPI&#x2F;I80 interface) Note: You can test your e-ink by running omni-epd-test. In my case, I do the following 1omni-epd-test -e waveshare_epd.epd7in5_V2 The omni-epd is a part of the installation. 3. Prepare the movie3.1 If you need convert mkv to mp412# assume you have ffmpeg installed on your macfor f in *.mkv; do ffmpeg -i &quot;$f&quot; -c copy &quot;$&#123;f%.mkv&#125;.mp4&quot;; done Read more examples at here. 3.2 Remove sound track for reducing the file size12# assume you have ffmpeg installed on your macfor f in *.mp4; do ffmpeg -i &quot;$f&quot; -c copy -an &quot;$&#123;f%.mp4&#125;-nosound.mp4&quot;; done Read more about “-an” parameter at here. 3.3 Copy files to Pi12# from movie folderscp *.mp4 pi@YOUR_PI_HOST_NAME:/home/pi/SlowMovie/Videos/ 4. Play4.1 VSMP as a serviceBy default VSMP is enabled as a service. Edit the slowmovie.conf file to specify parameters such as video locations and start frame 12345678910111213141516# Edit the config filevi slowmovie.conf## Content of slowmovie.conf ##random-frames = Falsedelay = 120increment = 4contrast = 2.0epd = waveshare_epd.epd7in5_V2directory = /home/pi/SlowMovie/Videostimecode = False## End of content #### Restart the servicesudo systemctl restart slowmoviesudo systemctl enable slowmovie 4.2 Manually runIf you want to manually run command, REMEMBER to disable the service. If you run it manually, considering use tmux to ensure the session continues after you log off. Example: 12345678910# Stop servicessudo systemctl stop slowmoviesudo systemctl disable slowmovie# enter tmux sessiontmux# Manual run in tmux session windowcd SlowMoviepython3 slowmovie.py -f ./Videos/Kiki.mp4 -d 20 -s 19970 #delay 20 sec, start from 19970 frame 5. Bonus content5.1 What to playWondering what to play? Read Content reviews: What makes a good slow movie. I am a big fan of Studio Ghibli so that is my choice. Also you might want to re-encode the videos as here. 5.2 Shoot a video for your VSMPYou can use iphone time-lapse to record your VSMP to see how it works. However, iphone time-lapse will sometimes capture some e-paper refresh, when the screen is all white or black. To remove these bad frames from your video, do following (ref #1, #2) 123456789101112# extract all frames from iphone time-lapse videomkdir imgffmpeg -i time-lapse.MOV -qscale:v 2 -r 30/1 img/img%03d.jpg #iphone time-lapse video is 30 fps, second best output img quality# remove bad frames# manual or using ML such as Amazon Lookout For Vision# regenerate the video from framesffmpeg -framerate 30 -pattern_type glob -i &#x27;img/*.jpg&#x27; output.mov# slow it down if neededffmpeg -i output.mov -filter:v &quot;setpts=1.3*PTS&quot; output_slow.mov Example of using Amazon Lookout for Vision for detecting bad frames, but that is another story. 6. Slightly longer demo","categories":[],"tags":[{"name":"Raspberry Pi","slug":"Raspberry-Pi","permalink":"https://feng.lu/tags/Raspberry-Pi/"},{"name":"e-paper","slug":"e-paper","permalink":"https://feng.lu/tags/e-paper/"},{"name":"DIY","slug":"DIY","permalink":"https://feng.lu/tags/DIY/"},{"name":"SVMP","slug":"SVMP","permalink":"https://feng.lu/tags/SVMP/"},{"name":"Amazon Lookout for Vision","slug":"Amazon-Lookout-for-Vision","permalink":"https://feng.lu/tags/Amazon-Lookout-for-Vision/"},{"name":"Movie","slug":"Movie","permalink":"https://feng.lu/tags/Movie/"}]},{"title":"Okta and AWS Control Tower - a happy path demo","slug":"Okta-and-AWS-Control-Tower-a-happy-path-demo","date":"2021-11-17T19:17:08.000Z","updated":"2025-03-28T09:05:16.881Z","comments":true,"path":"2021/11/17/Okta-and-AWS-Control-Tower-a-happy-path-demo/","permalink":"https://feng.lu/2021/11/17/Okta-and-AWS-Control-Tower-a-happy-path-demo/","excerpt":"This is a happy path demo of setting up Okta as the Idp for AWS Control Tower (via AWS SSO). Goal: To utilize users and groups in Okta to manage AWS control tower. 1. Create a brand new Control Tower instanceIn this demo, we create the AWS Control Tower instance in a brand new AWS account. During this process, control tower creates several services&#x2F;components, such as AWS Organizations, AWS SSO, default organizations unit (OU) “Security” and 2 AWS accounts “Log Archive” and “Audit”. In the AWS SSO, some default SSO user groups are created for managing Control Tower: The default admin user for organization management account is “AWS Control Tower Admin”. Detailed user info And it belongs to 2 groups: AWSAccountFactory and AWSControlTowerAdmins","text":"This is a happy path demo of setting up Okta as the Idp for AWS Control Tower (via AWS SSO). Goal: To utilize users and groups in Okta to manage AWS control tower. 1. Create a brand new Control Tower instanceIn this demo, we create the AWS Control Tower instance in a brand new AWS account. During this process, control tower creates several services&#x2F;components, such as AWS Organizations, AWS SSO, default organizations unit (OU) “Security” and 2 AWS accounts “Log Archive” and “Audit”. In the AWS SSO, some default SSO user groups are created for managing Control Tower: The default admin user for organization management account is “AWS Control Tower Admin”. Detailed user info And it belongs to 2 groups: AWSAccountFactory and AWSControlTowerAdmins 2. Setup Okta and use it as the idp of AWS SSO2.1 Create an environment of Okta]For this demo, we are using a free developer plan of Okta. 2.2 Setup Okta as the idp of AWS SSOFollow the steps in the following document, to use Okta as the idp of AWS SSO.Note that you need to check steps from both documentation to make sure the integration and user provisioning works. 2.2.1 Basic hand-shake, import metadata file from Okta to AWS SSOSteps: How to Configure SAML 2.0 for AWS Single Sign-on 2.2.2 Config provisioning and other settings in AWS SSOSteps: Configure provisioning for Okta in AWS SSO 2.3 The basic setup is ready, but not for users and groups yetAfter the basic hand-shake between AWS SSO and Okta, the AWS SSO is now using Okta. In Okta groups UI, you can see identical groups as in AWS SSO are created in Okta. The Everyone is a default Okta user group. Note: you cannot add&#x2F;remove users to it, as it says “This group is managed automatically by Okta, so you cannot edit it or modify its membership.” 3. Setup Okta users and groups, push them to AWS SSO3.1 Create user and groups in OktaLets create some test users: We also create user groups in Okta AWS-CT-Admin-Okta-Group, has 1 user: Feng AWS-CT-Developers-Okta-Group has 2 users: Alice and Bob However, they are not appearing in AWS SSO user list. There is still no Okta user nor Okta group. In order to user the users from Okta, these users need to be assigned to AWS SSO Application in Okta. 3.2 Assign users and&#x2F;or groups in OktaGo to Okta -&gt; Application -&gt; AWS SSO, in Assignments tab, you can either assign individual users or user groups. In this screenshot, all users are assigned to AWS SSO via Group (see the Type column). Soon, you can see these 3 users appear in AWS SSO interface. The detailed info. Note that it was created and updated by SCIM. Now you can assign them into AWS account, so the user can login to AWS console via login to Okta. 3.3 Push groups from Okta to AWS SSONow we can grant permission for individual Okta users. But how about Okta group? These new okta groups are not available in AWS SSO yet. And the groups with identical names from AWS SSO is not helping, as we cannot add users into it. To solve this, we need to push the Okta groups to AWS SSO by setting up the “Push Groups”. Go Okta &gt; Application &gt; AWS SSO, in tab “Push Groups”, here you can push group by name, or setup roles for batch pushing. In this demo, we setup a rule named “Pust-AWS-Related-Groups” for pushing any group that starts with “AWS-” Soon, these groups were pushed to AWS SSO: Now you can also grant permission to groups, such as every Okta user in AWS-CT-Admin-Okta-Group now have permission as AWS control tower admin. EoF.","categories":[],"tags":[{"name":"AWS","slug":"AWS","permalink":"https://feng.lu/tags/AWS/"},{"name":"Okta","slug":"Okta","permalink":"https://feng.lu/tags/Okta/"},{"name":"AWS Control Tower","slug":"AWS-Control-Tower","permalink":"https://feng.lu/tags/AWS-Control-Tower/"},{"name":"AWS SSO","slug":"AWS-SSO","permalink":"https://feng.lu/tags/AWS-SSO/"},{"name":"SSO","slug":"SSO","permalink":"https://feng.lu/tags/SSO/"}]},{"title":"How to build an IoT connected car - Part 2: Data Analytics in the Cloud","slug":"How-to-build-an-IoT-connected-car-Part-2-Data-Analytics-in-the-Cloud","date":"2020-09-22T08:00:13.000Z","updated":"2025-03-28T09:05:16.670Z","comments":true,"path":"2020/09/22/How-to-build-an-IoT-connected-car-Part-2-Data-Analytics-in-the-Cloud/","permalink":"https://feng.lu/2020/09/22/How-to-build-an-IoT-connected-car-Part-2-Data-Analytics-in-the-Cloud/","excerpt":"In Part 1, we have talked about the hardware&#x2F;software running on the edge (the car) for collecting data. Now we have the data, and how to gain some insights by doing data analytics? I have been using the following products, and would like to share my quick thoughts Azure Time Series Insight (TSI) Azure Databricks Azure Data Explorer (ADX) PowerBI Grafana","text":"In Part 1, we have talked about the hardware&#x2F;software running on the edge (the car) for collecting data. Now we have the data, and how to gain some insights by doing data analytics? I have been using the following products, and would like to share my quick thoughts Azure Time Series Insight (TSI) Azure Databricks Azure Data Explorer (ADX) PowerBI Grafana Please note that I tested these products back to Feb&#x2F;March of 2019 and all the feedback were from that time point. I am sure all products were significate upgraded and improved since then, so you might wanna check them again with the lastest features. 1. Azure Time Series Insight (TSI)Azure Time Series Insight (TSI) is an IoT analytics platform monitor, analyze, and visualize your industrial IoT data at scale. With native integration with Azure IoTHub or EventHub, it is easy to visualize and explore the IoT data such as from our connected car. 1.1 Simple data exploringYou can easily explore data by putting time series data into one screen: (click to enlarge)For example, you can identify the relationship between engine RPM and speed, and the increasing temperature of engine coolant. 1.2 Metadata&#x2F;Model managementAs TSI is built for handling IoT data, it has built-in functionality for managing metadata&#x2F;models of IoT data stream. This is a unique feature that only TSI offers, compares to other general-purpose analytics products that I tried. In another word, in order to use TSI, you will have to setup the following models: Time Series Model instances Time Series Model hierarchies Time Series Model types (source: Microsoft) For our case, we can setup the models for representing the 12345678910111213141516171819202122232425262728293031323334353637Assets - ABC Taxi Company Carpool ├── Car 1: Feng Toyota Auris │ ├── GPS │ │ └── FengsDevice_GPS │ │ ├── GPS Speed │ │ ├── Altitude │ │ └── ... │ └── OBD │ └── FengsDevice_OBD │ ├── RPM │ ├── SPEED │ ├── MAF │ ├── ENGINE_LOAD │ └── ... │ ├── Car 2: Thomas Two Engines Monster Truck │ ├── GPS │ │ └── TomsDevice_GPS │ │ ├── GPS Speed │ │ ├── Altitude │ │ └── ... │ └── OBD │ ├── TomsDevice_OBD_Engine_1 │ │ ├── RPM │ │ ├── SPEED │ │ ├── MAF │ │ ├── ENGINE_LOAD │ │ └── ... │ └── TomsDevice_OBD_Engine_2 │ ├── RPM │ ├── SPEED │ ├── MAF │ ├── ENGINE_LOAD │ └── ... │ └── Car 3: ... └── ... For our case, these model definitions can be found at here. 1.3 Summary of TSI experiencePro: Very nice user interface with animation, smoothly zoom in&#x2F;out Built-in support of data module&#x2F;metadata Supporting IoT data in scale (although I only tested with a small dataset) Data can be exported to parquet files, which is optimized for time-series data Con: Limited analytic possibility, cannot run customized query in UI Does not support map Data model contextualization is done only via TSI API, not in parquet file (which is raw data) Exported parquet file is somewhat messy (see more details in DataBricks section) 2. Azure DatabricksIt was nice to visualize the time series data in TSI, but I would like to play more with the dataset, such as calculating the fuel consumption vs. speed for example. I would like to use python and jupyter notebook. Therefore I continue the work with Azure Databrick. 2.1 Simple plot of Speed vs RPM 2.2 Calculate the fuel consumption and eco-driving zoneBy using MFA and speed, it is possible to calculate the fuel consumption, as explained in https://www.windmill.co.uk/fuel.html and https://www.wikihow.com/Convert-MPG-to-Liters-per-100km. 1234567# Adding MPG column# MPG=Speed(Km/h)*7.718/MAFdfwithMPG = df.withColumn(&quot;MPG&quot;,df.series_SPEED_double/df.series_MAF_double*7.718).select(&quot;timestamp&quot;, &quot;series_SPEED_double&quot;, &quot;series_RPM_double&quot;, &quot;MPG&quot;)# Then convert from MPG to L/100km, adding Consumption column, using US galoonsl/100km = 282.48/MPG (imperial gallons) or l/100km = 235.21/MPG (US gallons)dfwithConsumption = dfwithMPG.withColumn(&quot;Consumption&quot;,235.21/dfwithMPG.MPG).select(&quot;timestamp&quot;, &quot;series_SPEED_double&quot;, &quot;series_RPM_double&quot;, &quot;MPG&quot;, &quot;Consumption&quot;) (Picture: Oversimplified calculation of eco-driving zone) 2.3 Issue of TSI generated parquet filesIf we directly use the TSI parquet files as input for the databricks, we will encounter an error message “Found duplicate column(s) in data schema: “series_speed_double”. This is because both GPS and OBD modules are reporting speed, but with different case “Speed” and “SPEED”. TSI is fine with it, as the asset model&#x2F;metadata helps, but in Databrick there is no data contextualization - all data fields are flattened out, therefore it is quite often encountering this type of issue. As a workaround, we can set spark.sql.caseSensitive as true 12sqlContext.sql(&quot;set spark.sql.caseSensitive=true&quot;)df = sqlContext.read.parquet(file_path).select(&quot;column1&quot;, &quot;column2&quot; ) 2.4 Summary of Databricks experiencePro: The standard tool&#x2F;eco-system for data analytics, can do almost anything Rich and powerful libraries Con: Not for ordinary business users who do not work with python&#x2F;programming Some visualization such as map and animation requires extra work Tried PowerBI to databricks (spark), did not manage to make it work at that time (beta version) No builtin data contextualization support, take the input as it is, which is an issue with TSI parquet files 3. Azure Data Explorer (ADX)After tried the a-bit-too-simple TSI and a-bit-too-hardcore Databrick, I was looking for a better-balanced product between them. Therefore I started exploring Azure Data Explorer (ADX). 3.1 Setup database and ingestion mappingLong story short, I created an ADX cluster and a database for IoT Car data, created 2 tables: 123.create table OBDTable (timestamp: datetime, deviceId: string, speed: real, rpm: real, run_time: real, absolute_load: real, short_fuel_trim_1: real, long_fuel_trim_1: real, timing_advance: real, intake_pressure: real, intake_temp: real, throttle_pos: real, relative_throttle_pos: real, oil_temp: real, maf: real, coolant_temp: real, engine_load: real) .create table GPSTable (timestamp: datetime, deviceId: string, gps_speed: real, altitude: real, longitude: string, latitude: string) And create mappings as below: 12345678910111213&quot;Name&quot;: GPSMapping,&quot;Kind&quot;: Json,&quot;Mapping&quot;: [&#123;&quot;column&quot;:&quot;timestamp&quot;,&quot;path&quot;:&quot;$.timestamp&quot;,&quot;datatype&quot;:&quot;datetime&quot;,&quot;transform&quot;:&quot;None&quot;&#125;,&#123;&quot;column&quot;:&quot;deviceId&quot;,&quot;path&quot;:&quot;$.deviceId&quot;,&quot;datatype&quot;:&quot;string&quot;,&quot;transform&quot;:&quot;None&quot;&#125;,&#123;&quot;column&quot;:&quot;gps_speed&quot;,&quot;path&quot;:&quot;$.series[0].gps_speed&quot;,&quot;datatype&quot;:&quot;double&quot;,&quot;transform&quot;:&quot;None&quot;&#125;,&#123;&quot;column&quot;:&quot;altitude&quot;,&quot;path&quot;:&quot;$.series[0].altitude&quot;,&quot;datatype&quot;:&quot;double&quot;,&quot;transform&quot;:&quot;None&quot;&#125;,&#123;&quot;column&quot;:&quot;longitude&quot;,&quot;path&quot;:&quot;$.series[0].longitude&quot;,&quot;datatype&quot;:&quot;string&quot;,&quot;transform&quot;:&quot;None&quot;&#125;,&#123;&quot;column&quot;:&quot;latitude&quot;,&quot;path&quot;:&quot;$.series[0].latitude&quot;,&quot;datatype&quot;:&quot;string&quot;,&quot;transform&quot;:&quot;None&quot;&#125;],&quot;LastUpdatedOn&quot;: 2019-02-27T19:25:47.889932Z,&quot;Database&quot;: iotcardb,&quot;Table&quot;: GPSTable,&quot;Name&quot;: OBDMapping,&quot;Kind&quot;: Json,&quot;Mapping&quot;: [&#123;&quot;column&quot;:&quot;timestamp&quot;,&quot;path&quot;:&quot;$.timestamp&quot;,&quot;datatype&quot;:&quot;datetime&quot;,&quot;transform&quot;:&quot;None&quot;&#125;,&#123;&quot;column&quot;:&quot;deviceId&quot;,&quot;path&quot;:&quot;$.deviceId&quot;,&quot;datatype&quot;:&quot;string&quot;,&quot;transform&quot;:&quot;None&quot;&#125;,&#123;&quot;column&quot;:&quot;speed&quot;,&quot;path&quot;:&quot;$.series[0].SPEED&quot;,&quot;datatype&quot;:&quot;double&quot;,&quot;transform&quot;:&quot;None&quot;&#125;,&#123;&quot;column&quot;:&quot;rpm&quot;,&quot;path&quot;:&quot;$.series[0].RPM&quot;,&quot;datatype&quot;:&quot;double&quot;,&quot;transform&quot;:&quot;None&quot;&#125;,&#123;&quot;column&quot;:&quot;run_time&quot;,&quot;path&quot;:&quot;$.series[0].RUN_TIME&quot;,&quot;datatype&quot;:&quot;double&quot;,&quot;transform&quot;:&quot;None&quot;&#125;,&#123;&quot;column&quot;:&quot;absolute_load&quot;,&quot;path&quot;:&quot;$.series[0].ABSOLUTE_LOAD&quot;,&quot;datatype&quot;:&quot;double&quot;,&quot;transform&quot;:&quot;None&quot;&#125;,&#123;&quot;column&quot;:&quot;short_fuel_trim_1&quot;,&quot;path&quot;:&quot;$.series[0].SHORT_FUEL_TRIM_1&quot;,&quot;datatype&quot;:&quot;double&quot;,&quot;transform&quot;:&quot;None&quot;&#125;,&#123;&quot;column&quot;:&quot;long_fuel_trim_1&quot;,&quot;path&quot;:&quot;$.series[0].LONG_FUEL_TRIM_1&quot;,&quot;datatype&quot;:&quot;double&quot;,&quot;transform&quot;:&quot;None&quot;&#125;,&#123;&quot;column&quot;:&quot;timing_advance&quot;,&quot;path&quot;:&quot;$.series[0].TIMING_ADVANCE&quot;,&quot;datatype&quot;:&quot;double&quot;,&quot;transform&quot;:&quot;None&quot;&#125;,&#123;&quot;column&quot;:&quot;intake_pressure&quot;,&quot;path&quot;:&quot;$.series[0].INTAKE_PRESSURE&quot;,&quot;datatype&quot;:&quot;double&quot;,&quot;transform&quot;:&quot;None&quot;&#125;,&#123;&quot;column&quot;:&quot;intake_temp&quot;,&quot;path&quot;:&quot;$.series[0].INTAKE_TEMP&quot;,&quot;datatype&quot;:&quot;double&quot;,&quot;transform&quot;:&quot;None&quot;&#125;,&#123;&quot;column&quot;:&quot;throttle_pos&quot;,&quot;path&quot;:&quot;$.series[0].THROTTLE_POS&quot;,&quot;datatype&quot;:&quot;double&quot;,&quot;transform&quot;:&quot;None&quot;&#125;,&#123;&quot;column&quot;:&quot;relative_throttle_pos&quot;,&quot;path&quot;:&quot;$.series[0].RELATIVE_THROTTLE_POS&quot;,&quot;datatype&quot;:&quot;double&quot;,&quot;transform&quot;:&quot;None&quot;&#125;,&#123;&quot;column&quot;:&quot;oil_temp&quot;,&quot;path&quot;:&quot;$.series[0].OIL_TEMP&quot;,&quot;datatype&quot;:&quot;double&quot;,&quot;transform&quot;:&quot;None&quot;&#125;,&#123;&quot;column&quot;:&quot;maf&quot;,&quot;path&quot;:&quot;$.series[0].MAF&quot;,&quot;datatype&quot;:&quot;double&quot;,&quot;transform&quot;:&quot;None&quot;&#125;,&#123;&quot;column&quot;:&quot;coolant_temp&quot;,&quot;path&quot;:&quot;$.series[0].COOLANT_TEMP&quot;,&quot;datatype&quot;:&quot;double&quot;,&quot;transform&quot;:&quot;None&quot;&#125;,&#123;&quot;column&quot;:&quot;engine_load&quot;,&quot;path&quot;:&quot;$.series[0].ENGINE_LOAD&quot;,&quot;datatype&quot;:&quot;double&quot;,&quot;transform&quot;:&quot;None&quot;&#125;],&quot;LastUpdatedOn&quot;: 2019-02-27T19:17:24.3220181Z,&quot;Database&quot;: iotcardb,&quot;Table&quot;: OBDTable, 3.2 Data analytics by using kusto query languageNow we are ready to query by using powerful kusto query language, especially the timeseries related analytics. 3.2.1 Average GPS speed aggregated by every 20 secondsSimple data aggregation 1234567//avg gps speed every 20slet min_t = datetime(2019-03-06 12:30:00); //UTClet max_t = datetime(2019-03-06 13:00:00); //UTCGPSTable| where timestamp between (min_t .. max_t)| summarize avg(gps_speed) by bin(timestamp, 20s)| render timechart 3.2.2 Join 2 tables, Show obd speed, gps speed and avg MAFInner join two tables and apply aggregation 12345678910111213//join 2 tables, show obd speed, gps speed and avg maf let min_t = datetime(2019-03-06 12:30:00); //UTClet max_t = datetime(2019-03-06 13:00:00); //UTCGPSTable| where timestamp between (min_t .. max_t)| summarize avg(gps_speed) by bin(timestamp, 20s)| join kind=inner(OBDTable | where timestamp between (min_t .. max_t)| summarize avg(speed), avg(maf) by bin(timestamp, 20s))on timestamp | project timestamp, avg_gps_speed, avg_speed, avg_maf | render timechart 3.2.3 Applies two segments linear regression on engine_loadApply two segments linear regression on engine load, see the document here. 123456789//Applies two segments linear regression on engine_load. let min_t = datetime(2019-03-06 12:40:55); //UTClet max_t = datetime(2019-03-06 12:57:20); //UTCOBDTable| where coolant_temp &gt; 0| make-series engine_load_targetAvg = avg(engine_load) default = 0 on timestamp in range(min_t, max_t, 5s)| extend LineFit=series_fit_line_dynamic(engine_load_targetAvg).line_fit, LineFit2=series_fit_2lines_dynamic(engine_load_targetAvg).line_fit| project timestamp, engine_load_targetAvg, LineFit, LineFit2 | render timechart 3.3 Summary of ADX experiencePro: Easy and powerful kusto query language, good balance between capacity and user-friendly query. Powerful and proven data computing capacity (the same technology that powers Azure log) Contextualization in storage (table and columns) Works well with PowerBI (see PowerBI section) Con: Cannot replace the “real” big data products It has its own query limits Missing some visualization such as map and animation 4. PowerBISo far I have tried several products for analytics, but none of them have great built-in visualization features, especially on map support. PowerBI is a popular tool for data visualization, but it does not support big data analytics. However, by combining PowerBI and ADX, the job is easier. 4.1 Let ADX to handle the computing partInstead of doing the visualization in ADX, now we use a query to generate a dataset (two dimensional table) 12345678910111213//for powerbi map, CANNOT have comments!let min_t = datetime(2019-03-06 12:40:55);let max_t = datetime(2019-03-06 12:57:20);GPSTable| where timestamp between (min_t .. max_t)| summarize any(longitude), any(latitude) by bin(timestamp, 20s)| join kind=inner(OBDTable | where timestamp between (min_t .. max_t)| summarize avg(speed), avg(maf), avg(rpm) by bin(timestamp, 20s))on timestamp | project timestamp, any_latitude, any_longitude, avg_speed, avg_maf, avg_rpm Then use “Query to PowerBI” on the dropdown list. NOTE: When I was testing this, there was an issue that the Kusto query can NOT have inline comments, otherwise these inline comments will be mixed into the generated powerBI query, which ruined the syntax. Keep all the comments out of the kusto query block. 4.2 In PowerBI, create good-looking visualization dashboardBy using the generated PowerBI query from above, I can easily create differnt visualization dashboard in PowerBI. For example the map: It shows one of the trips on the map, as well as the speed: greener is faster, and reder means slower. 4.3 Replay a tripUsing PowerBI addon such as Play Axis (Dynamic Slicer), it is easy to replay a trip. Picture: Play a trip in PowerBI, with map and engine RPM. X-axis: time Y-axis: speed (from GPS) Size of the bubble: Engine RPM (from OBD) It clearly shows where was the traffic jam (drops of speed), and where had a good traffic condition (peak of speed and RPM). 4.4 Summary of PowerBI experiencePro: Easy to use Rich visualization (e.g. map) Easy to share (e.g. PowerBI Online) Con: Not for big data computing, but good for visualization Need a “big data computing” back end (in our case it is ADX) 5. GrafanaPowerBI is a good visualization tool, but it is not easy to directly create&#x2F;update kusto query in PowerBI. Most likely you will have to run and test the query in ADX, then export to PowerBI. We hope to overcome this issue with Grafana. Grafana is an open source tool mainly used for monitoring and data visualization. With the Azure Data Explorer Datasource For Grafana plugin, we can integrate the ADX and Kusto power with fancy and powerful Grafana visualization. 5.1 Run Grafana as docker, with preinstalled ADX data source plugin1docker run -p 3000:3000 -e &quot;GF_INSTALL_PLUGINS=grafana-azure-data-explorer-datasource&quot; grafana/grafana:latest Then follow the plugin documenation to config access. Now you can directly create Kusto-enabled dashboard, including map. 5.2 Summary of Grafana experiencePro: Very rich and powerful visualization Self-service Kusto enabled queries and build dashboard Built-in access control and notification support Big support by the community Con: (cannot find one…for now) ConclusionNow we have tried several products, and my favorite setup is ADX (as backend data storage and query) and Grafana (as front end self-service visualization). I believe it meet the most common needs of ordinary users. But of course other products have different focus areas and can&#x2F;should be used for different scenarios. After all, the old saying is always correct: “It depends.” Thanks for the reading. (Read Part 1 of this article series)","categories":[],"tags":[{"name":"IoT","slug":"IoT","permalink":"https://feng.lu/tags/IoT/"},{"name":"Azure","slug":"Azure","permalink":"https://feng.lu/tags/Azure/"},{"name":"PowerBI","slug":"PowerBI","permalink":"https://feng.lu/tags/PowerBI/"},{"name":"Data Analytics","slug":"Data-Analytics","permalink":"https://feng.lu/tags/Data-Analytics/"},{"name":"Python","slug":"Python","permalink":"https://feng.lu/tags/Python/"},{"name":"Jupyter Notebook","slug":"Jupyter-Notebook","permalink":"https://feng.lu/tags/Jupyter-Notebook/"},{"name":"Visualization","slug":"Visualization","permalink":"https://feng.lu/tags/Visualization/"},{"name":"Car","slug":"Car","permalink":"https://feng.lu/tags/Car/"},{"name":"Databricks","slug":"Databricks","permalink":"https://feng.lu/tags/Databricks/"},{"name":"Time Series Insight (TSI)","slug":"Time-Series-Insight-TSI","permalink":"https://feng.lu/tags/Time-Series-Insight-TSI/"},{"name":"Azure Data Explorer (ADX)","slug":"Azure-Data-Explorer-ADX","permalink":"https://feng.lu/tags/Azure-Data-Explorer-ADX/"},{"name":"Grafana","slug":"Grafana","permalink":"https://feng.lu/tags/Grafana/"}]},{"title":"How to build an IoT connected car - Part 1: On the Edge","slug":"How-to-build-an-IoT-connected-car-Part-1-On-the-Edge","date":"2020-09-15T08:38:12.000Z","updated":"2025-03-28T09:05:16.640Z","comments":true,"path":"2020/09/15/How-to-build-an-IoT-connected-car-Part-1-On-the-Edge/","permalink":"https://feng.lu/2020/09/15/How-to-build-an-IoT-connected-car-Part-1-On-the-Edge/","excerpt":"Previously I wrote a blog about how to measure hamster via IoT wheel. This reminds me another personal project I did back to the winter of 2018&#x2F;2019, for measuring car performance.","text":"Previously I wrote a blog about how to measure hamster via IoT wheel. This reminds me another personal project I did back to the winter of 2018&#x2F;2019, for measuring car performance. Other articles in this series: Part 1 (this article):Talk about the hardware&#x2F;software running on the edge (the car) for collecting data. Part 2: Talk about how to get insight from the data with different analytic tools. 1. Overview on the edge1.1 Hardware OBD2 USB connectorOBD2 is an interface&#x2F;protocol that is available for 1996 and newer vehicles. It reports various telemetries of the vehicle. USB GPS dongleFor collecting location information Raspberry PiRunning Linux, Azure IoT Edge runtime and hosting 2 modules (OBD and GPS location) USB Wifi dongleFor connecting mobile phone hotspot Mobile phoneFor realtime data uploading to the cloud via 4G Power bank (optional)For powering up raspberry pi. Alternatively, you can use a 12V-&gt;5V adapter to use the car battery. 1.2 Software Azure IoT Edge Docker Python 1.3 Dataflow and Architecture 2. Developing IoT Edge modules2.1 Introduction2.1.1 Design PrinciplesAs there are many possible situations can happen on the edge, such as disconnection from OBD2 connector, or loss of GPS signal (when going through an underground tunnel), the modules are built with the following principles: Design for failure Auto healing In addition, the modules are built into docker containers, together with Azure IoT Edge runtime, which makes it easier to deploy. 2.1.2 Source codeAll source code can be found at https://github.com/linkcd/IoTCar 2.1.3 Sample dataThis edge device (raspberry pi + OBD connector + GPS dongle) reports the following data per second: 2.2 GPS location module - detailsSource code With a USB GPS dongle, it is quite easy to get the location information by using tools such as GPSD. 2.2.1 You have to dev&#x2F;test in the field, not only in the officeI immediately met the first challenge: The USB GPS dongle requires a good open sky view to work well. The one that I used does not have an antenna, so I need to put the whole thing (raspberry pi + GPS dongle) outside of the building (or at least outside of the window). Remind you that it was winter in Norway during that time, and I was not a fan of typing keyboard in the snow with -5 degrees. Firstly, I have tried do this in my car: parked the car in an outdoor parking slot, put the raspberry pi on the dashboard and remote desktop to it. Well, it worked, GSP signal was strong, but it was quite difficult to type any keys behind the steering wheel :) But soon I figured out a better solution on my balcony (see below picture), and that worked perfectly (as far as the wifi signal is good and the power bank battery did not die from the low temperature) Now I can work from a warm cozy place and deal with the GPS data that is collected from the “cold box”. 2.2.2 Working with GPS data with pythonThe GPS receiver reports data as NMEA sentences, and we are combining GGA and RMC. 1234# GGA $GPGGA,123519,4807.038,N,01131.000,E,1,08,0.9,545.4,M,46.9,M,,*47# RMC$GPRMC,123519,A,4807.038,N,01131.000,E,022.4,084.4,230394,003.1,W*6A Here we are using a python lib pynmea2 for handling the NMEA sentences, the detailed logic can be found at source code here. In addition, we need to do some small math for calculating the correct latitude and longitude, otherwise you will find your car was driving in the ocean :) 123456789101112131415161718# The latitude is formatted as DDMM.ffff and longitude is DDDMM.ffff where D is the degrees and M is minutes plus the fractional minutes. # So, 1300.8067,N is 13 degrees 00.8067 minutes North and the longitude of 07733.0003,E is read as 77 degrees 33.0003 minutes East.# Converting to degrees you would have to do this: 13 + 00.8067/60 for latitude and 77 + 33.0003/60 for the longitude.# ##NMEA outputs in a human readable DDDMM.mmmm format NOT DECIMAL DEGREES# 3746.03837# 37 46.03837# 37 + (46.03837 / 60)# result = 37 + 0.7673062segments = value.split(&#x27;.&#x27;)if len(segments[0]) == 4: #lanitude degree = segments[0][:2]else: #longtitude degree = segments[0][:3]minute = round(Decimal(segments[0][-2:] + &quot;.&quot; + segments[1])/60, 6) Finally, this module reports the following data per second 1234567891011121314151617181920212223242526&#123; &quot;series&quot;: [ &#123; &quot;mag_variation&quot;: &quot;&quot;, &quot;geo_sep&quot;: &quot;39.1&quot;, &quot;num_sats&quot;: 5, &quot;fixed_time&quot;: &quot;20:33:21&quot;, &quot;geo_sep_units&quot;: &quot;M&quot;, &quot;horizontal_dil&quot;: &quot;2.21&quot;, &quot;longitude_dir&quot;: &quot;E&quot;, &quot;mag_var_dir&quot;: &quot;&quot;, &quot;gps_speed&quot;: 0.242, &quot;altitude_units&quot;: &quot;M&quot;, &quot;true_course&quot;: null, &quot;latitude&quot;: &quot;11.111111&quot;, &quot;fixed_full_timestamp&quot;: &quot;2019-02-26 20:33:21&quot;, &quot;latitude_dir&quot;: &quot;N&quot;, &quot;fixed_date&quot;: &quot;2019-02-26&quot;, &quot;gps_quality&quot;: 1, &quot;longitude&quot;: &quot;22.222222&quot;, &quot;altitude&quot;: 93.4 &#125; ], &quot;deviceId&quot;: &quot;FengsDevice_GPS&quot;, &quot;timestamp&quot;: &quot;2019-02-26 20:33:21&quot;&#125; 2.3 OBD2 module - detailsSource code 2.3.1 You cannot do it in the field, do it on an emulator insteadProgramming&#x2F;debugging OBD2 can be difficult - after all I do not want to be programming while driving. Instead of hiring a driver and typing the keyboard on the passenger seat, it is better to use an ODB emulator to emulate all telemetries (and error codes) of the car. OBD2 EmulatorLucky I am not alone who has the same problem during OBD development. There are professional and affordable emulators on Aliexpress and Taobao (BTW The price on Taobao is 1&#x2F;3 as Aliexpress!). The detailed features can be found at [here](OBD emulator features.jpg). My respects to the designers of this emulator - you are life savers! 2.3.2 Design for failure and auto healingNow, with the emulator and python obd lib, it is easy to collect the telemetries of the car. However, the library does not take care of failures and auto-healing, which we need to do it ourselves, otherwise the code just throw exceptions and stop working. Thanks to the emulator, it is easy to test all corner scenarios, such as disconnect the ODB and reconnect while “the engine” is still running, in a safe environment. That is impossible to test&#x2F;debug with real car. The following code snippet ensures the modules works with different scenarios and self-healing: Car is powered off Car is powered on but engine is not started yet Engine starts Engine is stopped but car is still powered on OBD receiver is disconnected (e.g lost bluetooth signal) 12345678910111213141516171819def getVehicleTelemtries(deviceId): global connection if(not connection.is_connected()): print(&quot;No connecting to the car, reconnecting...&quot;) connection = obd.OBD(fast=True) try: # Use library to get readings... if(telemtryDic[&quot;RPM&quot;] == 0): print(&quot;Cannot read RPM, reconnecting...&quot;) connection = obd.OBD(fast=True) return None else: return buildJsonPayload(deviceId, telemtryDic) except Exception as e: print(&quot;Error with OBDII, error: &quot; + str(e) + &quot;. Reconnecting...&quot;) connection = obd.OBD(fast=True) return None Finally, this module reports the following data per second: 1234567891011121314151617181920212223&#123; &quot;series&quot;: [ &#123; &quot;SPEED&quot;: 56, &quot;RPM&quot;: 2830.75, &quot;RUN_TIME&quot;: 639, &quot;ABSOLUTE_LOAD&quot;: 0.0, &quot;SHORT_FUEL_TRIM_1&quot;: -21.09375, &quot;TIMING_ADVANCE&quot;: 0.0, &quot;INTAKE_PRESSURE&quot;: 0, &quot;LONG_FUEL_TRIM_1&quot;: 18.75, &quot;INTAKE_TEMP&quot;: 0, &quot;THROTTLE_POS&quot;: 37.64705882352941, &quot;OIL_TEMP&quot;: 16, &quot;MAF&quot;: 655.35, &quot;RELATIVE_THROTTLE_POS&quot;: 0.0, &quot;COOLANT_TEMP&quot;: 0, &quot;ENGINE_LOAD&quot;: 45.490196078431374 &#125; ], &quot;timestamp&quot;: &quot;2019-02-20 19:27:11.705387&quot;, &quot;deviceId&quot;: &quot;FengsDevice_OBD&quot;&#125; 2.3.3 Tips: Considering use ‘Real Time Clock’ or RTC board for your PiAs Raspberry PI does not have an RTC, the system clock was reset after each power-on. If it has an internet connection, it will fetch the correct date-time from internet, with some delay. In the current logic, both GPS and OBD modules are using the system clock as the event timestamp. Therefore, if the Pi failed to have internet connection (happened often with mobile hotspot) or sending data before system clock is updated due to delay, the event timestamp will be incorrect. To overcome this issue, you can install a RTC (Real Time Clock) to the Raspberry Pi, such as this and this. I end up with a UPS-18650 Raspberry pi UPS Power Expansion Board With RTC. It comes with a power bank AND a built-in RTC. It is design and built by ACE design studio in China, and I am very happy about it. Definitely buy more from them next time. (Picture: My to-be-tested Raspberry Pi with UPS power expansion board and LoRa&#x2F;GPS Hat.Hopefully it can use LoRa network connections to replace 4G) 3. Put things together and send to Azure3.1 Config on the edgeNow we have 2 modules and we have built them into 2 docker images (in variables ${MODULES.OBDModule.arm32v7} and ${MODULES.LocatorModule.arm32v7}). I was hosting them in the dockerhub but it can also be hosted in any private registration. For now we did not do any computing on the edge but simply forward them to Azure Iot Hub (see here) 123456789101112&quot;$edgeHub&quot;: &#123; &quot;properties.desired&quot;: &#123; &quot;schemaVersion&quot;: &quot;1.0&quot;, &quot;routes&quot;: &#123; &quot;OBDModuleToIoTHub&quot;: &quot;FROM /messages/modules/OBDModule/outputs/* INTO $upstream&quot;, &quot;LocatorModuleToIoTHub&quot;: &quot;FROM /messages/modules/LocatorModule/outputs/* INTO $upstream&quot; &#125;, &quot;storeAndForwardConfiguration&quot;: &#123; &quot;timeToLiveSecs&quot;: 7200 &#125; &#125; &#125; More info can be found in the deployment.template.json. 3.2 Visualization in the cloudNow we 2 module docker images running on the Raspberry Pi and sending data to Azure IoT Edge runtime. The Raspberry Pi has wifi connection to mobile phone 4G hotspot and forwarding the data to Azure IoT Hub in realtime. With Azure IoT Hub and Azure Time Series Insights(TSI), we can now visualize the data: This is a quick example of data analytics for the IoT car. In the second part of the series, I will talk more about the data analytics part (including TSI, DataBrick ++) in the cloud. Continue reading part 2","categories":[],"tags":[{"name":"IoT","slug":"IoT","permalink":"https://feng.lu/tags/IoT/"},{"name":"Raspberry Pi","slug":"Raspberry-Pi","permalink":"https://feng.lu/tags/Raspberry-Pi/"},{"name":"Data Analytics","slug":"Data-Analytics","permalink":"https://feng.lu/tags/Data-Analytics/"},{"name":"Car","slug":"Car","permalink":"https://feng.lu/tags/Car/"},{"name":"Azure IoT Hub","slug":"Azure-IoT-Hub","permalink":"https://feng.lu/tags/Azure-IoT-Hub/"},{"name":"Azure IoT Edge","slug":"Azure-IoT-Edge","permalink":"https://feng.lu/tags/Azure-IoT-Edge/"},{"name":"GPS","slug":"GPS","permalink":"https://feng.lu/tags/GPS/"},{"name":"OBD2","slug":"OBD2","permalink":"https://feng.lu/tags/OBD2/"}]},{"title":"How to measure your Hamster's running with wireless IoT","slug":"How-to-measure-your-Hamster-s-running-with-IoT","date":"2020-08-05T17:30:43.000Z","updated":"2025-03-28T09:05:16.741Z","comments":true,"path":"2020/08/05/How-to-measure-your-Hamster-s-running-with-IoT/","permalink":"https://feng.lu/2020/08/05/How-to-measure-your-Hamster-s-running-with-IoT/","excerpt":"We recently welcomed our new family member Qiuqiu (球球) (a girl Syrian&#x2F;Golden hamster) home. She seems to enjoy the new environment fairly well, but she is a quiet girl - does not show much activities during the day time. Of course we understand hamsters are nocturnal animals, which means they are sleeping in day time and become more active at night. But I started wondering how she was doing during the nights, especially how much she ran on the hamster wheel. Let’s do something about it. Picture: Qiuqiu with her wheel","text":"We recently welcomed our new family member Qiuqiu (球球) (a girl Syrian&#x2F;Golden hamster) home. She seems to enjoy the new environment fairly well, but she is a quiet girl - does not show much activities during the day time. Of course we understand hamsters are nocturnal animals, which means they are sleeping in day time and become more active at night. But I started wondering how she was doing during the nights, especially how much she ran on the hamster wheel. Let’s do something about it. Picture: Qiuqiu with her wheel 1. HardwareThere are many possible ways to track the hamster wheel. Wired solution, attach wire and switch to the wheelIt should work pretty straight forward, but I am not a fan of having wires going through the cage for connecting to the computer. Also Qiuqiu will definitely chew on the wires. Wireless solution, with computer visionThis can be a pretty cool idea: Draw a mark (e.g. a red X) on the wheel, then place a camera (ie. AWS DeepLens) to run some computer vision tasks, for counting the wheel cycle.I like this idea because it requires minimal work on the wheel and no dangerous for the hamster at all. But there also are some challenges such as how to ensure the image quality if there is no light in the room, or the wheel is running too fast to get a stable high quality image. Wireless solution, with wireless sensorThis is what I did - need to attach a sensor on the wheel, but the sensor is so small that can be well protected in a shell. I decided to use zigbee protocol as I already have a smart home system that is well integrated. 1.1 Needed hardware Sensor: Aqara Door and Window Sensor Zigbee gateway: Conbee II PC&#x2F;Laptop&#x2F;Raspberry Pi 1.2 Installation1.2.1 Place the sensorCarefully place the sensor on the wheel and the body, make sure when wheel spins, the magnet on the wheel has a small but close enough gap with the sensor body. Used lego part for some adjustments. 1.2.2 Test with realtime sensor readingBefore we continue, I would like to test in action, to make sure the gap is OK. It is possible monitor realtime reading of the sensor, by using the Conbee API.I wrote a simple web app (source code) with javascript and WebSocket, to visualize the realtime reading. The WebSocket API is provided by the Conbee application, see the document here. Under the hood: 1.2.3 Mount the protective shellI made a protective shell from a spare plastic box, and mounted on the wheel. Therefore Qiuqiu cannot chew on the sensor. I even made a small hole on the shell to easily use a stick for pressing the sensor reset button, without remove the whole thing. 2. Software2.1 Manual data exportThe BeeCon 2 is a USB-based zigbee gateway that can be attached to a PC or raspberry pi. It talks to the zigbee mesh network and receives signals from sensors. For example, the sensor on the wheel send the following json payload, one for “close” event (magnet and sensor are closed) and another one for “open” event (magnet and sensor are parted). Logically one open-close event pair indicates a finished cycle: 12345678910111213141516171819202122&#123; &quot;e&quot;: &quot;changed&quot;, &quot;id&quot;: &quot;3&quot;, &quot;r&quot;: &quot;sensors&quot;, &quot;state&quot;: &#123; &quot;lastupdated&quot;: &quot;2020-08-05T17:32:37.102&quot;, &quot;open&quot;: false &#125;, &quot;t&quot;: &quot;event&quot;, &quot;uniqueid&quot;: &quot;00:15:8d:00:04:5c:d8:d3-01-0006&quot;&#125; &#123; &quot;e&quot;: &quot;changed&quot;, &quot;id&quot;: &quot;3&quot;, &quot;r&quot;: &quot;sensors&quot;, &quot;state&quot;: &#123; &quot;lastupdated&quot;: &quot;2020-08-05T17:32:37.227&quot;, &quot;open&quot;: true &#125;, &quot;t&quot;: &quot;event&quot;, &quot;uniqueid&quot;: &quot;00:15:8d:00:04:5c:d8:d3-01-0006&quot;&#125; By connecting Conbee gateway with Home Assistant via deCONZ integration, it is fairly easy to export the data as a CSV file. (I plan to build a data pipeline with time series database in the later stage, but for now let’s stay with manual data export.) Picture: exported csv, with 4 columns 2.2 Data analytics2.2.1 Data loading and transformationNow it is time for having some python&#x2F;jupyter notebook fun. Here we are going to use https://www.kaggle.com/. You can read more comparison of online jupyter notebook hosting at here. The above code snippet does: Load CSV file into pandas DataFrame, with needed 2 columns ‘last_changed’ and ‘state’ During loading, parse datetime and also set last_changed as index Convert fixed string value “on”&#x2F;“off” in ‘state’ to digital 0&#x2F;1 in ‘finshedOneRound’ that can be used for plot 2.2.2 Check the raw data and noisesLet’s take a look at the raw data. The first thing I noticed is the “noise” of each cycle. As the door-window close sensor is not designed for tracking a spin, whenever a cycle finished, instead of report simple 2 events: on and off, it actually generates a sequence of events: on-off-on-off-on-off. This is a “noise” that we need to take care of. it is worth noting that not all cycles follow the same pattern. For example, the 3rd red circle on the screenshot shows an exception: it only has one “on-off” event pair. 2.3 Noise reduction by rolling window calculationsWe need a way to “group” the multiple events (“on-off-on-off-on-off”) into one event that indicates a cycle, but we cannot group by a fixed pattern as there are exceptions (as we mentioned above). After some quick research and testing, without diving into hard-core data science part, I found the rolling window calculation can be a solution for our case. Lets set the rolling windows to 150 ms - it is “magic number” that works good with the raw data. It purely depends on how fast the hamster runs. Lets visualize the rolling calculation results. 1234567891011import plotly.graph_objects as gofig = go.Figure()fig.add_trace(go.Scatter(x=df.index, y=df[&#x27;finshedOneRound&#x27;], name=&#x27;raw&#x27;))fig.add_trace(go.Scatter(x=df.index, y=df[&#x27;finshedOneRound_rolled&#x27;], mode=&#x27;lines+markers&#x27;, name=&#x27;rolling&#x27;))fig.update_xaxes(rangeslider_visible=True)fig.update_yaxes(tick0=0, dtick=1)fig.show() Now you can see that the result of rolling calculation does generate unique markers for each cycle (the green circles), and it works for different patterns in the raw data! Extract the markers (where rolling result &#x3D;&#x3D; 1) into a new dataframe df_cycle_log for the next step. 1df_cycle_log = df.loc[df[&quot;finshedOneRound_rolled&quot;] == 1] 2.4 Calculation and visualization for the final resultI would like to know: When did Qiuqiu run? How far did she run? What was the speed? What kind of running pattern she has? (sprint or marathon?) Lets do some match and populate the results: 1234567891011121314151617181920import mathdef get_distance_by_wheel_cycle_count(cycle_count): diameter = 0.2 #the wheel diameter is 20cm return cycle_count * diameter * math.pidef get_speed_in_KMh(traveled_range_in_m, run_time_in_sec): return traveled_range_in_m / run_time_in_sec * 3.6 #(1m/s = 3.6 km/h)def get_speed_by_cycle_count(cycle_count, run_time_in_sec): distance = get_distance_by_wheel_cycle_count(cycle_count) return get_speed_in_KMh(distance, run_time_in_sec)#Aggreate the cycle counts every 30sec, popluate the datarun_time_segment = &quot;30s&quot;df_result = pd.DataFrame()df_result[&quot;cycle_count&quot;] = df_cycle_log[&quot;finshedOneRound_rolled&quot;].resample(run_time_segment).count()df_result[&quot;distance&quot;] = df_result[&quot;cycle_count&quot;].apply(get_distance_by_wheel_cycle_count)df_result[&quot;speed_km&quot;] = df_result[&quot;cycle_count&quot;].apply(lambda count: get_speed_by_cycle_count(count, 30)) Then plot 123456789101112import plotly.graph_objects as gofrom plotly.subplots import make_subplots# Create figure with secondary y-axisfig = make_subplots(specs=[[&#123;&quot;secondary_y&quot;: True&#125;]])#fig.add_trace(go.Scatter(x=df_result.index, y=df_result[&#x27;cycle_count&#x27;], name=&quot;wheel count&quot;))fig.add_trace(go.Scatter(x=df_result.index, y=df_result[&#x27;speed_km&#x27;], name=&quot;speed(km/h)&quot;), secondary_y=False)fig.add_trace(go.Scatter(x=df_result.index, y=df_result[&quot;distance&quot;].cumsum()/1000, name=&quot;distance(km)&quot;), secondary_y=True)fig.update_xaxes(rangeslider_visible=True)fig.show() Conclusion from the result: When did Qiuqiu run?She mainly ran for about 4 hours, between 21:30 and 01:30 UTC time (or 23:30 and 03:30 Oslo time). How far did she run?During these 4 hours, she ran about 7.74 KM (12315 cycles). What was the speed?Average speed was about 3KM&#x2F;h, with a peak 3.7 KM&#x2F;h. What kind of running pattern she had? (sprint or marathon?)She typically ran 5 minutes sprints, and took 2 min small breaks between them. According to the internet, Qiuqiu is not the fastest runner (a hamster can run up to 5-9 KM&#x2F;h), and also ran slightly less than average range 9 KM in that evening. Of course the speed&#x2F;range can vary from hamster to hamster, and this is data for one evening. The next step is to build a fully automated data pipeline with time series database, create some Grafana dashboards with daily&#x2F;weekly baseline for long term tracking. Thanks for the reading.","categories":[],"tags":[{"name":"IoT","slug":"IoT","permalink":"https://feng.lu/tags/IoT/"},{"name":"Hamster","slug":"Hamster","permalink":"https://feng.lu/tags/Hamster/"},{"name":"Data Analytics","slug":"Data-Analytics","permalink":"https://feng.lu/tags/Data-Analytics/"},{"name":"Home Assistant","slug":"Home-Assistant","permalink":"https://feng.lu/tags/Home-Assistant/"},{"name":"Python","slug":"Python","permalink":"https://feng.lu/tags/Python/"},{"name":"Jupyter Notebook","slug":"Jupyter-Notebook","permalink":"https://feng.lu/tags/Jupyter-Notebook/"},{"name":"Zigbee","slug":"Zigbee","permalink":"https://feng.lu/tags/Zigbee/"}]},{"title":"Encoding issue when calling API via powershell","slug":"Encoding-issue-when-calling-API-via-powershell","date":"2020-06-20T11:40:00.000Z","updated":"2025-03-28T09:05:16.570Z","comments":true,"path":"2020/06/20/Encoding-issue-when-calling-API-via-powershell/","permalink":"https://feng.lu/2020/06/20/Encoding-issue-when-calling-API-via-powershell/","excerpt":"","text":"Recently we need to fetch a big dataset from an API via powershell, then import to Azure Data Explorer (ADX). Problem12#Used Measure-Command for measuring performanceMeasure-Command &#123;curl &#x27;THE_API_END_POINT&#x27; | select -expand Content &gt; data.json&#125; The data.json file looks perfectly fine, but during import to ADX, it reported error “invalid json format”. Troubleshooting Using online validation tool such as https://jsonlint.com/, copy &amp; paste the content from data.json. The json objects are valid. Using local tool jsonlint, reports error. It shows the data.json file has encoding issue. 123456789101112131415PS C:\\Users\\lufeng\\Desktop&gt; jsonlint .\\data.jsonError: Parse error on line 1:��[ &#123; &quot; _ i d &quot; : &#123;^Expecting &#x27;STRING&#x27;, &#x27;NUMBER&#x27;, &#x27;NULL&#x27;, &#x27;TRUE&#x27;, &#x27;FALSE&#x27;, &#x27;&#123;&#x27;, &#x27;[&#x27;, got &#x27;undefined&#x27; at Object.parseError (C:\\Users\\lufeng\\AppData\\Roaming\\npm\\node_modules\\jsonlint\\lib\\jsonlint.js:55:11) at Object.parse (C:\\Users\\lufeng\\AppData\\Roaming\\npm\\node_modules\\jsonlint\\lib\\jsonlint.js:132:22) at parse (C:\\Users\\lufeng\\AppData\\Roaming\\npm\\node_modules\\jsonlint\\lib\\cli.js:82:14) at main (C:\\Users\\lufeng\\AppData\\Roaming\\npm\\node_modules\\jsonlint\\lib\\cli.js:135:14) at Object.&lt;anonymous&gt; (C:\\Users\\lufeng\\AppData\\Roaming\\npm\\node_modules\\jsonlint\\lib\\cli.js:179:1) at Module._compile (internal/modules/cjs/loader.js:955:30) at Object.Module._extensions..js (internal/modules/cjs/loader.js:991:10) at Module.load (internal/modules/cjs/loader.js:811:32) at Function.Module._load (internal/modules/cjs/loader.js:723:14) at Function.Module.runMain (internal/modules/cjs/loader.js:1043:10) SolutionSwitch to a different powershell command solved the problem 12Invoke-WebRequest -Uri &#x27;THE_API_END_POINT&#x27; -OutFile data.json EOF","categories":[],"tags":[{"name":"Powershell","slug":"Powershell","permalink":"https://feng.lu/tags/Powershell/"},{"name":"Encoding","slug":"Encoding","permalink":"https://feng.lu/tags/Encoding/"},{"name":"Json","slug":"Json","permalink":"https://feng.lu/tags/Json/"}]},{"title":"How to Decrypt Native App's HTTPS Traffic (and Debug for In-app Browser)","slug":"How-to-Decrypt-Native-Apps-HTTPS-Traffic-and-Debug-for-In-app-Browser","date":"2020-04-03T18:10:18.000Z","updated":"2025-03-28T09:05:16.570Z","comments":true,"path":"2020/04/03/How-to-Decrypt-Native-Apps-HTTPS-Traffic-and-Debug-for-In-app-Browser/","permalink":"https://feng.lu/2020/04/03/How-to-Decrypt-Native-Apps-HTTPS-Traffic-and-Debug-for-In-app-Browser/","excerpt":"Problem with in-app browser of LinkedIn and Facebook iOS appsRecently our QA reported an interesting issue regarding the native app and our website: When the webpage was shared on Linkedin iOS App and&#x2F;or Facebook iOS App, the built-in browsers cannot show it correctly but a blank page. This issue only happens on some of the iOS apps (see the list below). Other iOS native apps have no problem. Safari and Chrome for iOS have no problem. All Android-based native apps have no problem. All desktop browsers have no problem.","text":"Problem with in-app browser of LinkedIn and Facebook iOS appsRecently our QA reported an interesting issue regarding the native app and our website: When the webpage was shared on Linkedin iOS App and&#x2F;or Facebook iOS App, the built-in browsers cannot show it correctly but a blank page. This issue only happens on some of the iOS apps (see the list below). Other iOS native apps have no problem. Safari and Chrome for iOS have no problem. All Android-based native apps have no problem. All desktop browsers have no problem. | Native App | Platform | Result ||——————– |———- |:——: || Linkedin | iOS | Not OK || Facebook | iOS | Not OK || Facebook messenger | iOS | Not OK || Slack | iOS | OK || Skype for Business | iOS | OK || Linkedin | Android | OK || Facebook | Android | OK || Facebook messenger | Android | OK || Slack | Android | OK || Skype for Business | Android | OK || Safari | iOS | OK || Chrome for iOS | iOS | OK || Any desktop browser | Win 10 | OK | So the problem is about iOS in-app browser in some native apps. But unfortunately these apps (LinkedIn and Facebook) are too important to ignore, so we will have to fix it. Possible ways for troubleshootingIt is challenging to debug this issue, as it only happens in some of the iOS apps. It can not be reproduced in Safari or other browsers. Possible approaches are: Reach out to Linkedin or Facebook, ask for what web viewer they’re using in the app. Search on the internet and hope there is a solution for it. Let’s become a hacker: Perform a Man-in-the-middle attack between apps and the internet, and to decrypt and manipulate the web traffic of Apps as troubleshooting. The #1 and #2 are long shots, then I will continue with approach #3. The following diagram shows the architecture. Let’s decrypt the web traffic of a native app with FiddlerThere some many ways to place a “Man-in-the-middle” between mobile and internet. For example, the famous fiddler can do it. Follow the documentation https://docs.telerik.com/fiddler/Configure-Fiddler/Tasks/ConfigureForiOS. The key steps are: Follow the instruction to install and configure fiddler on your PC. Ensure the phone and PC are in the sane network (e.g. same wifi), so your phone can access the PC. Turn off cellphone data on the phone, to sure traffic from the phone always go through the PC. From the phone, access http://FiddlerMachineIP:8888 with safari. (Chrome does not support download and install profile) If your phone can not reach the url on PC, ensure the firewall is turned off on your PC. Download FiddlerRoot certificate, then install it via Settings -&gt; Profile Downloaded. On iOS 10 and later, after installing the FiddlerRoot certificate, go to Settings -&gt; General -&gt; About -&gt; Certificate Trust Settings and manually enable full trust for the FiddlerRoot root certificate. Config proxy on your phone as in the fiddler documentation. Done, you should be able to see HTTP and HTTPS traffics from the apps now. The blank page issue is caused by incorrect Content-Security-PolicyNow I have started comparing the HTTPS response for the same URL but from different Apps, and quickly narrowed down the cause to the different values in response header The Content-Security-Policy (CSP). Content-Security-Policy in the App that have the problem 1234567891011121314#Added line break for better readabilityContent-Security-Policy: script-src az416426.vo.msecnd.netveracitycdn.azureedge.net &#x27;unsafe-inline&#x27; https://tagmanager.google.comhttps://www.googletagmanager.com www.google-analytics.comsjs.bizographics.com/insight.min.js https://px.ads.linkedin.com/https://*.hotjar.com https://*.hotjar.io; connect-srcdc.services.visualstudio.com https://*.hotjar.com:* https://*.hotjar.io;frame-src https://*.hotjar.com https://*.hotjar.iohttps://www.googletagmanager.com/ns.html; img-src www.google-analytics.comstats.g.doubleclick.net ssl.gstatic.com www.gstatic.comhttps://px.ads.linkedin.com/ www.google.no www.google.com px.ads.linkedin.comwww.linkedin.com; font-src data: fonts.gstatic.com; style-srctagmanager.google.com fonts.googleapis.com&#x27;sha256-SvLgADqEePEV9RNxBrRQXSBJafFHcVNG7cPzHz6h9eA=&#x27; Content-Security-Policy in the Apps that do NOT have the problem 1234567891011121314151617181920212223242526272829303132#Added line break for better readabilityContent-Security-Policy: default-src &#x27;self&#x27; veracitystatic.azureedge.netveracitycdn.azureedge.net veracity-cdn.azureedge.netveracity-static.azureedge.net veracity.azureedge.net; style-src &#x27;self&#x27;&#x27;sha256-UTjtaAWWTyzFjRKbltk24jHijlTbP20C1GUYaWPqg7E=&#x27; tagmanager.google.comfonts.googleapis.com &#x27;sha256-SvLgADqEePEV9RNxBrRQXSBJafFHcVNG7cPzHz6h9eA=&#x27;;img-src &#x27;self&#x27; data: veracityprod.blob.core.windows.netveracitycdn.azureedge.net veracitystatic.azureedge.netveracity-cdn.azureedge.net veracity-static.azureedge.netveracitytest.azureedge.net veracity.azureedge.net brandcentral.dnvgl.comdevtestdevprofile.blob.core.windows.net testdevprofile.blob.core.windows.netstagdevprofile.blob.core.windows.net cdn.sanity.iodevprofile.blob.core.windows.net www.google-analytics.comstats.g.doubleclick.net ssl.gstatic.com www.gstatic.comhttps://px.ads.linkedin.com/ www.google.no www.google.com px.ads.linkedin.comwww.linkedin.com; script-src &#x27;self&#x27; veracitycdn.azureedge.netveracity.azureedge.net https://localhost:3010 az416426.vo.msecnd.net&#x27;unsafe-inline&#x27; https://tagmanager.google.com https://www.googletagmanager.comwww.google-analytics.com sjs.bizographics.com/insight.min.jshttps://px.ads.linkedin.com/ https://*.hotjar.com https://*.hotjar.io;media-src &#x27;self&#x27; veracityprod.blob.core.windows.netveracitystatic.azureedge.net veracitycdn.azureedge.netveracity-cdn.azureedge.net veracity-static.azureedge.net veracity.azureedge.netcdn.sanity.io brandcentral.dnvgl.com; connect-src &#x27;self&#x27;veracitystatic.azureedge.net veracitycdn.azureedge.netveracity-cdn.azureedge.net veracity-static.azureedge.net veracity.azureedge.netcdn.sanity.io wss://localhost:3011 dc.services.visualstudio.comhttps://*.hotjar.com:* https://*.hotjar.io; font-src veracitycdn.azureedge.netdata: fonts.gstatic.com; report-urihttps://veracitycommon.report-uri.com/r/d/csp/enforce; report-tohttps://veracitycommon.report-uri.com/a/d/g; frame-src https://*.hotjar.comhttps://*.hotjar.io https://www.googletagmanager.com/ns.html It is pretty clear that due to the incorrect (much shorter) value of Content-Security-Policy caused the problem. Some User-Agent caused incorrect Content-Security-Policy (CSP)Now we need to check what caused the different CSP values. By comparing the requests that these apps were sending in Fiddler, I have quickly identified the request header “User-Agent” is the key. User-Agent values from Apps that cause wrong CSP 12345678#LinkedinMozilla/5.0 (iPhone; CPU iPhone OS 13_3_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148 [LinkedInApp]#FacebookMozilla/5.0 (iPhone; CPU iPhone OS 13_3_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148 [FBAN/FBIOS;FBDV/iPhone11,2;FBMD/iPhone;FBSN/iOS;FBSV/13.3.1;FBSS/3;FBID/phone;FBLC/en_US;FBOP/5;FBCR/Telenor]#Facebook MessengerMozilla/5.0 (iPhone; CPU iPhone OS 13_3_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148 LightSpeed [FBAN/MessengerLiteForiOS;FBAV/256.0.1.26.113;FBBV/203261359;FBDV/iPhone11,2;FBMD/iPhone;FBSN/iOS;FBSV/13.3.1;FBSS/3;FBCR/;FBID/phone;FBLC/en_NO;FBOP/0] User-Agent values from Apps that cause correct CSP 12345#SlackMozilla/5.0 (iPhone; CPU iPhone OS 13_3_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.5 Mobile/15E148 Safari/604.1#Skype for BusinessMozilla/5.0 (iPhone; CPU iPhone OS 13_3_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.5 Mobile/15E148 Safari/604.1 Manipulate web traffic of the apps to simulate different behaviorsAlthough we cannot change the logic of these apps, we can still easily manipulate the request or response, to simulate the different behaviors. Head to Fiddler, go to “Filters” table, then you can Setup the filter to target the manipulation to a specific page Manipulate requests, such as add&#x2F;update&#x2F;remove request headers and body Manipulate responses, such as headers and body Some findings are: “[…]” part in the user-agent does NOT cause the problem, even though they are quite long strings Missing the “Version&#x2F;13.0.5” part is causing the problem | Original LinkedIn User-Agent (with issue) | Updated LinkedIn User-Agent (without issue) ||——————————————————————————————————————————— |————————————————————————————————————————————————- || Mozilla&#x2F;5.0 (iPhone; CPU iPhone OS 13_3_1 like Mac OS X) AppleWebKit&#x2F;605.1.15 (KHTML, like Gecko) Mobile&#x2F;15E148 [LinkedInApp] | Mozilla&#x2F;5.0 (iPhone; CPU iPhone OS 13_3_1 like Mac OS X) AppleWebKit&#x2F;605.1.15 (KHTML, like Gecko) Mobile&#x2F;15E148 Version&#x2F;13.0.5 [LinkedInApp] | Root cause and solutionGenerally, the web site should return the same CSP for most of the cases. So this is an issue that we should fix on the website. The investigation led us to the opensource library Helmet where we reported a bug https://github.com/helmetjs/csp/issues/105. Now we have fixed this issue locally and once Helmet merged the PR, we are ready to go. Take away It is easy to perform a “Man-in-the-middle” like attack, but ONLY IF you have control on the device (e.g. you can install the root certificate). Once you have a machine between mobile and internet, you can not only monitor the web traffic, but also manipulate both request and response. So if you are interested in how an app is talking to its backend, you have tools to do that. Advice to normal app user: be careful who have access to your device and try to stay away from the public wifi. Advice to app developers: always remind yourself that technically it is possible for a hacker to “open your app” and look at which API endpoint your app is talking to, and manipulate the requests. Your app’s private API endpoint will be exposed, take all necessray security measurement on it! References Establish the proxy https://docs.telerik.com/fiddler/Configure-Fiddler/Tasks/ConfigureForiOS Manipulate the request header https://hackernoon.com/manipulating-web-application-http-traffic-with-fiddler-140d789d0a1c","categories":[],"tags":[{"name":"Troubleshooting","slug":"Troubleshooting","permalink":"https://feng.lu/tags/Troubleshooting/"},{"name":"Fiddler","slug":"Fiddler","permalink":"https://feng.lu/tags/Fiddler/"},{"name":"Native App","slug":"Native-App","permalink":"https://feng.lu/tags/Native-App/"},{"name":"HTTPS","slug":"HTTPS","permalink":"https://feng.lu/tags/HTTPS/"},{"name":"Decryption","slug":"Decryption","permalink":"https://feng.lu/tags/Decryption/"}]},{"title":"Jump-start Kubernetes and Istio with Docker Desktop on Windows 10","slug":"Jump-start-Kubernetes-with-Docker-Desktop-on-Windows-10","date":"2019-10-05T17:57:31.000Z","updated":"2025-03-28T09:05:16.831Z","comments":true,"path":"2019/10/05/Jump-start-Kubernetes-with-Docker-Desktop-on-Windows-10/","permalink":"https://feng.lu/2019/10/05/Jump-start-Kubernetes-with-Docker-Desktop-on-Windows-10/","excerpt":"Here we will setup a single-node Kubernetes cluster on a windows 10 PC (In my case it is a surface 5 with 16GB RAM). If you are new to docker, feel free to check out Jump-start with docker.We are going to setup: A single-node Kubernetes cluster Kubernetes dashboard Helm Isito (service mesh, including Kiali) Deployment samples","text":"Here we will setup a single-node Kubernetes cluster on a windows 10 PC (In my case it is a surface 5 with 16GB RAM). If you are new to docker, feel free to check out Jump-start with docker.We are going to setup: A single-node Kubernetes cluster Kubernetes dashboard Helm Isito (service mesh, including Kiali) Deployment samples 1. Enable Kubernetes in Docker DesktopDocker Desktop (or Docker for Windows) is a nice environment for developers on Windows. The community stable version of Docker Desktop is good enough for this jump-start, just make sure the version you installed include Kubernetes 1.14.x or higher. (I am using Docker Desktop Community 2.1.0.3). Once installed, you can enable Kubernetes in Setting (see detailed info at here) Then, you can verify it by running “kubectl version“ in powershell (or Command window) In my case, I got error while connecting to [::1]:8080: 1234PS C:\\&gt; kubectl version#Output:Client Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;14&quot;, GitVersion:&quot;v1.14.3&quot;, GitCommit:&quot;5e53fd6bc17c0dec8434817e69b04a25d8ae0ff0&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2019-06-06T01:44:30Z&quot;, GoVersion:&quot;go1.12.5&quot;, Compiler:&quot;gc&quot;, Platform:&quot;windows/amd64&quot;&#125;Unable to connect to the server: dial tcp [::1]:8080: connectex: No connection could be made because the target machine actively refused it. This is because I am missing an environment variable “KUBECONFIG“. Set this variable to your user directory such as “C:\\Users\\YOUR__USER_NAME\\.kube\\config“. After adding this and restart your powershell, it should work. 12345678910111213141516171819PS C:\\&gt; Get-Item -Path Env:KUBECONFIG#Output:Name Value---- -----KUBECONFIG C:\\Users\\lufeng\\.kube\\configPS C:\\&gt; kubectl version#Output:Client Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;14&quot;, GitVersion:&quot;v1.14.3&quot;, GitCommit:&quot;5e53fd6bc17c0dec8434817e69b04a25d8ae0ff0&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2019-06-06T01:44:30Z&quot;, GoVersion:&quot;go1.12.5&quot;, Compiler:&quot;gc&quot;, Platform:&quot;windows/amd64&quot;&#125;Server Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;14&quot;, GitVersion:&quot;v1.14.3&quot;, GitCommit:&quot;5e53fd6bc17c0dec8434817e69b04a25d8ae0ff0&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2019-06-06T01:36:19Z&quot;, GoVersion:&quot;go1.12.5&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125;PS C:\\&gt; kubectl get namespaces#Output:NAME STATUS AGEdefault Active 18hdocker Active 18hkube-node-lease Active 18hkube-public Active 18hkube-system Active 18h 2. Installing Kubernetes DashboardIt is always nice to have a GUI for a complicated system such as Kubernetes, so lets install the dashboard https://github.com/kubernetes/dashboard. 2.1 Dashboard deployment12345678PS C:\\&gt; kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml#Output:secret/kubernetes-dashboard-certs createdserviceaccount/kubernetes-dashboard createdrole.rbac.authorization.k8s.io/kubernetes-dashboard-minimal createdrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard-minimal createddeployment.apps/kubernetes-dashboard createdservice/kubernetes-dashboard created 2.2 Accessing the dashboardFirst of all, we need to enable the proxy, so you can access the dashboard from your localhost: 123PS C:\\&gt; kubectl proxy#Output:Starting to serve on 127.0.0.1:8001 Once the proxy is up and running, visit the dashboard URL: http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/ Normally you will meet this [login view] (https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/README.md#login-view) You can find more info from the dashboard github about Access control, but here we will do it simpler (This is for demo purpose, do not apply the same setup in your production environment). 2.2.1 Get tokenGet the default token name 1234PS C:\\&gt; kubectl get secrets#Output:NAME TYPE DATA AGEdefault-token-n92hz kubernetes.io/service-account-token 3 18h Then get the token 123456789101112131415PS C:\\&gt; kubectl describe secrets default-token-n92hz#Output:Name: default-token-n92hzNamespace: defaultLabels: &lt;none&gt;Annotations: kubernetes.io/service-account.name: default kubernetes.io/service-account.uid: c56ad00e-e5e5-11e9-91a0-00155d3a9005Type: kubernetes.io/service-account-tokenData====ca.crt: 1025 bytesnamespace: 7 bytestoken: eyJhbGciOiJSUzI1NiIsImt3NlcnZpY2UtYWNjb......CIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjfv4TPDVZoOrLWHZecEw-8XBQPS C:\\&gt; Use the token in the login form, then you are in. 3. Installing Helm on WindowsHelm is a tool for managing Kubernetes charts. Charts are packages of pre-configured Kubernetes resources. You can read more at https://helm.sh/.According to the installation guide, we are going to: Install scoop Install helm via scoop 1PS C:\\&gt; scoop install helm Ensure configure the environment variable for “HELM_HOME“, such as “C:\\Users\\USERNAME.kube”. It should be an valid directory in your file system. 12345PS C:\\&gt; Get-Item -Path Env:HELM_HOME#Output:Name Value---- -----HELM_HOME C:\\Users\\lufeng\\.kube Initialize Helm and install TillerOnce you have Helm ready, you can initialize the local CLI and also install Tiller into your Kubernetes cluster in one step: 123456789101112131415161718192021222324#Check current kubernetes cluster contextPS P:\\&gt; kubectl config current-context#Output:docker-desktop#Init helmPS C:\\&gt; helm init --history-max 200#Output:$HELM_HOME has been configured at C:\\Users\\lufeng\\.kube.Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster.#Verify the triller is up and running (the last row)PS C:\\&gt; kubectl get pods --namespace kube-system#Output:NAME READY STATUS RESTARTS AGEcoredns-fb8b8dccf-b5lq5 1/1 Running 0 19hcoredns-fb8b8dccf-t5kdf 1/1 Running 0 19hetcd-docker-desktop 1/1 Running 0 19hkube-apiserver-docker-desktop 1/1 Running 0 19hkube-controller-manager-docker-desktop 1/1 Running 0 19hkube-proxy-bj2x4 1/1 Running 0 19hkube-scheduler-docker-desktop 1/1 Running 0 19hkubernetes-dashboard-5f7b999d65-vqdq6 1/1 Running 0 19htiller-deploy-5454fb964d-8tp5t 1/1 Running 0 76s 4. Installing IstioIstio is a microservice-mesh management framework, that provides traffic management, policy enforcement, and telemetry collection.We are going to: Install Istio (and addons such as Kiali) via Helm (doc) Accessing Kiali dashboard (doc) Install bookinfo demo (doc) 4.1 Install Istio via HelmSimply follow the steps in https://istio.io/docs/setup/install/helm/, remember to config docker desktop as mentioned. Unzip the downloaded package into “c:\\Istio“ as we might want to update some files there. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556PS C:\\&gt; helm repo add istio.io https://storage.googleapis.com/istio-release/releases/1.3.1/charts/#Output:&quot;istio.io&quot; has been added to your repositories#Use Helm’s Tiller pod to manage Istio release (option 2), as we installed Tiller in previous step.PS C:\\&gt; cd istio#1. Make sure you have a service account with the cluster-admin role defined for Tiller. If not already defined, create one using following commandPS C:\\istio&gt; kubectl apply -f install/kubernetes/helm/helm-service-account.yaml#Output:serviceaccount/tiller createdclusterrolebinding.rbac.authorization.k8s.io/tiller created#2. Config Tiller on your cluster with the service account:PS C:\\istio&gt; helm init --upgrade --service-account tiller#Output:$HELM_HOME has been configured at C:\\Users\\lufeng\\.kube.Tiller (the Helm server-side component) has been upgraded to the current version.#3. Install the istio-init chart to bootstrap all the Istio’s CRDs:PS C:\\istio&gt; helm install install/kubernetes/helm/istio-init --name istio-init --namespace istio-system#Output:NAME: istio-initLAST DEPLOYED: Fri Oct 4 11:36:15 2019NAMESPACE: istio-systemSTATUS: DEPLOYEDRESOURCES:==&gt; v1/ClusterRoleNAME AGEistio-init-istio-system 0s==&gt; v1/ClusterRoleBindingNAME AGEistio-init-admin-role-binding-istio-system 0s==&gt; v1/ConfigMapNAME DATA AGEistio-crd-10 1 0sistio-crd-11 1 0sistio-crd-12 1 0s==&gt; v1/JobNAME COMPLETIONS DURATION AGEistio-init-crd-10-1.3.1 0/1 0sistio-init-crd-11-1.3.1 0/1 0s 0sistio-init-crd-12-1.3.1 0/1 0s 0s==&gt; v1/Pod(related)NAME READY STATUS RESTARTS AGEistio-init-crd-11-1.3.1-qz4fh 0/1 ContainerCreating 0 0sistio-init-crd-12-1.3.1-6rk5w 0/1 ContainerCreating 0 0s==&gt; v1/ServiceAccountNAME SECRETS AGEistio-init-service-account 1 0s Then select a configuration profile. We go with “demo“ as it include some nice addons such as Kiali. 12345678910111213141516171819202122#InstallationPS C:\\istio&gt; helm install install/kubernetes/helm/istio --name istio --namespace istio-system --values install/kubernetes/helm/istio/values-istio-demo.yaml#VerifyPS C:\\istio&gt; kubectl get pods -n istio-system#Output:NAME READY STATUS RESTARTS AGEgrafana-6fc987bd95-zj4kn 1/1 Running 0 98sistio-citadel-55646d8965-wvflc 1/1 Running 0 97sistio-egressgateway-7bdb7bf7b5-ck4k6 1/1 Running 0 98sistio-galley-56bf6b7497-c9szw 1/1 Running 0 98sistio-ingressgateway-64dbd4b954-64gj8 1/1 Running 0 98sistio-init-crd-10-1.3.1-tvnr4 0/1 Completed 0 4h1mistio-init-crd-11-1.3.1-qz4fh 0/1 Completed 0 4h1mistio-init-crd-12-1.3.1-6rk5w 0/1 Completed 0 4h1mistio-pilot-5d4c86d576-crn2k 2/2 Running 0 97sistio-policy-759d4988df-c7tnb 2/2 Running 1 97sistio-sidecar-injector-5d6ff6d758-8tlrx 1/1 Running 0 97sistio-telemetry-7c88764b9c-245mk 2/2 Running 1 97sistio-tracing-669fd4b9f8-gmlh9 1/1 Running 0 97skiali-94f8cbd99-zwz8z 1/1 Running 0 98sprometheus-776fdf7479-jwnvh 1/1 Running 0 97s You can also verify these pod via dashboard 4.2 Accessing Kiali dashboardAs we installed the Demo configuration profile of Istio, Kiali was also installed. Kiali is an observability console for Istio with service mesh configuration capabilities. (Read more at https://istio.io/docs/tasks/telemetry/kiali/ also) To open Kiali UI, pls run 1234PS C:\\istio&gt; kubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=kiali -o jsonpath=&#x27;&#123;.items[0].metadata.name&#125;&#x27;) 20001:20001#Output:Forwarding from 127.0.0.1:20001 -&gt; 20001Forwarding from [::1]:20001 -&gt; 20001 Then go to http://localhost:20001 for visting Kiali UI. Again, it ask for login. As in this case, Kiali was installed as a part of the Demo configuration profile, you can use default user name “admin“ and password “admin“ to login. 4.3 Install bookinfo demoNow, lets deploy a demo application composed of four separate microservices. The detailed doc can be found at https://istio.io/docs/examples/bookinfo/. Start the application services 123456789101112131415161718192021222324252627282930313233343536373839404142434445#1. Set automatic sidecar injectionPS C:\\istio&gt; kubectl label namespace default istio-injection=enabled#2. DeploymentPS C:\\istio&gt; kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml#Output:service/details createdserviceaccount/bookinfo-details createddeployment.apps/details-v1 createdservice/ratings createdserviceaccount/bookinfo-ratings createddeployment.apps/ratings-v1 createdservice/reviews createdserviceaccount/bookinfo-reviews createddeployment.apps/reviews-v1 createddeployment.apps/reviews-v2 createddeployment.apps/reviews-v3 createdservice/productpage createdserviceaccount/bookinfo-productpage createddeployment.apps/productpage-v1 created#3. Verify services and podsPS C:\\istio&gt; kubectl get services#Output:NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEdetails ClusterIP 10.110.165.24 &lt;none&gt; 9080/TCP 33skubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 6h50mproductpage ClusterIP 10.97.123.119 &lt;none&gt; 9080/TCP 32sratings ClusterIP 10.111.216.40 &lt;none&gt; 9080/TCP 33sreviews ClusterIP 10.109.244.28 &lt;none&gt; 9080/TCP 33sPS C:\\istio&gt; kubectl get pods#Output:NAME READY STATUS RESTARTS AGEdetails-v1-c5b5f496d-sgr6w 2/2 Running 0 85sproductpage-v1-c7765c886-6cpr9 2/2 Running 0 83sratings-v1-f745cf57b-87m7q 2/2 Running 0 85sreviews-v1-75b979578c-vmzn2 2/2 Running 0 84sreviews-v2-597bf96c8f-plml7 2/2 Running 0 85sreviews-v3-54c6c64795-x67ss 2/2 Running 0 84s#4. Verify by calling the applicationPS C:\\istio&gt; kubectl exec -it $(kubectl get pod -l app=ratings -o jsonpath=&#x27;&#123;.items[0].metadata.name&#125;&#x27;) -c ratings -- curl productpage:9080/productpage | select-string -pattern &quot;&lt;title&gt;&quot;#Output: &lt;title&gt;Simple Bookstore App&lt;/title&gt; Establish gateway for the bookinfo app 1234567891011#1. Apply gatewayPS C:\\istio&gt; kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml#Output:gateway.networking.istio.io/bookinfo-gateway createdvirtualservice.networking.istio.io/bookinfo created#2. Verify the gatewayPS C:\\istio&gt; kubectl get gateway#Output:NAME AGEbookinfo-gateway 38s Confirm the app is accessible from outside the clusterGo to http://localhost/productpage to verify you can open the page. You can refresh the page several times for generating telemtries. Kiali VisualizationAssuming the 20001 port forwarding is still running, then you can visualize the service relationship in Kiali http://localhost:20001/ 5. Deployment examplesLet’s deploy a single-container-application (Grafana) to the cluster, which is described at https://grafana.com/docs/installation/docker/ 1. Docker version 1docker run -d -p 3000:3000 grafana/grafana 2. Kubernetes kubectl command version 12345678910111213141516171819202122232425# 1. DeploymentPS C:\\&gt; kubectl run grafana-test --generator=run-pod/v1 --image=grafana/grafana --port=3000#Output:pod/grafana-test created# 2. Check the name of the grafana pod. Note it is sitting in &quot;default&quot; namespacePS C:\\&gt; kubectl -n default get pod#Output:NAME READY STATUS RESTARTS AGEdetails-v1-c5b5f496d-sgr6w 2/2 Running 0 29hgrafana-test 2/2 Running 0 97skubernetes-bootcamp-b94cb9bff-vsprh 2/2 Running 0 3h6mproductpage-v1-c7765c886-6cpr9 2/2 Running 0 29hratings-v1-f745cf57b-87m7q 2/2 Running 0 29hreviews-v1-75b979578c-vmzn2 2/2 Running 0 29hreviews-v2-597bf96c8f-plml7 2/2 Running 0 29hreviews-v3-54c6c64795-x67ss 2/2 Running 0 29h# 4. Enable port forwarding.# In case you wanna use select as the pod name contains random string, # Use &quot;kubectl -n default port-forward $(kubectl -n default get pod -l run=grafana-test -o jsonpath=&#x27;&#123;.items[0].metadata.name&#125;&#x27;) 3000:3000&quot;PS C:\\&gt; kubectl -n default port-forward grafana-test 3000:3000#Output:Forwarding from 127.0.0.1:3000 -&gt; 3000Forwarding from [::1]:3000 -&gt; 3000 3. Kubernetes YAML deployment versionIt is recommended to use YAML file for defining a deployment. See doc at https://kubernetes.io/docs/concepts/workloads/controllers/deployment/Create a deployment grafana-deployment.yaml file as below: 123456789101112131415161718192021apiVersion: apps/v1kind: Deploymentmetadata: name: grafana-yaml-deployment labels: app: grafana-yamlspec: replicas: 1 selector: matchLabels: app: grafana-yaml template: metadata: labels: app: grafana-yaml spec: containers: - name: grafana-yaml image: grafana/grafana ports: - containerPort: 3000 Then apply the yaml file and run 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#1. DeploymentPS C:\\&gt; kubectl apply -f .\\grafana-deployment.yaml#Output:deployment.apps/grafana-yaml-deployment created#2. VerifyPS C:\\&gt; kubectl get deployments#Output:NAME READY UP-TO-DATE AVAILABLE AGEdetails-v1 1/1 1 1 29hgrafana-yaml-deployment 1/1 1 1 40skubernetes-bootcamp 1/1 1 1 3h27mproductpage-v1 1/1 1 1 29hratings-v1 1/1 1 1 29hreviews-v1 1/1 1 1 29hreviews-v2 1/1 1 1 29hreviews-v3 1/1 1 1 29h#3. Enable forward port, by using selector app=grafana-yamlPS C:\\&gt; kubectl -n default port-forward $(kubectl -n default get pod -l app=grafana-yaml -o jsonpath=&#x27;&#123;.items[0].metadata.name&#125;&#x27;) 3000:3000#4. Expose the service via nodeportPS C:\\&gt; kubectl expose deployment grafana-yaml-deployment --type=NodePort --port=3000#Output:service/grafana-yaml-deployment exposed#5. Get the external ip and portPS C:\\&gt; kubectl get services#Output:NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEdetails ClusterIP 10.110.165.24 &lt;none&gt; 9080/TCP 3d8hgrafana-yaml-deployment NodePort 10.98.52.86 &lt;none&gt; 3000:30857/TCP 9skubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 3d15hproductpage ClusterIP 10.97.123.119 &lt;none&gt; 9080/TCP 3d8hratings ClusterIP 10.111.216.40 &lt;none&gt; 9080/TCP 3d8hreviews ClusterIP 10.109.244.28 &lt;none&gt; 9080/TCP 3d8hPS C:\\&gt; kubectl describe service grafana-yaml-deploymentName: grafana-yaml-deploymentNamespace: defaultLabels: app=grafana-yamlAnnotations: &lt;none&gt;Selector: app=grafana-yamlType: NodePortIP: 10.98.52.86LoadBalancer Ingress: localhostPort: &lt;unset&gt; 3000/TCPTargetPort: 3000/TCPNodePort: &lt;unset&gt; 30857/TCPEndpoints: 10.1.0.208:3000Session Affinity: NoneExternal Traffic Policy: ClusterEvents: &lt;none&gt; Then you can access grafana pod via http://localhost:30857 6. SummaryNow, you should have a kubernetes environment up and running, together with Istio and Kiali enabled. It can be used as your sandbox, for developing and testing your applications in Kubernetes. With Istio and Kiali, you can also play with service mesh. Everything is running locally in “one box”, so you do not need to worry about any cloud running cost. Have fun.","categories":[],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://feng.lu/tags/Docker/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://feng.lu/tags/Kubernetes/"},{"name":"Istio","slug":"Istio","permalink":"https://feng.lu/tags/Istio/"},{"name":"Service mesh","slug":"Service-mesh","permalink":"https://feng.lu/tags/Service-mesh/"},{"name":"Kiali","slug":"Kiali","permalink":"https://feng.lu/tags/Kiali/"},{"name":"Jump-start","slug":"Jump-start","permalink":"https://feng.lu/tags/Jump-start/"}]},{"title":"Customize social sharing on Linkedin via API","slug":"Customize-social-sharing-on-Linkedin-via-API","date":"2019-02-06T19:34:51.000Z","updated":"2025-03-28T09:05:16.529Z","comments":true,"path":"2019/02/06/Customize-social-sharing-on-Linkedin-via-API/","permalink":"https://feng.lu/2019/02/06/Customize-social-sharing-on-Linkedin-via-API/","excerpt":"(edited 10.06.2020: Updated how to get User ID as LinkedIn upgraded their endpoints) Problem:Nowadays it is pretty common to share articles on social media such as Facebook and Linkedin. Thanks to the widely implemented Open Graph protocol, sharing is no long just a dry url, but with enrich text and thumbnails. However, there are still some web pages that do not have Open Graph implemented, which significantly reduces the readers’ willingness for clicking it. In addition, even you introduced the Open Graph tags as a hotfix, some times you will have wait for approximately 7 days for linkedin crawler to refresh the preview caching, as mentioned in linkedin documentation: The first time that LinkedIn’s crawlers visit a webpage when asked to share content via a URL, the data it finds (Open Graph values or our own analysis) will be cached for a period of approximately 7 days.This means that if you subsequently change the article’s description, upload a new image, fix a typo in the title, etc., you will not see the change represented during any subsequent attempts to share the page until the cache has expired and the crawler is forced to revisit the page to retrieve fresh content. Some solutions are here and here, but they are more like a workaround. Solution:We can overcome this issue by using linkedin API, which provide huge flexibility for customizing the sharing experiences.","text":"(edited 10.06.2020: Updated how to get User ID as LinkedIn upgraded their endpoints) Problem:Nowadays it is pretty common to share articles on social media such as Facebook and Linkedin. Thanks to the widely implemented Open Graph protocol, sharing is no long just a dry url, but with enrich text and thumbnails. However, there are still some web pages that do not have Open Graph implemented, which significantly reduces the readers’ willingness for clicking it. In addition, even you introduced the Open Graph tags as a hotfix, some times you will have wait for approximately 7 days for linkedin crawler to refresh the preview caching, as mentioned in linkedin documentation: The first time that LinkedIn’s crawlers visit a webpage when asked to share content via a URL, the data it finds (Open Graph values or our own analysis) will be cached for a period of approximately 7 days.This means that if you subsequently change the article’s description, upload a new image, fix a typo in the title, etc., you will not see the change represented during any subsequent attempts to share the page until the cache has expired and the crawler is forced to revisit the page to retrieve fresh content. Some solutions are here and here, but they are more like a workaround. Solution:We can overcome this issue by using linkedin API, which provide huge flexibility for customizing the sharing experiences. 1. Create an application in LinkedinHead to https://www.linkedin.com/developers/ and create an application. As showed in the screenshot, I created an application named “Linkedin Poster”. Take notes on Client ID and Client Secret, set the Redirect URLs as https://www.getpostman.com/oauth2/callback. 2. Generate OAuth token in PostmanUse postman application to generate OAuth 2.0 token (Authorization Code Flow). The detailed documentation is here. Auth URL: https://www.linkedin.com/oauth/v2/authorization Access Token URL: https://www.linkedin.com/oauth/v2/accessToken Call back: https://www.getpostman.com/oauth2/callback (same as in the linkedin app setting) Scope: “r_emailaddress r_liteprofile w_member_social” (need “w_member_social” as we need to post) Login to generate token 3. Get user id from linkedinIn order to post articles in LinkedIn via API, we need to provide the user id.Make a GET request to API https://api.linkedin.com/v2/me (see document), make sure the token from step 2 is included. The result is something like below: 1234567891011121314151617181920212223242526&#123; &quot;localizedLastName&quot;: &quot;Lu&quot;, &quot;profilePicture&quot;: &#123; &quot;displayImage&quot;: &quot;urn:li:digitalmediaAsset:BACABCqwPVej-w&quot; &#125;, &quot;firstName&quot;: &#123; &quot;localized&quot;: &#123; &quot;en_US&quot;: &quot;Feng&quot; &#125;, &quot;preferredLocale&quot;: &#123; &quot;country&quot;: &quot;US&quot;, &quot;language&quot;: &quot;en&quot; &#125; &#125;, &quot;lastName&quot;: &#123; &quot;localized&quot;: &#123; &quot;en_US&quot;: &quot;Lu&quot; &#125;, &quot;preferredLocale&quot;: &#123; &quot;country&quot;: &quot;US&quot;, &quot;language&quot;: &quot;en&quot; &#125; &#125;, &quot;id&quot;: &quot;ABC123-ab1&quot;, &quot;localizedFirstName&quot;: &quot;Feng&quot;&#125; 4. Customize your article sharing via APIRef to the documentation it is pretty straightforward for customizing the shared content. In my case, I would like to share http://feng.lu/archives/ (which does not have Open Graph) with a nice archive picture. POST to https://api.linkedin.com/v2/shares with body: 12345678910111213141516171819202122&#123; &quot;content&quot;: &#123; &quot;contentEntities&quot;: [ &#123; &quot;entityLocation&quot;: &quot;http://feng.lu/archives/&quot;, &quot;thumbnails&quot;: [ &#123; &quot;resolvedUrl&quot;: &quot;http://feng.lu/2019/02/06/Customize-social-sharing-on-Linkedin-via-API/archives.jpg&quot; &#125; ] &#125; ], &quot;title&quot;: &quot;Article archives of feng.lu&quot; &#125;, &quot;distribution&quot;: &#123; &quot;linkedInDistributionTarget&quot;: &#123;&#125; &#125;, &quot;owner&quot;: &quot;urn:li:person:MY_LINKEDIN_ID&quot;, &quot;text&quot;: &#123; &quot;text&quot;: &quot;Checkout my blog archives! Hopefully you will find it useful. :)&quot; &#125;&#125; Checkout the result: Conclusion:By using LinkedIn API, we can easily customize the sharing experience with your professional networks. It does not only overcome the challenges such as missing Open Graph implementation, but also can improve the social media campaign experience and better integration with CMS.","categories":[],"tags":[{"name":"Linkedin","slug":"Linkedin","permalink":"https://feng.lu/tags/Linkedin/"},{"name":"Social Sharing","slug":"Social-Sharing","permalink":"https://feng.lu/tags/Social-Sharing/"},{"name":"API","slug":"API","permalink":"https://feng.lu/tags/API/"},{"name":"OAuth","slug":"OAuth","permalink":"https://feng.lu/tags/OAuth/"},{"name":"Postman","slug":"Postman","permalink":"https://feng.lu/tags/Postman/"}]},{"title":"Data Integrity and Lineage by using DLT, Part 3","slug":"Data-Integrity-and-Lineage-by-using-DLT-Part-3","date":"2019-01-03T10:08:30.000Z","updated":"2025-03-28T10:26:55.649Z","comments":true,"path":"2019/01/03/Data-Integrity-and-Lineage-by-using-DLT-Part-3/","permalink":"https://feng.lu/2019/01/03/Data-Integrity-and-Lineage-by-using-DLT-Part-3/","excerpt":"Other articles in this series: Part 1 Part 2 Part 3 (this article) RecapIn the second part of this series, we have went though both the detailed technical design that is based on IOTA. Some quick recap are: Use MAM protocol for interacting with IOTA Tangle. Defined the core data schema (4 mandatory fields: “dataPackageId”, “wayOfProof” , “valueOfProof” and “inputs”). Although the core data schema is quite easy to implement, companies and developers might meet some challenges to get started, such as: Developers need to build the knowledge of IOTA and MAM protocol. Need to build user interface for data lineage visualization. Companies most likely need to setup and maintain their dedicated infrastructure (Web server that runs IOTA server code, database, resource to perform Proof-of-Work, connection to neighbor nodes in IOTA network, etc), as public nodes from community are not stable. Data Lineage Service - an open source application to get you startedWe want to address above challenges, and help everyone to gain benefits of data integrity and lineage. Therefore, we have built “Data Lineage Service“ application. Developers and companies can apply this technology without deep understanding of IOTA and MAM protocol. It can be used either as a standalone application, or a microservice that integrates with existing systems. The key functions are: Wrapped IOTA MAM protocol to well-known HTTP(s) protocol as standard Restful API, with swagger definition. Developers do not need to worry about MAM protocol and it technical details, but focus on the normal data pipeline. Reusable web interface for lineage visualization. User-friendly interface for submitting data integrity and lineage information to DLT. Built-in functionalities for addressing common issues such as caching and monitoring. It is open-sourced on github with MIT license.","text":"Other articles in this series: Part 1 Part 2 Part 3 (this article) RecapIn the second part of this series, we have went though both the detailed technical design that is based on IOTA. Some quick recap are: Use MAM protocol for interacting with IOTA Tangle. Defined the core data schema (4 mandatory fields: “dataPackageId”, “wayOfProof” , “valueOfProof” and “inputs”). Although the core data schema is quite easy to implement, companies and developers might meet some challenges to get started, such as: Developers need to build the knowledge of IOTA and MAM protocol. Need to build user interface for data lineage visualization. Companies most likely need to setup and maintain their dedicated infrastructure (Web server that runs IOTA server code, database, resource to perform Proof-of-Work, connection to neighbor nodes in IOTA network, etc), as public nodes from community are not stable. Data Lineage Service - an open source application to get you startedWe want to address above challenges, and help everyone to gain benefits of data integrity and lineage. Therefore, we have built “Data Lineage Service“ application. Developers and companies can apply this technology without deep understanding of IOTA and MAM protocol. It can be used either as a standalone application, or a microservice that integrates with existing systems. The key functions are: Wrapped IOTA MAM protocol to well-known HTTP(s) protocol as standard Restful API, with swagger definition. Developers do not need to worry about MAM protocol and it technical details, but focus on the normal data pipeline. Reusable web interface for lineage visualization. User-friendly interface for submitting data integrity and lineage information to DLT. Built-in functionalities for addressing common issues such as caching and monitoring. It is open-sourced on github with MIT license. Also, for one who simply wanna try it out in the live environment, we are hosting this service that connects to the live DLT environment (IOTA tangle mainnet). As a live environment, it allows anyone to: Submit and receive integrity&#x2F;lineage information with the live IOTA tangle mainnet, without maintain his&#x2F;her own infrastructure. Outsource Proof-of-work (PoW) from clients to the service. Our host environment is taking care of the PoW on the server side. It helps IoT devices with low computing power (such as Raspberry PI) to submit information to DLT without consuming local resources. This also helps to improve the submission throughput (Number of submission per second). All functions can be done via either web browser or restful APIs. Zero cost for testing and building Proof-of-Concept applications with real-world DLT. Source code of Data-Lineage-ServiceThe source code is hosted in Github: https://github.com/veracity/data-lineage-service Live environmentThe live demo environment can be found at https://datalineage-viewer.azurewebsites.netThis live environment is backed by IOTA public network (public IOTA nodes). Feel free to use it (either GUI or API swagger) to store your integrity and lineage data into IOTA mainnet, as well as visualize the existing data. The API Swagger is at https://datalineage-viewer.azurewebsites.net/swagger/ Screenshot: Real World Demo: Real-time data integrity on IoT deviceBy using this service, an IoT device can ensure the integrity of its IoT data stream. As a demo, I have a raspberry pi with sense hat that is reporting temperature as well as saving the integrity information to DLT. The integrity information can be read at here from DLT. Therefore, the data consumer of this temperature sensor can be confident that: the temperature report (and the report timestamp) is not tampered it is indeed from this raspberry pi this data integrity information can be included into the downstream data lineage The source code of this demo is at https://github.com/linkcd/data-integrity-on-pi Performance testing and resultsFrom day 1, the performance of DLT is a known issue. By expanding this technology into the IoT and real-time data exchanging world, the performance can be a blocking issue. This is also the reason that we started look into IOTA in the beginning, hope its performance can meet the need. We have conducted the performance testing in 3 iterations: Using a public IOTA node Using our own self-hosted IOTA node Using self-hosted IOTA node but outsource the PoW to https://powsrv.io/ In each iteration, we tested performance both for reading and writing. The testing code is also open-sourced at github https://github.com/veracity/IOTA-MAM-performance-testing Test results (on 20.09.2018) Conclusion: Performance of reading is OK (0.5 second per read), as far as you have a stable IOTA node (either self-host or from a provider) Outsource the PoW to dedicated service providers such as powsrv.io can significantly improve the performance of writing, but the best result allows us to do about 15 transactions per minute. Next stepIn veracity we are researching and building Data Integrity and Lineage as a Service (DILAAS) to bring down the barriers for both data providers and data consumers. DILAAS offers: A cloud service for managing and exchanging data integrity and lineage information between parties. Standard HTTP(s) API, without building competence of backend DLT, such as MAM programming. It helps to reduce the development cost and boost the onboard progress. Visualization of data integrity and lineage information. Managed infrastructure that offers stable IOTA network accessibility. Seed&#x2F;Identity management, for properly managing the seeds&#x2F;identifies in the secure environment. Other articles in this series: Part 1 Part 2 Part 3 (this article)","categories":[],"tags":[{"name":"IoT","slug":"IoT","permalink":"https://feng.lu/tags/IoT/"},{"name":"IOTA","slug":"IOTA","permalink":"https://feng.lu/tags/IOTA/"},{"name":"Data Lineage","slug":"Data-Lineage","permalink":"https://feng.lu/tags/Data-Lineage/"},{"name":"Data Integrity","slug":"Data-Integrity","permalink":"https://feng.lu/tags/Data-Integrity/"},{"name":"DLT","slug":"DLT","permalink":"https://feng.lu/tags/DLT/"},{"name":"Distributed Ledger Technology","slug":"Distributed-Ledger-Technology","permalink":"https://feng.lu/tags/Distributed-Ledger-Technology/"},{"name":"Tangle","slug":"Tangle","permalink":"https://feng.lu/tags/Tangle/"}]},{"title":"Data Integrity and Lineage by using DLT, Part 2","slug":"Data-Integrity-and-Lineage-by-using-DLT-Part-2","date":"2018-10-03T18:14:40.000Z","updated":"2025-03-28T09:05:16.550Z","comments":true,"path":"2018/10/03/Data-Integrity-and-Lineage-by-using-DLT-Part-2/","permalink":"https://feng.lu/2018/10/03/Data-Integrity-and-Lineage-by-using-DLT-Part-2/","excerpt":"Other articles in this series: Part 1 Part 2 (this article) Part 3 In my previous article, we discussed different approaches for solving the data integrity and lineage challenges, and concluded that the “Hashing with DLT“ solution is the direction we will move forward. In this article, we will have deep dive into it. Please not that Veracity’s work on data integrity and data lineage is testing many technologies in parallel. We utilise and test proven centralized technologies as well as new distributed ledger technologies like Tangle and Blockchain. This article series uses the IOTA Tangle as the distributed ledger technology. The use cases described can be solved with other technologies. This article does not necessarily reflect the technologies used in Veracity production environments. Which DLT to select?As Veracity is part of an Open Industry Ecosystem we have focused our data integrity and data lineage work using public DLT and open sourced technologies. We believe that to succeed with providing transparency from the user to the origin of data many technology vendors must collaborate around common standards and technologies. The organizational setup and philosophies for some of the public distributed ledgers provides the right environment to learn and develop fast with an adaptive ecosystem.","text":"Other articles in this series: Part 1 Part 2 (this article) Part 3 In my previous article, we discussed different approaches for solving the data integrity and lineage challenges, and concluded that the “Hashing with DLT“ solution is the direction we will move forward. In this article, we will have deep dive into it. Please not that Veracity’s work on data integrity and data lineage is testing many technologies in parallel. We utilise and test proven centralized technologies as well as new distributed ledger technologies like Tangle and Blockchain. This article series uses the IOTA Tangle as the distributed ledger technology. The use cases described can be solved with other technologies. This article does not necessarily reflect the technologies used in Veracity production environments. Which DLT to select?As Veracity is part of an Open Industry Ecosystem we have focused our data integrity and data lineage work using public DLT and open sourced technologies. We believe that to succeed with providing transparency from the user to the origin of data many technology vendors must collaborate around common standards and technologies. The organizational setup and philosophies for some of the public distributed ledgers provides the right environment to learn and develop fast with an adaptive ecosystem. There are many public DLT platforms nowadays, but not all of them (such as Bitcoin and Ethereum) are suitable for Big Data or IoT scenarios, such as: We are tracking logical data entities (bits, files or data streams) instead of physical entities (coal, car parts or packages). The granularity of data has much more detailed scale in IoT and the Big Data world. One example is, tracking every single piece of coal from a carrier ship sounds crazy, but tracking every data signal from thousands of sensors from the very same ship is quite common. We need to use DLT to handle large volume of transactions within a short time period (e.g. send 1000 data points from one device to another device per minute) We need to use DLT to store large amount of data (e.g. data integrity information of thousands of sensors) High transaction fees will weaken the business case. IOTA - the selected DTL for exploringWe have been watching closely at the technology evolution of distributed ledgers and exploring different possibilities. Currently we are exploring IOTA, which is a new type of DLT that foundationally different from other blockchain-based technologies. The high-level comparison can be found at IOTA FAQs, question “How is IOTA different from Blockchain?”We decide to test our solution on top of IOTA, due to the following key features that IOTA offers: Promise of higher performance and scalability: Thanks to tangle data structure. Zero Transaction Fee: Machine to Machine micropayments. This way machines can pay each other for certain services and resources. This is not an article of introducing IOTA, but you can learn more from https://www.iota.org MAM protocol from IOTAIn addition, IOTA provides a protocol named Masked Authenticated Messaging (MAM) that easily fit into our solution. MAM provides an abstract data structure layer (channels) on top of regular transactions. In our solution, all read and write data into DLT (tangle) is around MAM channels. Check the article appendix for more resources of MAM. One person or application creates a private seed. The seed shall be considered as a private key, and not be shared with others. One seed can create one unique MAM channel in IOTA tangle that can store messages. Messages contain data such as json object. The private seed ensures that only the seed owner is authorized to write messages in the channel. Therefore the origin of the messages is trusted by others. Once the message was written into channel, the message is replicated and stored in all nodes in DLT. It means the message is immutable. Once you know a MAM address in the channel, you can go through all addresses (and fetch their messages) that follows the known address, such as root address -&gt; address 1 -&gt; address 2 -&gt; address 3 -&gt; address N… The message can be fetched from channels by using the MAM address. Therefore, Alice can publish the hash values like the following diagram In above case, Alice creates one channel with her private seed. Then she sends messages into this channel, one address has one message. Sample code for sending MAM message to channelThere is a sample code for sending message into IOTA tangle at https://github.com/linkcd/IOTAPoC/blob/master/tangleWriter.js from my repository. This code simply: Generates a random seed and create a public MAM channel Accepts inputs from keyboard and send it to tangle as json format Once the message was sent do tangle, anyone can query the tangle and read it from the public channel. But none can change it since it is immutable. You can read the MAM message by using a good tool https://thetangle.org/mam. Design principles and conceptual entitiesFirst, let’s agree some design principles and conceptual entities 1. Self-service verification processThe verification process of both data integrity and data lineage should be self-service. It means that all verification information should be available to public. Data provider should not be bothered by this process.(Technically it is possible to have permission control of the verification process, by using private or restricted MAM channels, it means that data provider has to response to the ad-hoc verification requests) 2. Data lineage verification is an add-on on the side of the main data-flowIt means that data lineage will not impact the existing data flow, nor become a bottleneck. 3. Conceptual entities3.1 Data Source A sensor, person or application that generates data package(s) Has 0 or more inputs (upstream data source) Has 1 or more outputs (downstream data source) A company can have more than one data sources 3.2 Data PackageAn atomic unit in data flow from one data source to another data source. For example: A data point (23 degree at 10:30am), or A file (json, xml) Files (1.json, 2.json…10.json…) Data rows in a database (row #1 to row #10 in table “employee”) 3.3 Data Package IdThe unique ID of a data package in the scope of a data source. A typical data package ID is a number, a GUID or a time-stamp. 3.4 Data StreamData stream is data package series from the same data source. It contains more than one packages and their IDs. Deep dive #1: Data integrityGoal: Data consumers can verify the integrity of data packages from a data source. Overview of data integrity workflowThe high level overview of data integrity workflow is as following: Step 1. Data source announces its MAM channel root address, for publishing integrity informationData source creates a MAM public channel by using its private seed, then share the root address of this channel with public. It can be done, for instance, via data source’s web site. Side Note: Why publish only root address, not all individual addresses per each packageThe data source can, of course, publish all individual addresses for all messages, but it will be too many. As long as consumers have the channel root address, they can go through all addresses from the root, to find the specific address&#x2F;message to verify. See step 5. Step 2. Decide your integrity information contentIn order to allow the consumer to verify the integrity, data source need to provide enough information to make it possible. Therefore, you need to decide what information should be stored in the tangle as json object. Mandatory fieldsAll object must have the following core fields. All of them are mandatory. 12345&#123; datapackageId: string, wayofProof:string, valueOfProof:string&#125; Field Description Example datapackageId The package ID is used for querying the data lineage info from the channel. Data source decides the ID format, such as integer or GUID. Different channels can have the same package ID. “123456” wayofProof Information about how to verify the integrity based on valueOfProof. For example, it explains the used hash algorithms (SHA1 or SHA2 or others), or it simply copied the data package content into field valueOfProof. “SHA256(packageId, data-content)” valueOfProof The value of the proof, such as hash value, or the copy of the data content in clear text. (hash value or data itself) ExampleAn application (aka data source) generates big csv files and pass to the downstream (aka data consumer). All csv files have a unique file name. The application decides to hash the file content together with the file name. The hash function can be one of the Secure Hash Algorithms, such as SHA-512&#x2F;256. Therefore, for file “file201.csv”, the application computed the hash based on SHA512(“201”, filecontent.string()), which is “7EC8E…AAFAA” 12345&#123; datapackageId: &quot;201&quot;, wayofProof:&quot;SHA512(201, filecontent.string())&quot;, valueOfProof:&quot;&quot;7EC8E...AAFAA&quot;&#125; Use hash for reducing calls to tangleThe hash is also useful if you want to reduce the data sent to tangle. For example, a data source is generating a small file per second. However, pushing data to tangle in every second can be a performance bottleneck. If the data source pack all files of every 10 minutes into one, assign an ID and compute the hash value of this data trunk, it can still publish integrity data into tangle but with much lower frequency. Extend it with optional fieldsIn addition to the above mandatory fields, you can extend the json object by adding more additional fields for fitting your logic. For example, you can add a field “location” for storing the application name, and “sensorType” of application owner type. These fields will be tightly coupled with these core fields, and stored in the tangle. 12345678910&#123; datapackageId: &quot;201&quot;, wayofProof:&quot;SHA512(201, filecontent.string())&quot;, valueOfProof:&quot;&quot;7EC8E...AAFAA&quot; applicationName:&quot;temperature reporter v2.1&quot;, applicationOwner:&quot;feng.lu@veracity.com&quot;, ... additionalField:... ...&#125; Note“timestamp” is not a mandatory field, as all transactions in Tangle already has a system timestamp that shows when the data was submitted to the tangle. You can add a “package-received-timestamp” field that shows when the original data package was collected. Step 3. Data source send data integrity information to the channelData source sends above json object to the MAM channel (IOTA tangle). The json object will be stored in a MAM address inside of the channel. This can be done by using the demo code shown above. — At this point, data source completes all needed tasks—- Step 4. Data consumer obtains the root address of the MAM channel from the data sourceData consumer goes to the website from step 1 to get the root address of the MAM channel that belongs to the data source. Step 5. Data consumer obtains the wayOfProof and valueOfProof for a specific packageData consumer goes through the MAM channel, address by address, to find the json object for the specific package, by using packageID. Then it obtains the wayOfProof and valueOfProof for that package. The pseudo-code is 123456789101112131415161718192021222324// get root address of the channel, from data source websitecurrentAddress = channel-root-address-got-from-data-source-website// to find message for package #201 targetPackageId = 201// start from the root address, go through all messages in this channel, to find the target messagewhile(currentAddress != null)&#123; var currentInfo = MAM.GetInfoFromAddress(currentAddress) if(currentInfo.PackageId == targetPackageId) &#123; // found the verification info for the target package, return return currentInfo &#125; else &#123; //the current address is not for the target package, go to next address currentAddress = currentAddress.nextAddress &#125;&#125;//check currentInfo.wayOfProof and currentInfo.valueOfProof Step 6. Data consumer prepare for verificationData consumer read the “wayOfProof” to understand how to check the “valueOfProof” field. For example, compute the hash by using the same hash function “SHA512(201, filecontent.string())” for package 201. Step 7. Data consumer to verify the integrityData consumer compares the hash values from the MAM channel and the local compute process. If hash value matches, then integrity is OK, accept the package. If hash value does NOT match, then integrity is NOT OK, reject the package. Deep dive #2: Data LineageA case studyLet’s look at a real life case: You as an American tourist was having an vacation in Norway. You were driving a car and had a great experience of the fjords. Unfortunately you had a small accident outside of a gas station, and the car windshield was damaged (but lucky, no one has been injured). The local police station was informed and issued a form (in Norwegian!) about this accident. Now you would like to report this to your insurance company. Most likely the insurance company would like to know if they can trust the damage report. You can of course explain that the data flow is: The police station in Norway issued a statement in Norwegian “2018 20. februar 13:00: En liten bilulykke utenfor Esso bensinstasjon på Billingstadsletta 9, 1396 Billingstad. Bilskade: 5000 NOK.” Since the US based insurance company only accept documents in English, a translation provider helped to translate this statement from Norwegian to English: “2018 Feb 20th, 13:00PM: A small car accident outside of Esso gas station at Billingstadsletta 9, 1396 Billingstad.Car damage: 5000 NOK. “ The next step is to convert the currency 5000 NOK to US Dollar, according to the currency rate of 2018 Feb 20th, as well as provide a map for describing the location of the accident. Therefore a currency converter and a geolocation service provider were involved for providing data. If we can store and verify this flow (data lineage of the report), it will: Provide full traceability of the end-to-end process Save huge cost from insurance company by reducing the time-consuming manual verification process Immutable histories of all inputs of the report. It also helps to identify responsibility of any false input. Solution DesignOn the top of the data integrity layer that we discussed above, it is easy to extend the format to build the data lineage layer. Now we extend the format to include an optional field “inputs“, which is an array of MAM addresses. These addresses represent the data integrity information of all inputs of the current data package. A MAM address is a global unique identifier in Tangle, regardless of which channel it belongs to. Extent the format to include inputs123456789&#123; datapackageId: string, wayofProof:string, valueOfProof:string, inputs: [array-of-input-addresses], ... additionalField:... ...&#125; Depends on if you have any input, “inputs“ field is optional. You can ignore this field, or have this field, but the value is null. The illustration is as below: Then, for above insurance report case, by using the additional inputs field, it is easy to establish the follow data lineage flow: It means that The insurance company can verify that the report#33 is indeed from the report generator and it was not tampered. By following the inputs fields, the insurance company can also find all upstream data and data origin in report#33, and verify the integrity of them as well. It also helps, for example, if there is a mistake of the currency convert from NOK to USD, it is the currency converter, not the report generator (customer), that holds the responsibility. FAQQ1: Can I use IOTA and MAM protocol without IOTA token?A1: Yes. Technically all MAM messages are 0(zero) value transactions. You can create unlimited MAM messages without any IOTA token. Q2: Can I use IOTA and MAM protocol without hosting an IOTA node?A1: Yes. You can use public hosted nodes on mainnet. For example, check https://thetangle.org/nodes or google “iota public nodes”. However, for production-ready solutions, having a managed node is recommended, which can offer you, for example, capacity of permanode, see Q3. Q3: What are Public nodes and Permanode? Which one should I use?A3: Some explanation can be found at here, for example. The short answer is, if you need to keep the historical MAM messages away from IOTA snapshots, go with permanode. Veracity is planning to host permanode(s) for the platform and its partners&#x2F;customers. Q4: Is it free to create a private seed and send messages to MAM channels?A4: Yes, you can simply create a seed (a string) locally and store data into MAM channel. Feel free to generate seeds for your sensors&#x2F;applications. Q5: I do not want to use public MAM channels that anyone can take a look at, even if I know the messages only contain hash values. How can I protect the channels?A5: MAM channel supports 3 access levels: public, private and restricted. In our solution, in order to make the verification as a self-service, we decided to use public channels. But it is possible to switch to private or restricted channels, and grant access to the selected data consumers to the channel. Q6: I have an application that is sending out data to consumers. Do I need to do anything if a new consumer starts using my data and build the lineage on top of it?A6: No. As a data source sitting in the upstream, you do not need to do anything for downstream consumers. Q7: I am a data consumer. What information do I need to create the whole data lineage covering all inputs in different levels? For example, if the data flow is Alice-&gt;Bob-&gt;Carol-&gt;myself, do I need to know the MAM root address of Alice, Bob and Carol?A7: No, you only need MAM root of Carol. As far as you follow the input fields recursively，you can check integrity and lineage of Bob (Carol’s upstream) and Alice (Bob’s upstream). In above insurance case, the insurance company can also follow the inputs fields to check, for example, the translator’s message and the Norwegian police station’s message. Q8: This solution sounds great, but it can take some efforts to build it, such as build the UX for data lineage visualization, API for read&#x2F;write MAM messages, manage seeds properly, etc. Is there anything we can reuse?A8: I am glad that you asked. In veracity we are building Data Integrity and Lineage as a Service (DILAAS) to bring down the barriers for both data providers and data consumers. DILAAS offers: A cloud service for managing and exchanging data integrity and lineage information between parties. Standard HTTP API, without building competence of backend DLT, such as MAM programming. It helps to reduce the development cost and boost the onboard progress. Seed&#x2F;Identity management, for properly managing the seeds&#x2F;identifies in the secure environment. Visualization of data integrity and lineage information. SummaryIn this article, we discussed the detailed design of verification data schema, such as these crucial fields: “datapackageId”, “wayofProof” , “valueOfProof” and “inputs”. We also implemented the solution on the selected DLT: IOTA and its MAM protocol. In next article of this series, we will put this solution into actions, and have closer look at some components of the DILAAS. AppendixMasked Authenticated Messaging (MAM) was introduced by IOTA in Nov 2017. The high level description can be found here. In addition, some deep dive information of Tangle transaction and MAM can be found at: IOTA: MAM Eloquently Explained Javascript lib of MAM: https://github.com/iotaledger/mam.client.js MAM deep dive (youtube) Other articles in this series: Part 1 Part 2 (this article) Part 3","categories":[],"tags":[{"name":"IoT","slug":"IoT","permalink":"https://feng.lu/tags/IoT/"},{"name":"IOTA","slug":"IOTA","permalink":"https://feng.lu/tags/IOTA/"},{"name":"Data Lineage","slug":"Data-Lineage","permalink":"https://feng.lu/tags/Data-Lineage/"},{"name":"Data Integrity","slug":"Data-Integrity","permalink":"https://feng.lu/tags/Data-Integrity/"},{"name":"DLT","slug":"DLT","permalink":"https://feng.lu/tags/DLT/"},{"name":"Distributed Ledger Technology","slug":"Distributed-Ledger-Technology","permalink":"https://feng.lu/tags/Distributed-Ledger-Technology/"},{"name":"Tangle","slug":"Tangle","permalink":"https://feng.lu/tags/Tangle/"}]},{"title":"Data Integrity and Lineage by using DLT, Part 1","slug":"Data-Integrity-and-Lineage-by-using-DLT-Part-1","date":"2018-09-25T18:33:49.000Z","updated":"2025-03-28T09:05:16.540Z","comments":true,"path":"2018/09/25/Data-Integrity-and-Lineage-by-using-DLT-Part-1/","permalink":"https://feng.lu/2018/09/25/Data-Integrity-and-Lineage-by-using-DLT-Part-1/","excerpt":"Other articles in this series: Part 1 (this article) Part 2 Part 3 IntroductionWith the proliferation of data – collecting and storing it, sharing it, mining it for gains – a basic question goes unanswered: is this data even good? The quality of data is of utmost concern because you can’t do meaningful analysis on data which you can’t trust. Here in Veracity, we are trying to address this is very concern. This is a 3 part series, going all the way from concept to a working implementation using DLT (Distributed Ledger Technology). Side note, Veracity is designed to help companies unlock, qualify, combine and prepare data for analytics and benchmarking. It helps data providers to easily onboard data to the platform, and enable data consumers to access and mine value. The data can be from various sources, such as sensors and edge devices, production systems, historical databases and human inputs. Data is generated, transferred, processed and stored, from one system to another system, one company to another company. Veracity is by DNV GL, and DNV GL has held a strong brand for more than 150 years as being a trusted 3rd party, yet it is still pretty common to hear questions from data consumers such as:1. Can I trust the data I got from Veracity?2. How was the data collected and processed?","text":"Other articles in this series: Part 1 (this article) Part 2 Part 3 IntroductionWith the proliferation of data – collecting and storing it, sharing it, mining it for gains – a basic question goes unanswered: is this data even good? The quality of data is of utmost concern because you can’t do meaningful analysis on data which you can’t trust. Here in Veracity, we are trying to address this is very concern. This is a 3 part series, going all the way from concept to a working implementation using DLT (Distributed Ledger Technology). Side note, Veracity is designed to help companies unlock, qualify, combine and prepare data for analytics and benchmarking. It helps data providers to easily onboard data to the platform, and enable data consumers to access and mine value. The data can be from various sources, such as sensors and edge devices, production systems, historical databases and human inputs. Data is generated, transferred, processed and stored, from one system to another system, one company to another company. Veracity is by DNV GL, and DNV GL has held a strong brand for more than 150 years as being a trusted 3rd party, yet it is still pretty common to hear questions from data consumers such as:1. Can I trust the data I got from Veracity?2. How was the data collected and processed? In order to answer these questions and bring more transparency to the data process lifecycle, we must address both data integrity and data lineage. Both Data integrity and data lineage are the foundation of trust. In this series of articles, we are going to look at different challenges of data integrity and lineage, and evolve the solution. (Note that integrity is one of the 3 parts of CIA triad: confidentiality, integrity and availability, but we will not cover confidentiality and availability in this series.) Data IntegrityLet’s start with a basic example:Alice sends messages (ie. files) to Bob. The messages were sent via an insecure channel, such as http-based data transfer, ftp, file share or even an USB stick. Basic requirements:There are 2 basic requirements for any data communication: The messages were not tampered by man-in-the-middle. The messages that Bob received, are indeed from Alice. There are mainly 2 ways to ensure this: encryption and&#x2F;or hashing. (A nice articles for comparing hashing and encryption can be found at here.) Iteration #1In iteration 1 we focus on solving requirement #1: The messages were not tampered by man-in-the-middle. We either use encryption or hashing. 1.1 Using Encryption (with symmetric or asymmetric key) Pro: Encryption is pretty straight forward: Either use symmetric or asymmetric key, Alice and Bob can encrypt and decrypt messages without worrying data tampering. Con: However, this requires key management, both for Alice and Bob. 1.2 Using Hashing Pro: Does not require key management for both Alice and Bob. Con: With hashing, it requires an additional data flow for passing hash values from Alice to Bob. It actually requires the same security mechanism as the normal data flow. We can address the Con by introducing a trusted area for Alice. For example, Alice also publishes the hash values of the messages on https://alice.com. Bob can verify the message by compare the hash values. It is also OK to make the trusted area public, as hash value is irreversible - nobody can obtain the data by using the hash value, they can only check the message integrity. This solution is sort of adding a secured “safeguard” track on the side, to help verifying the data flowing in the insecure channel. Pro: Does not require key management for both Alice and Bob. Solved the problem for protecting hash value data flow. Iteration #2In iteration #2, in additional to requirement #1, we also need to also fulfill requirement #2: The messages that Bob received, are indeed from Alice. 2.1 Using Encryption (with Asymmetric key)This normally requires asymmetric encryption: Alice encrypts the message with her private key, and Bob decrypts it with Alice’s public key. Therefore, Bob is confident that Alice is the message author. 2.2 Using Hashing (with a trusted area), same as iteration #1Hashing solution with a trusted area can also meet this requirement, simply counting on ONLY Alice can write into the trusted area. Conclusion of iteration #1 and #2For ensuring the basic requirements, both solutions work: Using Encryption (with asymmetric key), and Using Hashing (with trusted area) Challenge of accountability and non-repudiationNow, an interesting real challenge: Once Alice sends out a message, she can neither deny the message was sent, the message’s origin nor the original content. In another word, the challenge is about accountability and non-repudiation. It can be explained by the following example: (click to enlarge the picture) At this point, these above solutions that we have so far cannot help Bob. For example, Alice can replace both the message and the hash value in https://alice.com. With encryption solution, although Bob can prove the buggy version of message #2 is from Alice, but he cannot prove that Alice sent out the buggy version on Monday. **In general, Bob (and we) need an immutable history that provides immutable traceability of data, such as when and what data was sent and processed. ** It definitely helps provides data consumers like Bob, but there are benefits for well-behaved data providers as well: By offering the immutable history, it increases the data’s acceptance from that providers, as well as increases value and trust of the provider. Iteration #3 - introduce DLT as immutable historyDistributed Ledger Technology (DLT) shows its potential capacity to become a natural place for storing data integrity and data lineage information, as it has the following key features: Data ImmutabilityAs the data is replicated in all nodes in the DLT network, it means that the data is immutable, even the author cannot modify his&#x2F;her records once it is confirmed in ledgers. DecentralizedThe ledger network is decentralized, means all participators have the same copy of the data, including Alice and Bob. There is not a central authority can control the whole network and the records in it. Built-in authenticationIn order to send data to the ledger, the author must use his&#x2F;her private key. It provides the built-in authentication for identifying who is the author of the data&#x2F;transaction. 3.1 Encrypted message within DLTAs DLT does support built-authentication, there is no need to use asymmetric(public&#x2F;private) key for identity purpose. You can still use symmetric key for protecting the message from unauthorized access.However, there are some limitations for using DLT as the secured channel. The biggest one is the size limitation of the message. For example, bitcoin size limitation is 1 MB and ethereum is of similar size. For lots of the cases, this limitation is show-stopper. Therefore, the hashing solution with DLT is more realizable. See below. 3.2 Hashing with DLT By only putting hash value of the messages into DLT, we can solve the size limitation issue.It means: Alice continues to send messages via the insecure channels. Meanwhile, Alice sends the hash values of the message into DLT. All hash values that Alice sent to DLT, is signed by Alice’s private key. So everyone knows it was author (Alice), the original message’s digest and when the data was sent (timestamp). Once Bob received the message via insecure channel, he can Find the transaction that contains the hash value for that message from DLT By check the transaction’s metadata, Bob can check the author and timestamp. By check the hash value from that transaction, Bob can verify the message content is not tampered. Also, it is impossible for Alice want to replace an old hash value. So Bob is protected from an immutable history. Data LineageThe goal of data lineage is to track data over its entire lifecycle, to gain a better understanding of what happens to data as it moves through the course of its life. It increases trust and acceptance of result of data process. It also helps to trace errors back to the root cause, and comply with laws and regulations. You can easily compare this with the traditional supply chain of raw materials in manufacturing industry and&#x2F;or logistic industry. For example, Bob is running a data process. This process takes inputs from Alice, then produces results. The results are sent to Carol. Bob produces result #X, based on inputs from Alice #1 and #2. Bob produces result #Y, based on input from Alice #3. For Carol, some typical questions are: What inputs Bob used for producing the results #X and #Y? Do these inputs also have another input? If yes, what are they? Is there a way to have to full picture of the whole data process lifecycle, without asking Bob (and every upstream) in the supply chain? Data Lineage with DLTNow we continue building on top of the Hashing solution with DLT. Whenever a data provider (for example, Bob) sends out data, he writes into DLT that contains: Hash value, which will be used for data integrity verification (same as before), and If the data has inputs, the reference to the input are also stored into DLT It means that the DLT contains the end-to-end data lifecycle information. Carol (and anyone else) only need to query the public information from DLT to build the lineage diagram. With this solution, Carol can: Gain the knowledge that that Bob is using data from Alice as inputs, without asking Bob. Verify the data integrity for both Alice and Bob, even Carol does not directly consume the data from Alice. Data integrity and data lineage information is immutable. Extra protection for data processorIn above process, Bob is a data processor that accepts inputs from Alice (upstream), process it and send results to Carol (downstream). This solution also provides an extra protection for Bob. For example, if Bob sent a data to Carol based on an incorrect input from Alice, Bob can simply explain that the root cause of the error is not on him but Alice, and Alice cannot deny that. This also means this solution can greatly simplifying the ability to trace errors back to the root cause, even the whole process includes different parties&#x2F;organizations. ConclusionNow we have went through different requirements and evolved solutions accordingly. At the end we believe the hashing solution with DLT can solve both data integrity and data lineage challenges. If the eco-system (data source, data processors and platform) can follows the same design, it will significantly increase the trust of data consumers as well as build more value into the data. In the next article, we will look at this solution in action, by using IOTA as the selected DLT. Other articles in this series: Part 1 (this article) Part 2 Part 3","categories":[],"tags":[{"name":"IoT","slug":"IoT","permalink":"https://feng.lu/tags/IoT/"},{"name":"IOTA","slug":"IOTA","permalink":"https://feng.lu/tags/IOTA/"},{"name":"Data Lineage","slug":"Data-Lineage","permalink":"https://feng.lu/tags/Data-Lineage/"},{"name":"Data Integrity","slug":"Data-Integrity","permalink":"https://feng.lu/tags/Data-Integrity/"},{"name":"DLT","slug":"DLT","permalink":"https://feng.lu/tags/DLT/"},{"name":"Distributed Ledger Technology","slug":"Distributed-Ledger-Technology","permalink":"https://feng.lu/tags/Distributed-Ledger-Technology/"},{"name":"Tangle","slug":"Tangle","permalink":"https://feng.lu/tags/Tangle/"}]},{"title":"Using new domain feng.lu","slug":"Using-new-domain-feng-lu","date":"2018-08-27T19:13:52.000Z","updated":"2025-03-28T09:05:16.902Z","comments":true,"path":"2018/08/27/Using-new-domain-feng-lu/","permalink":"https://feng.lu/2018/08/27/Using-new-domain-feng-lu/","excerpt":"","text":"Shortly after I have renewed my blog domain fenglu.me, it just crossed my mind that “hey, is it possible to register a top-level domain with my family name .lu? So I can literally have my name for my site: feng.lu! That will be cool!” (picture copyright: www.dreamhost.com) And, (after googling), yes! It is possible! .lu is the Internet country code top-level domain for Luxembourg. OK… (continue googling) “Can I register a .lu domain without been a Luxembourgers?” “No problem!” Great! Long story short, after some quick research on vendors and paid 24 Euro, I got the brand new feng.lu domain! :) The remaining is pretty straightforward: In feng.lu domain provider, set up an apex domain and www subdomain for my real blog host Github page, according to their document. In github page settings, update the custom domain (equals to update the CNAME file). Update blog source code (hexo) with the new domain Important!: Since I would like to keep all existing links from the old domain fenglu.me continue working, I also setup the domain forwarding. Document. Remember to use “Redirect to a specific page&#x2F;folder&#x2F;subfolder”. Update Google Analytics, GTM, etc Done! Happy blogging!","categories":[],"tags":[{"name":"Blog","slug":"Blog","permalink":"https://feng.lu/tags/Blog/"},{"name":"Domain","slug":"Domain","permalink":"https://feng.lu/tags/Domain/"},{"name":"DNS","slug":"DNS","permalink":"https://feng.lu/tags/DNS/"}]},{"title":"Data Integrity and Lineage by using IOTA","slug":"Data-integrity-and-data-lineage-by-using-IOTA","date":"2018-04-16T09:22:13.000Z","updated":"2025-03-28T09:05:16.560Z","comments":true,"path":"2018/04/16/Data-integrity-and-data-lineage-by-using-IOTA/","permalink":"https://feng.lu/2018/04/16/Data-integrity-and-data-lineage-by-using-IOTA/","excerpt":"Edit log:###2018.09.25This article is now expanded to an article series, where we have more detailed discussion and open-source code, check them out! 2018.08.26 - Updated the data schema: Have an unified format that covers both lightweight format and standard format, but more flexible and self-explained. Specified mandatory fields and optional field in the format. For example, Timestamp is now an optional field. IntroductionIf we say “Data is the new oil”, then data lineage is an issue that we must to solve. Various data sets are generated (most likely by sensors), transferred, processed, aggregated and flowed from upstream to downstream. The goal of data lineage is to track data over its entire lifecycle, to gain a better understanding of what happens to data as it moves through the course of its life. It increases trust and acceptance of result of data process. It also helps to trace errors back to the root cause, and comply with laws and regulations. You can easily compare this with the traditional supply chain of raw materials in manufacturing industry and&#x2F;or logistic industry. However, compares to the traditional industries, data lineage are facing new challenges.","text":"Edit log:###2018.09.25This article is now expanded to an article series, where we have more detailed discussion and open-source code, check them out! 2018.08.26 - Updated the data schema: Have an unified format that covers both lightweight format and standard format, but more flexible and self-explained. Specified mandatory fields and optional field in the format. For example, Timestamp is now an optional field. IntroductionIf we say “Data is the new oil”, then data lineage is an issue that we must to solve. Various data sets are generated (most likely by sensors), transferred, processed, aggregated and flowed from upstream to downstream. The goal of data lineage is to track data over its entire lifecycle, to gain a better understanding of what happens to data as it moves through the course of its life. It increases trust and acceptance of result of data process. It also helps to trace errors back to the root cause, and comply with laws and regulations. You can easily compare this with the traditional supply chain of raw materials in manufacturing industry and&#x2F;or logistic industry. However, compares to the traditional industries, data lineage are facing new challenges. Data lineage challengesSome top challenges are: We are tracking logical entities (bits, files or data streams) instead of physical entities (coal, car parts or packages). The granularity of data has much more detailed scale in IoT and Big Data world. One example is, tracking every single piece of coal from a carrier ship sounds crazy, but tracking every data signal from thousands sensors from the very same ship is quite common. Also, protecting logical entities is even harder in the cyberspace, both in data transportation and storage. In addition, Data lineage is built on top of data integrity, which has to be solved first. We need a neutral and trustworthy 3rd-party for keeping both data integrity and data lineage information, as most likely the upstream (supplier) and downstream (consumer) are different organization. TechnologiesDistributed Ledger Technology (DLT) shows its potential capacity to become the neutral and trustworthy 3rd party in data lineage world, as it has the following key features: Data Immutable Decentralized But not all DLT are suitable for Big Data or IoT scenarios, when we have, for example, following requirements: Need to use DLT to store large amount of data (e.g. data integrity information of thousands of sensors) Need to use DLT to handle large volume of transactions within a short time period (e.g. send 1000 data points from one device to another device per minute) Cannot afford high transaction fee Therefore, IOTA becomes an outstanding DTL compares to other blockchain platform, by offering the following features: Data Integrity&#x2F;Security: All data cryptographically secured in a Tangle. This data can be made visible to certain parties. Zero Transaction Fee: Machine to Machine micropayments. This way machines can pay each other for certain services and resources. Full scalability: Thanks to tangle data structure This is not an article of introducing IOTA, but you can learn more from https://www.iota.org/ and https://blog.iota.org and IOTA channel in Discord. But most importantly, it brings Masked Authenticated Messaging (MAM) which fits into our need for data integrity and data lineage. Masked Authenticated Messaging (MAM)Masked Authenticated Messaging (MAM) was introduced by IOTA in Nov 2017. The high level description can be found at here.Besides, some deep dive information of Tangle transaction and MAM can be found at: IOTA: MAM Eloquently Explained IOTA blogs by Louie Lu (in Chinese) IOTA research group (in Chinese) Javascript lib of MAM: https://github.com/iotaledger/mam.client.js MAM deep dive (youtube) Solution Deep DiveDesign principles1. Data integrity is the foundation of data lineageData Integrity is the prerequisite of Data Lineage, and they can be addressed separately. 2. Self-service verification process###The verification process of both data integrity and data lineage should be self-service. It means that all verification information should be available to public. Data provider should not be bothered by this process.(Technically it is possible to have permission control of the verification process, it means that data provider has to response to the ad-hoc verification requests) 3. Data lineage verification is an add-on on the side of the main data-flowIt means that data lineage will not impact the existing data flow, nor become bottleneck. Conceptual EntitiesData Source A sensor, person, application that generates data package (s) Has 0 or more inputs (upstream data source) Has 1 or more outputs (downstream data source) A company can have more than one data sources Data PackageAn atomic unit in data flow from one data source to another data source. For example: A data point (23 degree at 10:30am), or A file (json, xml) Files (1.json, 2.json…10.json…) Data rows in a database (row #1 to row #10 in table “employee”) Data Package IdThe unique ID of a data package in the scope of a data source. A typical data package id is a number, a GUID or a time-stamp. Data StreamData stream is data package series from the same data source. It contains more than more packages and their IDs. Solution Part 1: Data IntegrityGoal: One can verify the integrity of data packages from a data source. Step A. Data source creates a MAM channel for publishing integrity informationData source creates a public MAM public channel by using its private seed. The private seed ensures only the the data source can publish information into that channel, and so the channel is trusted by others. Step B. Decide your integrity information contentIn order to allow the consumer to verify the integrity, you need to provide enough information to make it possible. Therefore you need to decide what information should be stored in the tangle as json object. Mandatory fieldsAll object must have the following core fields. All of them are mandatory. 12345&#123; datapackageId: string, wayofProof:string, valueOfProof:string&#125; Field Description Example datapackageId The package ID is used for querying the data lineage info from the channel. Data source decides the ID format, such as integer or GUID. Different channels can have the same package ID. “123456” wayofProof Information about how to verify the integrity based on valueOfProof. For example, it explains the used hash algorithms (SHA1 or SHA2 or others), or it simply copied the data package content into field valueOfProof. “SHA256(packageId, original-data-content)” valueOfProof The value of the proof, such as hash value, or the copy of the data content in clear text. (hash value or data itself) Case 1A temperature sensor decides to use timestamp as package Id, and since the data point is small and not confidential, it decides to put the data point as clear text in the integrity information object. Therefore, at 2012-08-29 11:38:22, the temperature is 20 degree. It sends the integrity json into its own MAM channel: 12345&#123; datapackageId: &quot;1346236702&quot;, wayofProof:&quot;copy of original data&quot;, valueOfProof:&quot;20&quot;&#125; Case 2An application generates big csv files and pass to the down stream. all csv files have an unique file name. Since we do not have do expose the csv file itself, either due to confidentiality or huge file size, it decides to use hash value in the integrity json. The hash function can be one of the Secure Hash Algorithms, such as SHA-512&#x2F;256. This application decide to hash the file content together with the filename. therefore, for file with unique name “file075.csv”, the application computed the hash based on SHA256(“file075.csv”+”:”+filecontent.string()), which is “8c20f3d24…43a6cfb7c4” 12345&#123; datapackageId: &quot;file075.csv&quot;, wayofProof:&quot;SHA256(&quot;file075.csv&quot;+&quot;:&quot;+filecontent.string())&quot;, valueOfProof:&quot;&quot;8c20f3d24...43a6cfb7c4&quot;&#125; Use hash for reducing calls to tangleThe hash is also useful if you wanna reduce the data sent to tangle. For example, a data source is generating a small file per second. However, pushing data to tangle in every second can be a performance bottleneck. If the data source pack all files of every 10 minutes into one, assign an ID and compute the hash value of this data trunk, it can still publish integrity data into tangle but with much lower frequency. Extend it with optional fieldsIn addition to the above mandatory fields, you can extend the json object by adding more additional fields for fitting your logic. For example, for case 1, you can add a field “location” for storing the location of that sensor, and “sensorType” of sensor type. These fields will be tightly coupled with these core fields, and stored in the tangle. 12345678910&#123; datapackageId: &quot;1346236702&quot;, wayofProof:&quot;copy of original data&quot;, valueOfProof:&quot;20&quot;, location:&quot;Oslo&quot;, sensorType:&quot;temperature sensor XY200&quot;, ... additionalField:... ...&#125; Note“timestamp” is not a mandatory field, as all transactions in Tangle already has a system timestamp shows when the data was submitted to the tangle. You can add “timestamp” field to store when the original data was collected. Step C. Data source send data integrity information to the channelData source sends data or hash value of the data to the MAM channel (IOTA tangle), it ensures: Data verification information is immutable Can verify the integrity with or without have access to data itself (hash) Performance is acceptable (assuming IOTA can handle large volume of data from devices) No extra cost due to zero transaction fee Demo code of sending MAM message to channelYou can have a look at the sample code of sending message into IOTA tangle at https://github.com/linkcd/IOTAPoC/blob/master/tangleWriter.js from my repository. This code simply: Generate a random seed and create a public MAM channel Accept inputs from keyboard and send it to tangle as json format Once the message was sent do tangle, everyone can query the tangle and read it. But none can change it since it is immutable. You can read the MAM message by using a good tool https://iota-mam-explorer.now.sh/. Step D. Data source publish information of the channelData source publish (on web site or equivalent) the following information for anyone who wants to verify the integrity: Root address of the MAM channel (NOT the private seed!) Step E. Data consumer to verify the integrityIf a data consumer would like to verify the data he&#x2F;she got from the data source is not tampered, the consumer can: Obtain the MAM channel root address of the data source read the “wayOfProof” to understand how to check the “valueOfProof” field.For example: if “wayOfProof” is “copy of original data”, then simply compare the value from tangle and value from received package. if “wayOfProof” is “SHA256(“file075.csv”+”:”+filecontent.string())”, the perform the same hash locally, the compare the result with the value from “valueOfProof” Overview of data integrity workflow Solution Part 2: Data LineageA case studyLets look at an real life case: You as an American tourist was having an vacation in Norway. You were driving a car and had a great experience of the fjord. Unfortunately you had a small accident outside of a gas station, and the car windshield was damaged (but lucky, no one has been injured). The local police station was informed and issued a form (in Norwegian!) about this accident. Now you would like to report this to your insurance company. Most likely the insurance company would like to know if they can trust the damage report. You can of course explain that the data flow is: The police station in Norway issued a statement in Norwegian “2018 20. februar 13:00: En liten bilulykke utenfor Esso bensinstasjon på Billingstadsletta 9, 1396 Billingstad. Bilskade: 5000 NOK.” Since the US based insurance company only accept documents in English, a translation provider helped to translated this statement from Norwegian to English: “2018 Feb 20th, 13:00PM: A small car accident outside of Esso gas station at Billingstadsletta 9, 1396 Billingstad.Car damage: 5000 NOK. “ The next step is to convert the currency 5000 NOK to US Dollar, according to the currency rate of 2018 Feb 20th, as well as provide a map for describing the location of the accident. Therefore a currency converter and a geolocation service provider were involved for providing data. If we can store and verify this flow (data lineage of the report), it will: Provide full traceability of the end-to-end process Save huge cost from insurance company by reducing the time consuming manual verification process Immutable histories of all inputs of the report. It also help to identify responsibility of any false input. Solution DesignOn the top of the data integrity layer that we discussed above, it is easy to extend the format to build the data lineage layer. Now we extend the format to include an optional field inputs, which is an array of MAM addresses. These addresses represent the data integrity information of all inputs of the current data package. Depends on if you have any input, inputs is optional. You can ignore this field, or have this field but the value is null. Extent the format to include inputs123456789&#123; datapackageId: string, wayofProof:string, valueOfProof:string, inputs: [array-of-input-addresses], ... additionalField:... ...&#125; By using the additional inputs field, it is easy to establish the follow data lineage flow: It means that The insurance company can verify the report#33 is indeed from the report generator and it was not tampered. By following the inputs fields, the insurance company can also find all upstream and data origin in report#33, and verify the integrity of them as well. It also helps, for example, if there is a mistake of the currency convert from NOK to USD, it is the currency converter, not the report generator (customer), to take the responsibility. ConclusionData integrity and data lineage play important roles in the coming data-first era. By using DLT, especially IOTA, it is possible to build the infrastructure of them. However, we have to keep in mind, even IOTA looks promising, it is under development, and it is not production ready. We will continue our investigation and collaboration with IOTA team&#x2F;communities to continue this journey.","categories":[],"tags":[{"name":"IoT","slug":"IoT","permalink":"https://feng.lu/tags/IoT/"},{"name":"IOTA","slug":"IOTA","permalink":"https://feng.lu/tags/IOTA/"},{"name":"Data Lineage","slug":"Data-Lineage","permalink":"https://feng.lu/tags/Data-Lineage/"},{"name":"Data Integrity","slug":"Data-Integrity","permalink":"https://feng.lu/tags/Data-Integrity/"},{"name":"DLT","slug":"DLT","permalink":"https://feng.lu/tags/DLT/"},{"name":"Distributed Ledger Technology","slug":"Distributed-Ledger-Technology","permalink":"https://feng.lu/tags/Distributed-Ledger-Technology/"},{"name":"Tangle","slug":"Tangle","permalink":"https://feng.lu/tags/Tangle/"}]},{"title":"Running IOTA Full Node","slug":"Running-IOTA-Full-Node","date":"2018-02-19T21:29:05.000Z","updated":"2025-03-28T09:05:16.902Z","comments":true,"path":"2018/02/19/Running-IOTA-Full-Node/","permalink":"https://feng.lu/2018/02/19/Running-IOTA-Full-Node/","excerpt":"I have been looking at IOTA since last winter, as it seems promising for IoT, Machine-to-Machine Micro-payments and Data Market scenarios. Installing an IOTA light wallet is pretty straightforward, but running a full node is not. But thanks to the great playbook, I managed to setup a Virtual Private Server to run as an IOTA full node. 2 cores CPU 4 GM memory SSD Hosted 24&#x2F;7 in a data center in Western Europe","text":"I have been looking at IOTA since last winter, as it seems promising for IoT, Machine-to-Machine Micro-payments and Data Market scenarios. Installing an IOTA light wallet is pretty straightforward, but running a full node is not. But thanks to the great playbook, I managed to setup a Virtual Private Server to run as an IOTA full node. 2 cores CPU 4 GM memory SSD Hosted 24&#x2F;7 in a data center in Western Europe Setup steps Followed the steps in playbook Enabled remote access for the node, so the light wallet can connect to it. Setup firewall rules to allow IOTA node talking to internet Setup DNS to make the node more friendly for my neighbors Found good neighbors from IOTA Discord #nodesharing channel Tips: go to #rank-yourself and type “!rank fullnode”, then you’ll get access to the #nodesharing channel Security!There are lots of things you need to think about when you are hosting a 24&#x2F;7 server on the internet. This blog and Security Hardening section provides a good guideline. Use SSH key access Disable password authentication Disable SSH root access In addition, if you are using the playbook installer , you basically have the default user name and ports for your full node.** IT IS IMPORTANT TO CHANGE THEM!** Otherwise the attacker only need to crack the password, as they already know your user name (iotapm) and your ports. Update user name and password in bash123456789nano /opt/iri-playbook/group_vars/all/iotapm.yml#update the following valuesiotapm_nginx_user: new_user_accountiotapm_nginx_password: &#x27;a-strong-password&#x27; nano /opt/iri-playbook/group_vars/all/z-override-iotapm.yml#update the following valuesiotapm_nginx_user: new_user_accountiotapm_nginx_password: &#x27;a-strong-password&#x27; reference You can perform the following steps after you completed the installer. Update nginx user Remove the default user iotpm 1htpasswd -D /etc/nginx/.htpasswd iotpm Create new user 1htpasswd /etc/nginx/.htpasswd new_user_account Update system account in grafana Stop grafana-server: 1systemctl stop grafana-server Delete grafana’s database: 1rm -f /var/lib/grafana/grafana.db Edit &#x2F;etc&#x2F;grafana&#x2F;grafana.ini, set correct values for admin_user and admin_password (from above step) Start grafana-server: 1systemctl start grafana-server re-install grafana by using iric, select “update monitoring” reference Screenshots Overview of connected neighbors The node in the map: http://field.carriota.com/ Also, connect the wallet to the our node Build the communityIf you are looking for neighbors, or would like to connect your wallet to this node, please feel free to let me know.If you would like to donate, please use the following address. :) 1LPQRSZKJM9IRXHMUYJZQLKMAKJHJQDERJWIPSLKCYAPXVZPGEWG9QDXQUNTXCMZYLLIHPHGULVGFIAZAWDFECWYKGC EoF.","categories":[],"tags":[{"name":"IOTA","slug":"IOTA","permalink":"https://feng.lu/tags/IOTA/"},{"name":"DLT","slug":"DLT","permalink":"https://feng.lu/tags/DLT/"},{"name":"Distributed Ledger Technology","slug":"Distributed-Ledger-Technology","permalink":"https://feng.lu/tags/Distributed-Ledger-Technology/"},{"name":"Blockchain","slug":"Blockchain","permalink":"https://feng.lu/tags/Blockchain/"},{"name":"Fullnode","slug":"Fullnode","permalink":"https://feng.lu/tags/Fullnode/"}]},{"title":"Infrastructure-as-Code and CI/CD in the real world, with VSTS and Azure (Part 1)","slug":"Infrastructure-as-code-in-the-real-world","date":"2017-11-05T10:13:36.000Z","updated":"2025-03-28T09:05:16.811Z","comments":true,"path":"2017/11/05/Infrastructure-as-code-in-the-real-world/","permalink":"https://feng.lu/2017/11/05/Infrastructure-as-code-in-the-real-world/","excerpt":"Hello again! It has be been a while since my last post. It is because I was quite busy leading a team in a program for delivering veracity.com, the open industry data platform from DNV GL. It is a pretty exciting project - to build an open, independent data platform with bleeding edge technologies, to serve a large user base (100 000 registered users). You can read more about veracity at here and here. It actually is a long and interesting story behind veracity (and its predecessor), together with all challenges that we encountered in this journey. Hopefully I can share them with you in the future. Anyway, today I would like to talk about in the real world, how Infrastructure-as-Code looks like, together with Azure and VSTS.","text":"Hello again! It has be been a while since my last post. It is because I was quite busy leading a team in a program for delivering veracity.com, the open industry data platform from DNV GL. It is a pretty exciting project - to build an open, independent data platform with bleeding edge technologies, to serve a large user base (100 000 registered users). You can read more about veracity at here and here. It actually is a long and interesting story behind veracity (and its predecessor), together with all challenges that we encountered in this journey. Hopefully I can share them with you in the future. Anyway, today I would like to talk about in the real world, how Infrastructure-as-Code looks like, together with Azure and VSTS. ChallengesThere are tons of azure templates which is a great start point to use Infrastructure-as-Code in Azure. However, in the real world project, we always need to do a lot of extra work due to:1. We need multiple environments such as Nightlybuild, Testing and Production environment.2. Different environments are 90% identical, but we have to handle the 10% differences For example, we use more powerful App Service Plan in Production, but cheaper plan for Nightly build, in order to save the cost.3. We cannot include secrets (e.g. database password and keys) in version control.4. We want to release without downtime.5. We want to release new version fast, with quality control6. We do NOT like manual job! Above introduces the complexity to the CI&#x2F;CD process, so it is important to have some best practices and common understanding in the team. Let’s start with something simpleLet’s start with something simple, then we evolve it overtime for addressing different challenges. Lets say we are going to build a simple nodejs web application as following, and host it in Azure. This application is named “MyWords”. It has 3 components: An Azure App Service, An Azure Storage Account, and An Azure Application InsightThe application setting of the web app contains the storage account connection string and application insight instrumentation key. One environment, for the PoC##As a developer, you can simple go to Azure portal and create them manually. It is perfectly OK especially when you are building a PoC. Create resource group Create a app service Create a storage account, copy the connection string into app settings of the web app. Create an application-insight, and copy the instrumentation key into app settings of the web app. Multiple environments, when things are getting realNow as usual, when the project became serious, we need multiple environments for a better control. In this case, they are Nightlybuild, Testing and Production. Nightlybuild: Every night we pull the latest code changes and deploy to this environment. Fully automatic process. Testing: We would like to deploy the version that we have verified in NightlyBuild to Testing environment. The deployment process should be automatically but it requires manual approval, by Quality Assurance (QA). Production: Fully automatic deployment process, but requires manual approval, by Product Owner (PO). For now, these 3 environments are identical (of course, they are 3 different web app with different url, 3 storage accounts and 3 application-insight instances). Now the manual steps from previous stage become tedious and time consuming, we would like to automate them. It can be simply achieved by using Azure Resource Manager template and VSTS task. Tips: Azure quickstart template is a good place to start. The official and latest schema can be found at https://docs.microsoft.com/en-us/azure/templates/, for example the web site schema. If you have VS2017, you can use the built-in template Azure Resource Group. you can create the resources manually, then export them in json format by going to https://resources.azure.com/ At the end, we have our infra-as-code for our applications. A basic skeleton of resource provisioning (see source code), and The parameter file for Nightly Build environment (see source code) The result of provisioning is Testing Infra-as-codeIt will be several iteration before you get the template right, but if your using VS2017, you can use some GUI for debugging. VS2017 is simply calling the following command (Deploy-AzureResourceGroup.ps1 is a standard powershell that VS generated for you, you can also download it here) 1Deploy-AzureResourceGroup.ps1 -ResourceGroupName &#x27;Real-life-infra-as-code-manual-testing&#x27; -ResourceGroupLocation &#x27;northeurope&#x27; -TemplateFile &#x27;azuredeploy.json&#x27; -TemplateParametersFile &#x27;azuredeploy.nightlybuild.parameters.json&#x27; -ValidateOnly Pay attention to the switch parameter: -ValidateOnly. Without it, you can actually provision resources. As alternative, you can run 1Test-AzureRmResourceGroupDeployment -ResourceGroupName &#x27;Real-life-infra-as-code-manual-testing&#x27; -TemplateFile $Env:BUILD_SOURCESDIRECTORY\\Real-life-infra-as-code\\azuredeploy.json -TemplateParameterFile $Env:BUILD_SOURCESDIRECTORY\\Real-life-infra-as-code\\azuredeploy.nightlybuild.parameters.json Auto passing keys from newly created resources##It is nice to let the script to create resources for us, but there is only a hard-coded value that we specified in the json file is stored in application settings. Therefore we still have to manually copy keys and connection strings from application-insight and storage account into web site app settings. 123456789101112131415&#123; &quot;name&quot;: &quot;[parameters(&#x27;webAppName&#x27;)]&quot;, &quot;type&quot;: &quot;Microsoft.Web/sites&quot;, ......... &quot;properties&quot;: &#123; &quot;siteConfig&quot;: &#123; ......... &quot;appSettings&quot;: [ &#123; &quot;name&quot;: &quot;WEBSITE_NODE_DEFAULT_VERSION&quot;, &quot;value&quot;: &quot;6.11.1&quot; &#125; ] &#125;.... To automate this process, we can look into Azure RM template functions. The function listkeys and listvalue is useful for fetching values of a resource. Now we use this function for passing keys from storage account, and direclty use InstrumentationKey property to get the key from application insight. 1234567891011121314151617181920212223&#123; &quot;name&quot;: &quot;[parameters(&#x27;webAppName&#x27;)]&quot;, &quot;type&quot;: &quot;Microsoft.Web/sites&quot;, .... &quot;appSettings&quot;: [ &#123; &quot;name&quot;: &quot;WEBSITE_NODE_DEFAULT_VERSION&quot;, &quot;value&quot;: &quot;6.11.1&quot; &#125;, &#123; &quot;name&quot;: &quot;STORAGE_ACCOUNT&quot;, &quot;value&quot;: &quot;[variables(&#x27;storageAccountName&#x27;)]&quot; &#125;, &#123; &quot;name&quot;: &quot;STORAGE_ACCESSKEY&quot;, &quot;value&quot;: &quot;[listKeys(resourceId(&#x27;Microsoft.Storage/storageAccounts&#x27;, variables(&#x27;storageAccountName&#x27;)), &#x27;2016-01-01&#x27;).keys[0].value]&quot; &#125;, &#123; &quot;name&quot;: &quot;APPINSIGHTS_INSTRUMENTATIONKEY&quot;, &quot;value&quot;: &quot;[reference(resourceId(&#x27;microsoft.insights/components&#x27;, variables(&#x27;appinsightName&#x27;)), &#x27;2015-05-01&#x27;).InstrumentationKey]&quot; &#125; ].... In addtion, make sure we add dependencies into website resource, to make sure that we have created the storage account and application insight before the website is created 12345678910 &#123; &quot;name&quot;: &quot;[parameters(&#x27;webAppName&#x27;)]&quot;, &quot;type&quot;: &quot;Microsoft.Web/sites&quot;, .... &quot;dependsOn&quot;: [ &quot;[concat(&#x27;Microsoft.Web/serverfarms/&#x27;, variables(&#x27;serverFarmName&#x27;))]&quot;, &quot;[concat(&#x27;Microsoft.Storage/storageAccounts/&#x27;, variables(&#x27;storageAccountName&#x27;))]&quot;, &quot;[concat(&#x27;microsoft.insights/components/&#x27;, variables(&#x27;appinsightName&#x27;))]&quot; ]..... Double check in application settings: The updated code can be found at here. Let’s recap Now we have a basic infra-as-code that can provision multiple identical environments. It can connect different resources together by automatically passing keys from resource to another. We have tools and scripts to verify the infra-as-code What is next?In the coming articles, we will continue addressing the following challenges: Different environments are 90% identical, but we have to handle the 10% differences We cannot include secrets (e.g. database password and keys) in version control. We want to release without downtime. We want to release new version fast, with quality control To be continued.","categories":[],"tags":[{"name":"Azure","slug":"Azure","permalink":"https://feng.lu/tags/Azure/"},{"name":"DevOps","slug":"DevOps","permalink":"https://feng.lu/tags/DevOps/"},{"name":"VSTS","slug":"VSTS","permalink":"https://feng.lu/tags/VSTS/"},{"name":"Azure Template","slug":"Azure-Template","permalink":"https://feng.lu/tags/Azure-Template/"},{"name":"Infrastructure as Code","slug":"Infrastructure-as-Code","permalink":"https://feng.lu/tags/Infrastructure-as-Code/"},{"name":"Continuous Integration","slug":"Continuous-Integration","permalink":"https://feng.lu/tags/Continuous-Integration/"},{"name":"Continuous Deployment","slug":"Continuous-Deployment","permalink":"https://feng.lu/tags/Continuous-Deployment/"},{"name":"Release Management","slug":"Release-Management","permalink":"https://feng.lu/tags/Release-Management/"},{"name":"Build","slug":"Build","permalink":"https://feng.lu/tags/Build/"}]},{"title":"OAuth in Azure AD B2C with Nodejs","slug":"OAuth-in-Azure-AD-B2C-with-Nodejs","date":"2017-06-28T12:54:24.000Z","updated":"2025-03-28T09:05:16.881Z","comments":true,"path":"2017/06/28/OAuth-in-Azure-AD-B2C-with-Nodejs/","permalink":"https://feng.lu/2017/06/28/OAuth-in-Azure-AD-B2C-with-Nodejs/","excerpt":"Recently we need to build a Nodejs single-page-application (SPA) solution that is using Azure AD B2C as the identity provider (idp). Since it is a single-page-application, we are going to use OAuth2 Implicit Flow. This article demonstrates the basic steps for setting up both the server side (WebAPI) as well as the client application.","text":"Recently we need to build a Nodejs single-page-application (SPA) solution that is using Azure AD B2C as the identity provider (idp). Since it is a single-page-application, we are going to use OAuth2 Implicit Flow. This article demonstrates the basic steps for setting up both the server side (WebAPI) as well as the client application. Setup your own Azure AD B2CCreate an Azure AD B2C tenantFirst of all, let’s create an AAD B2C tenant with domain luconsultingb2c.onmicrosoft.com by following the steps in this document. Then you can switch by using the top-right menu If you want, you can connect the AAD to an existing Azure subscription Now you can start using this tenant Configure Linkedin as an identity providerFollow the step in Azure Active Directory B2C: Provide sign-up and sign-in to consumers with LinkedIn accounts. Create a Linkedin App to generate the client id and secret Add Linkedin as an identity provider in AAD B2C, together with Email as local accounts Remember to give it a meaningful name for your Linkedin idp, as the name will be used in the login page. (Do not use “LI” as the Microsoft article suggested) Create policy Now create a Signup&#x2F;Signin prolicy by following the steps in Azure Active Directory B2C: Built-in policies. We name this policy “SiUpIn”, and it will be automatically renamed “B2C_1_SiUpIn” (the B2C_1_ fragment is automatically added) When user sign up, I would like to ask for Given Name, Surname Name, Email Address, Display Name, Job Title and Country&#x2F;Region After sign in, I would like to have Display Name, Country&#x2F;Region and Job Title to be included in the claim. In addition, I want to know the idp and if newUser is true, so select them. Done Create and register your WebAPIRegister your WebAPIFollow the steps in Azure Active Directory B2C: Register your application to register a web api named B2CEchoWebAPI Note: Enable the Web App&#x2F;Web API setting for the web api. Reply URL to http://localhost:5000/ as we will run the web api there. HTTP protocol is allowed as far as it is for localhost. If it is external, we have to use HTTPS. Even the App ID URI is marked as optional, this is needed to construct the scopes that are configured in you single page application’s code Setup policyOnce Web API is registered, open the app’s Published Scopes blade and add any extra scopes you want. Note: The scope is used for controlling permissions: an access token defines the permissions that are granted to the bearer of the token. The permissions are defined by you (the API owner) and you control what happens when your API receives a token with that permission. In this case, I define 3 permission levels: “read”, “write” and “performXYZ”. When the API receives a token that has the “read” value in the scope, the API will do actions that I think are okay for a user that has a “read” permission. Microsoft document mentioned that there is a default scope “user_impersonation”, but it is not mandatory. AAD B2C doesn’t care what the permission values are. It simply ensures that scopes that are requested are valid and then generates a proper token when they are valid. It is up to you what you want your API to do when it receives an access token with a permission called “user_impersonation”. The “user_impersonation” scope is there by default because there needs to be at least one scope defined when a user requests a token, and you can use the “user_impersonation” to be that scope. By default, applications are granted the ability to access the user’s profile via the “openid” permission, and generate refresh tokens via the “offline_access” permission. But you do NOT need to specify them here. Write down the AppID URI and Published Scopes values, You will need them in your client application code. The format for calling will be “https:&#x2F;&#x2F;{tenent}&#x2F;{AppID URI}&#x2F;{scope value}”, for example “https://luconsultingb2c.onmicrosoft.com/B2CEchoWebAPI/performXYZ“. Create a Nodejs based web apiIn this case, I am using a forked version of Microsoft sample code, with small modification. You can access it at https://github.com/linkcd/active-directory-b2c-javascript-nodejs-webapi. PackagesIt is using the common passport and passport-azure-ad for AAD strategies. Code for authentication(full code is at https://github.com/linkcd/active-directory-b2c-javascript-nodejs-webapi/blob/master/index.js) 12345678910111213141516171819202122232425262728293031323334var express = require(&quot;express&quot;);var passport = require(&quot;passport&quot;);var BearerStrategy = require(&#x27;passport-azure-ad&#x27;).BearerStrategy;//our tenentvar tenantID = &quot;luconsultingb2c.onmicrosoft.com&quot;; //client id of registered web api: &quot;B2CEchoWebAPI&quot;var clientID = &quot;f40734c1-5990-47fc-91b5-deceebac0089&quot;; //our defined policy, include Linkedin var policyName = &quot;B2C_1_SiUpIn&quot;; var options = &#123; identityMetadata: &quot;https://login.microsoftonline.com/&quot; + tenantID + &quot;/v2.0/.well-known/openid-configuration/&quot;, clientID: clientID, policyName: policyName, isB2C: true, validateIssuer: true, loggingLevel: &#x27;info&#x27;, passReqToCallback: false&#125;;var bearerStrategy = new BearerStrategy(options, function (token, done) &#123; // Send user info using the second argument done(null, &#123;&#125;, token); &#125;);var app = express();app.use(passport.initialize());passport.use(bearerStrategy); Then define API endpoint 1234567891011121314151617181920212223app.use(function (req, res, next) &#123; res.header(&quot;Access-Control-Allow-Origin&quot;, &quot;*&quot;); res.header(&quot;Access-Control-Allow-Headers&quot;, &quot;Authorization, Origin, X-Requested-With, Content-Type, Accept&quot;); next();&#125;);app.get(&quot;/hello&quot;, passport.authenticate(&#x27;oauth-bearer&#x27;, &#123;session: false&#125;), function (req, res) &#123; var claims = req.authInfo; console.log(&#x27;User info: &#x27;, req.user); console.log(&#x27;Validated claims: &#x27;, claims); //do this ONLY if the required scope include &quot;read&quot; if (claims[&#x27;scp&#x27;].split(&quot; &quot;).indexOf(&quot;read&quot;) &gt;= 0) &#123; // Service relies on the name claim. res.status(200).json(&#123;&#x27;name&#x27;: claims[&#x27;name&#x27;]&#125;); &#125; else &#123; console.log(&quot;Invalid Scope, 403&quot;); res.status(403).json(&#123;&#x27;error&#x27;: &#x27;insufficient_scope&#x27;&#125;); &#125; &#125;); Now run it at local And confirm that the endpoint is protected Now the Web API part is done, let’s move to the client part. Create and register your client appRegister your client appFollow the steps in register your single page application in your B2C tenant, so that your client has its own Application&#x2F;client ID. Note: Enable the Web App&#x2F;Web API setting for the web api. Reply URL to http://localhost:6420/ as we will run the client app there. HTTP protocol is allowed as far as it is for localhost. If it is external, we have to use HTTPS. Grant the client app access to the web API Create the client appAgain, I am using a forked version of Microsoft sample code, you can find it at https://github.com/linkcd/active-directory-b2c-javascript-msal-singlepageapp 1234567891011&lt;script class=&quot;pre&quot;&gt; // The current application coordinates were pre-registered in a B2C tenant. var applicationConfig = &#123; clientID: &#x27;df8f3cb5-b668-4e11-a8ca-ad4f78cb87f4&#x27;, authority: &quot;https://login.microsoftonline.com/tfp/luconsultingb2c.onmicrosoft.com/b2c_1_siupin&quot;, //use scope &quot;read&quot;, as it is required in the Web API (see the webapi code in above) b2cScopes: [&quot;https://luconsultingb2c.onmicrosoft.com/B2CEchoWebAPI/read&quot;], webApi: &#x27;http://localhost:5000/hello&#x27;, &#125;;&lt;/script&gt; Test Look at the claim properties As you can see that most of the user properties are from my Linkedin profile. Since this is the first time I signin, the newUser is true. Also verify that the new user is created in AAD B2C EOF. ref https://docs.microsoft.com/en-us/azure/active-directory-b2c/active-directory-b2c-reference-spa","categories":[],"tags":[{"name":"Azure","slug":"Azure","permalink":"https://feng.lu/tags/Azure/"},{"name":"OAuth","slug":"OAuth","permalink":"https://feng.lu/tags/OAuth/"},{"name":"OpenID","slug":"OpenID","permalink":"https://feng.lu/tags/OpenID/"},{"name":"Azure AD B2C","slug":"Azure-AD-B2C","permalink":"https://feng.lu/tags/Azure-AD-B2C/"},{"name":"Nodejs","slug":"Nodejs","permalink":"https://feng.lu/tags/Nodejs/"},{"name":"Authentication","slug":"Authentication","permalink":"https://feng.lu/tags/Authentication/"},{"name":"Authorization","slug":"Authorization","permalink":"https://feng.lu/tags/Authorization/"}]},{"title":"Using python to organize pictures","slug":"Using-python-to-organize-pictures","date":"2017-05-17T17:46:05.000Z","updated":"2025-03-28T09:05:16.902Z","comments":true,"path":"2017/05/17/Using-python-to-organize-pictures/","permalink":"https://feng.lu/2017/05/17/Using-python-to-organize-pictures/","excerpt":"ProblemHaving several digital cameras is fun: you can have different photography experiences. However, organizing pictures is far less interesting, especially if you do not have a consistent process (like naming convention) for archiving. After several years, I end up with hundred thousand pictures sitting in messy huge folders: Nikon_Pictures Backup_SDCard01 100_0302 DCIM_From_Old_Phone 100CANON Backup-Photo etc… The most tricky part, is that I have so many duplicate pictures everywhere due to inconsistent archiving during years. It is so messy that I never dare to manually clean them up. Naturally, the knowledge of programming came to my rescue. This time, it is Python.","text":"ProblemHaving several digital cameras is fun: you can have different photography experiences. However, organizing pictures is far less interesting, especially if you do not have a consistent process (like naming convention) for archiving. After several years, I end up with hundred thousand pictures sitting in messy huge folders: Nikon_Pictures Backup_SDCard01 100_0302 DCIM_From_Old_Phone 100CANON Backup-Photo etc… The most tricky part, is that I have so many duplicate pictures everywhere due to inconsistent archiving during years. It is so messy that I never dare to manually clean them up. Naturally, the knowledge of programming came to my rescue. This time, it is Python. SolutionLong story short, I am using Python 3, and a lib exifread for extracting exif data from pictures. You can install exifread by running below command 1pip install exifread Then, here comes the ~100 line of code: sourceRootFolder and targetRootFolder specify the source and target folders (sorry, I am too lazy to parametric them) It extracts camera model and create a folder with same name, such as NIKON D40. It also works with smart phones, like Nokia 925. It extracts picture date, then create sub-folder with YYYY-MM format. Move the picture to that sub-folder. If there is already a file with same file name in the target folder, move this file to a backup folder. If the backup folder also have the same file, do nothing. It never delete any pictures. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import exifreadimport osimport pdbimport shutilsourceRootFolder = &quot;c:\\\\data\\\\source_photo&quot;targetRootFolder = &quot;c:\\\\data\\\\photo_archive&quot;def Do(root): for dirpath, dirnames, filenames in os.walk(root): for f in filenames: filepath = dirpath + &quot;\\\\&quot; + f processFile(filepath, f)def processFile(sourceFilePath, filename): targetFolder = getTargetFolder(sourceFilePath, filename) processMoveBusiness(sourceFilePath, targetFolder, filename)def getTargetFolder(filepath, filename): currentFile = open(filepath, &#x27;rb&#x27;) tags = exifread.process_file(currentFile) cameraTag = &quot;Image Model&quot; if cameraTag in tags: camera = str(tags[cameraTag]) if not camera: camera = &quot;Unknown&quot; else: camera = &quot;Unknown&quot; shootTimeTag = &quot;EXIF DateTimeOriginal&quot; #shootTimeTag = &quot;EXIF DateTimeDigitized&quot; if shootTimeTag in tags: shootTime = str(tags[shootTimeTag]) if not shootTime: shootTime = &quot;0000:00 0000:0000&quot; else: shootTime = &quot;0000:00 0000:0000&quot; #pdb.set_trace() dateArray = shootTime.split(&quot; &quot;)[0].split(&quot;:&quot;) year = dateArray[0] month = dateArray[1] return targetRootFolder + &quot;\\\\&quot; + camera.strip() + &quot;\\\\&quot; + year + &quot;-&quot; + month def processMoveBusiness(sourceFilePath, targetFolder, filename): targetFolderBackup = targetFolder + &quot;\\\\backup&quot; #make sure target folders exits if not os.path.isdir(targetFolder): os.makedirs(targetFolder) firstAttemptFilePath = targetFolder + &quot;\\\\&quot; + filename print(firstAttemptFilePath) if os.path.isfile(firstAttemptFilePath): #check if we can move to the backup attemptToBackupFilePath = targetFolderBackup + &quot;\\\\&quot; + filename if os.path.isfile(attemptToBackupFilePath): #have the same file in the backup folder #do nothing print (&quot;------[skip]:&quot; + sourceFilePath) else: #move to backup if not os.path.isdir(targetFolderBackup): os.makedirs(targetFolderBackup) shutil.move(sourceFilePath, attemptToBackupFilePath) print (&quot;&gt;&gt;&gt;&gt;&gt;&gt;[backup]:&quot; + sourceFilePath) else: #normal move shutil.move(sourceFilePath, firstAttemptFilePath) print (&quot;[move]:&quot; + sourceFilePath)Do(sourceRootFolder) After 20 min, everything is neat! Happy scripting!","categories":[],"tags":[{"name":"Python","slug":"Python","permalink":"https://feng.lu/tags/Python/"},{"name":"Scripting","slug":"Scripting","permalink":"https://feng.lu/tags/Scripting/"}]},{"title":"Tracking subdomains with Google Tag Manager","slug":"Tracking-subdomains-with-GTM","date":"2017-04-16T19:10:06.000Z","updated":"2025-03-28T09:05:16.902Z","comments":true,"path":"2017/04/16/Tracking-subdomains-with-GTM/","permalink":"https://feng.lu/2017/04/16/Tracking-subdomains-with-GTM/","excerpt":"Recently I am investigating how to track user behaviors across our digital services. We have web applications like: example.com (the company homepage) service-A.example.com (digital service A) service-B.example.com (digital service B) and we are using Google Tag Manager (and Google Analytics)","text":"Recently I am investigating how to track user behaviors across our digital services. We have web applications like: example.com (the company homepage) service-A.example.com (digital service A) service-B.example.com (digital service B) and we are using Google Tag Manager (and Google Analytics) To be straight forward, what we need are: Use Universal Analytics In tag settings of GTM, set cookieDomain‘s value to auto In Referral Exclusion List of GA, add example.com Done For trouble shooting, I recommend to install Google Analytics Debugger (A chrome extension). To test it, you can go to example.com Use GA debugger to find the GA clientId, e.g. 1753731437.1492367410, and It has a cookie _ga with value “GA1.2.1753731437.1492367410“, and the domain is: “.example.com“ Click a link to navigate to service-A.example.com The GA clientID on Service A should be the same, and It has an identical _ga cookie: same value, same domain “.example.com” Directly type URL service-B.example.com Again, the same GA clientID and _ga cookie There some some great articles about this topic. The most useful posts that I found are from lunametrics (cross-domain tracking and sub-domain tracking with a great diagram), and from Simo Ahava (here and here) with deep core code explanation. A post from e-nor is also useful.","categories":[],"tags":[{"name":"Google Tag Manager","slug":"Google-Tag-Manager","permalink":"https://feng.lu/tags/Google-Tag-Manager/"},{"name":"Google Analytics","slug":"Google-Analytics","permalink":"https://feng.lu/tags/Google-Analytics/"}]},{"title":"Setup VIM plugin","slug":"Setup-VIM-plugin","date":"2017-04-09T09:53:53.000Z","updated":"2025-03-28T09:05:16.902Z","comments":true,"path":"2017/04/09/Setup-VIM-plugin/","permalink":"https://feng.lu/2017/04/09/Setup-VIM-plugin/","excerpt":"Time to revisit my VIM plugin system after VIM is upgraded to version 8.0 Previously I was using Vundle but it is bit complicate to set up quickly. This time I am using vim-plug.","text":"Time to revisit my VIM plugin system after VIM is upgraded to version 8.0 Previously I was using Vundle but it is bit complicate to set up quickly. This time I am using vim-plug. Install Vim 8.0 on a clean window 10 systemBy default the installer creates C:\\Program Files (x86)\\Vim\\vim80 (folder) C:\\Program Files (x86)\\Vim_vimrc (file) C:\\Users\\lufeng\\vimfiles (folder. Sometimes the vimfiles folder is installed in C:\\Program Files (x86)\\Vim) Install vim-plugAssume you have curl installed. If not, download the exe file and add the path to environment PATH. 1curl -fLo %USERPROFILE%/vimfiles/autoload/plug.vim --create-dirs https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim This will download and save the file plug.vim in *C:\\Users\\lufeng\\vimfiles\\autoload* Setup your pluginsEdit the C:\\Program Files (x86)\\Vim_vimrc, to insert below at the top of the file 1234567891011121314call plug#begin(&#x27;$HOME/vimfiles/plugged&#x27;)&quot; Make sure you use single quotes&quot; Shorthand notation; fetches https://github.com/junegunn/vim-easy-align&quot; Plug &#x27;junegunn/vim-easy-align&#x27;Plug &#x27;flazz/vim-colorschemes&#x27;Plug &#x27;PProvost/vim-ps1&#x27;Plug &#x27;scrooloose/nerdtree&#x27;Plug &#x27;bling/vim-airline&#x27;Plug &#x27;kien/ctrlp.vim&#x27;Plug &#x27;majutsushi/tagbar&#x27;&quot; Initialize plugin systemcall plug#end() The first line specified where the plugin will be stored 1call plug#begin(&#x27;$HOME/vimfiles/plugged&#x27;) Therefore all plugins are stored in C:\\Users\\lufeng\\vimfiles\\plugged EOF","categories":[],"tags":[{"name":"Vim","slug":"Vim","permalink":"https://feng.lu/tags/Vim/"},{"name":"Plugin","slug":"Plugin","permalink":"https://feng.lu/tags/Plugin/"}]},{"title":"Jump-start ASP.Net Core with Docker","slug":"Jump-start-ASP-Net-Core-with-Docker","date":"2017-03-31T18:15:27.000Z","updated":"2025-03-28T09:05:16.821Z","comments":true,"path":"2017/03/31/Jump-start-ASP-Net-Core-with-Docker/","permalink":"https://feng.lu/2017/03/31/Jump-start-ASP-Net-Core-with-Docker/","excerpt":"Environment SetupI have a typical developers environment: Windows 10 Enterprise X64 (Version 1511, OS build 10586.839). Installed DotNet Core 1.0.1 and VS Code. In VS Code there are two extension installed. Enable Hyper-VVirtualBox is no longer needed! Simply enable the Hyper-V on on Windows 10 by running powershell commands (as Administrator) 12Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Hyper-V -AllDISM /Online /Enable-Feature /All /FeatureName:Microsoft-Hyper-V You might need to change the BIOS setting. Read more at here. Note:The document from Docker also mentioned that the virtualization must be enabled, and said you can verify it in the Task Manager. However, I can not find “Virtualization” label in my Task Manager. But the following steps work fine anyway. Install DockerHead to Docker official site, download and install Docker for Windows. The version I installed was 17.03.1-ce, build c6d412e Community Edition, via Edge channel. Lets verify it.","text":"Environment SetupI have a typical developers environment: Windows 10 Enterprise X64 (Version 1511, OS build 10586.839). Installed DotNet Core 1.0.1 and VS Code. In VS Code there are two extension installed. Enable Hyper-VVirtualBox is no longer needed! Simply enable the Hyper-V on on Windows 10 by running powershell commands (as Administrator) 12Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Hyper-V -AllDISM /Online /Enable-Feature /All /FeatureName:Microsoft-Hyper-V You might need to change the BIOS setting. Read more at here. Note:The document from Docker also mentioned that the virtualization must be enabled, and said you can verify it in the Task Manager. However, I can not find “Virtualization” label in my Task Manager. But the following steps work fine anyway. Install DockerHead to Docker official site, download and install Docker for Windows. The version I installed was 17.03.1-ce, build c6d412e Community Edition, via Edge channel. Lets verify it. Build ASP.Net Core ApplicationWith (new version of) dotnet core, you can simply use command line for creating an web application. 1234C:\\Private&gt;md aspnetcore_dockerC:\\Private&gt;cd aspnetcore_dockerC:\\Private\\aspnetcore_docker&gt;dotnet new mvcC:\\Private\\aspnetcore_docker&gt;dotnet restore Open this foler with VS Code. If it asked to add “Required assets to build and debug”, click Yes. Debug and verifyYou can F5 to make sure the source code works. If you saw error as below: double check your auto-generated .vscode\\launch.json file. It should looks like below 1234567891011121314151617181920212223242526272829303132333435363738394041&#123; &quot;version&quot;: &quot;0.2.0&quot;, &quot;configurations&quot;: [ &#123; &quot;name&quot;: &quot;.NET Core Launch (web)&quot;, &quot;type&quot;: &quot;coreclr&quot;, &quot;request&quot;: &quot;launch&quot;, &quot;preLaunchTask&quot;: &quot;build&quot;, &quot;program&quot;: &quot;$&#123;workspaceRoot&#125;/bin/Debug/netcoreapp1.1/aspnetcore_docker.dll&quot;, &quot;args&quot;: [], &quot;cwd&quot;: &quot;$&#123;workspaceRoot&#125;&quot;, &quot;stopAtEntry&quot;: false, &quot;launchBrowser&quot;: &#123; &quot;enabled&quot;: true, &quot;args&quot;: &quot;$&#123;auto-detect-url&#125;&quot;, &quot;windows&quot;: &#123; &quot;command&quot;: &quot;cmd.exe&quot;, &quot;args&quot;: &quot;/C start $&#123;auto-detect-url&#125;&quot; &#125;, &quot;osx&quot;: &#123; &quot;command&quot;: &quot;open&quot; &#125;, &quot;linux&quot;: &#123; &quot;command&quot;: &quot;xdg-open&quot; &#125; &#125;, &quot;env&quot;: &#123; &quot;ASPNETCORE_ENVIRONMENT&quot;: &quot;Development&quot; &#125;, &quot;sourceFileMap&quot;: &#123; &quot;/Views&quot;: &quot;$&#123;workspaceRoot&#125;/Views&quot; &#125; &#125;, &#123; &quot;name&quot;: &quot;.NET Core Attach&quot;, &quot;type&quot;: &quot;coreclr&quot;, &quot;request&quot;: &quot;attach&quot;, &quot;processId&quot;: &quot;$&#123;command:pickProcess&#125;&quot; &#125; ]&#125; PublishOnce you are happy with your app, compile it with Release mode and save the package to “Publish” folder 1C:\\Private\\aspnetcore_docker&gt;dotnet publish -c Release -o Publish You can also run it without VS code 1C:\\Private\\aspnetcore_docker\\Publish&gt;dotnet aspnetcore_docker.dll Run the application in DockerCreate the dockerfileBe familiar with VS Code short-cut “Ctrl+Shift+P” to popup the Command Palette, then start typing “Docker “ The add dockerfile command will generate 3 files at current root folder: Dockerfile docker-compose.yml docker-compose.debug.yml Note:We only need to work with Dockerfile in this case. You can even remove these 2 yml files. Understand the dockerfileBy default the Dockerfile is like below We will change it to: 1234567FROM microsoft/aspnetcore:1.1.1LABEL Name=aspnetcore_docker Version=0.0.1 ARG source=PublishWORKDIR /appEXPOSE 80COPY $source .ENTRYPOINT dotnet aspnetcore_docker.dll Translate to English, it can be read as FROM: This image is based from the “mother image” microsoft&#x2F;aspnetcore:1.1.1. This image already contains all the dependencies for running .NET Core on Linux, which is prefect for us. LABEL: Add metadata to this image. ARG: Set an variable named “source”, and its value is “Publish“. It is pointing to the published application binary files on local file system. This folder was created by us in above steps. WORKDIR: Set the work directory to “&#x2F;app“ inside of the container, which is a Linux system. EXPOSE: tells Docker to expose port 80 on the container. COPY application binary files from “Publish“ folder in local system to the work directory “&#x2F;app“ in container. ENTRYPOINT: Set the command to execute when the container starts up. It is the same way that you run this application locally in Windows 10.(For detailed document, click here) Build Docker imageAgain Ctrl+Shift+P to build docker image: It helps you to write this command (pay attention to the “ .” at the end) 1docker build -f Dockerfile -t aspnetcore_docker:latest . Verfiy that Docker has this image Run it from Docker!1docker run -d -p 8090:80 -t aspnetcore_docker:latest “-d“: Run container in background and print container ID “-p 8090:80“: Map port 8090 on the host machine to port 80 of the container (Port 80 was specified in Dockerfile) “-t“: Allocate a pseudo-tty. For interactive processes (like a shell), you must use -i -t together in order to allocate a tty for the container process. -i -t is often written -it. “aspnetcore_docker:latest“: Specify the image and version (optional)Note: Anything before the image name is paramter to docker command. Anything after image name will be considered as arguments to the running container process.It means that you should put -d or -p before image name. Now you are running ASP.NET Core from Docker","categories":[],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://feng.lu/tags/Docker/"},{"name":"ASP.Net Core","slug":"ASP-Net-Core","permalink":"https://feng.lu/tags/ASP-Net-Core/"}]},{"title":"Review and manage your web site disk space in Azure","slug":"Review-and-manage-your-web-site-space-in-Azure","date":"2017-02-23T19:07:31.000Z","updated":"2025-03-28T09:05:16.892Z","comments":true,"path":"2017/02/23/Review-and-manage-your-web-site-space-in-Azure/","permalink":"https://feng.lu/2017/02/23/Review-and-manage-your-web-site-space-in-Azure/","excerpt":"ProblemWe have a large distributed system which is hosted in Azure. The front end web application are Azure web sites. From time to time, the web applications were down, due to running out disk space in the Azure web sites. Our operation team would like to quickly identify what are the large files and how we can free up disk space in Azure web sites. Lucky, Azure application service already provides a nice tool for this type of work: Kudu service.","text":"ProblemWe have a large distributed system which is hosted in Azure. The front end web application are Azure web sites. From time to time, the web applications were down, due to running out disk space in the Azure web sites. Our operation team would like to quickly identify what are the large files and how we can free up disk space in Azure web sites. Lucky, Azure application service already provides a nice tool for this type of work: Kudu service. Kudu ServiceKudu is a tool set for troubleshooting and analysis. To access it, simply go to Azure portal -&gt; Your_APP_Service -&gt; Development tools -&gt; Advanced Tools. Another shortcut is directly access https://Your_APP_Service_URL.SCM.azurewebsites.net. Note: you need contributor or owner permission to access kudu, as you can do almost anything to your site. The kudu landing page is like below There are many possibilities with Kudu such as Execute cmd and&#x2F;or powershell Check environment variables Browse processes Browse files Update files (Upload, download, edit and delete files) Install extensions but in this post I will focus on the monitoring of disk space. Alternative 1: You can run powershell script to list the folder size. Alternative 2: Install the disk space extension Once the extension is installed, there is a nice diagram showing how the disk space is used. Root causeNow the problem is clear: The search functionality in this solution is powered by a lucene library. Whenever the data source is updated, the lucene instance in the Azure web site node will re-index the data, generate index files and store them locally. Due to some issues, the lucene instance is generating new index files every single time without removing old index files. These old index files quickly ate up all disk space and eventually bring the site down. SolutionOnce we have identified these out-of-date index files, it is fairly easy for our operation team to delete them by using Kudu debug console. But this process is not ideal since the team need to check the disk space site regularly. To address that, we created a disk space dashboard by using Application Insight and Azure web site API. Now it is much easier to avoid similar issue in the future. PS: And our smart team fixed the lucene issue afterwards. :)","categories":[],"tags":[{"name":"Azure","slug":"Azure","permalink":"https://feng.lu/tags/Azure/"},{"name":"Monitoring","slug":"Monitoring","permalink":"https://feng.lu/tags/Monitoring/"},{"name":"DevOps","slug":"DevOps","permalink":"https://feng.lu/tags/DevOps/"},{"name":"KUDU","slug":"KUDU","permalink":"https://feng.lu/tags/KUDU/"}]},{"title":"Announcing new version of DNV GL Rules Chatbot","slug":"Announcing-new-version-of-DNV-GL-Rules-Chatbot","date":"2017-01-07T18:40:31.000Z","updated":"2025-03-28T10:26:55.649Z","comments":true,"path":"2017/01/07/Announcing-new-version-of-DNV-GL-Rules-Chatbot/","permalink":"https://feng.lu/2017/01/07/Announcing-new-version-of-DNV-GL-Rules-Chatbot/","excerpt":"Happy new year! I am happy to announce the new version of Rules Chatbot is ready now. The key new features are below: Support full text search (based on SharePoint Search) Indexing DNV GL classification rules documents with enhanced metadata Customized ranking Upgraded to Bot Framework 3.0 URL:https://dnvgl-rules-bot.azurewebsites.net","text":"Happy new year! I am happy to announce the new version of Rules Chatbot is ready now. The key new features are below: Support full text search (based on SharePoint Search) Indexing DNV GL classification rules documents with enhanced metadata Customized ranking Upgraded to Bot Framework 3.0 URL:https://dnvgl-rules-bot.azurewebsites.net live preview Demo Architect OverviewThe architect of the chat bot is shown in below 1. Bot framework and channelsBot framework out-of-the-box supports several conversation channels, but you might noticed that they are more consumer based channels. Skype for Business, for example, is not supported at this moment. 2. Chatbot Web API application in AzureChatbot Web API is a simple standard ASP.Net Web API project. It has two key class: MessagesController RulesAndStandardsDialog MessagesController classThis is the end-point for listening all incoming messages, and forward to RulesAndStandardsDialog class 123456789101112131415161718192021222324[BotAuthentication]public class MessagesController : ApiController&#123; /// &lt;summary&gt; /// POST: api/Messages /// Receive a message from a user and reply to it /// &lt;/summary&gt; /// public async Task&lt;HttpResponseMessage&gt; Post([FromBody]Activity activity) &#123; ConnectorClient connector = new ConnectorClient(new Uri(activity.ServiceUrl)); if (activity != null &amp;&amp; activity.Type == ActivityTypes.Message) &#123; await Conversation.SendAsync(activity, () =&gt; new DNVGLBot.Controllers.RulesAndStandardsDialog()); &#125; else &#123; return null; //HandleSystemMessage &#125; return new HttpResponseMessage(System.Net.HttpStatusCode.Accepted); &#125;&#125; RulesAndStandardsDialog classThis class is responsible for consuming the result from LUIS web service and perform actual task. For example, [LuisIntent(“bot.intent.search”)] attribute means that the SearchDocument function will be called once the bot.intent.search intent was identified by LUIS. 12345678910111213141516171819202122232425262728293031323334353637[LuisModel(&quot;YOUR-LUIS-MODEL-ID&quot;, &quot;YOUR-SUBSCRIPTION-ID&quot;)][Serializable]public class RulesAndStandardsDialog: LuisDialog&lt;object&gt;&#123; //Some lines of code were omitted [LuisIntent(&quot;bot.intent.search&quot;)] public async Task SearchDocument(IDialogContext context, LuisResult result) &#123; string keywords = TryFindKeywords(result); if (!string.IsNullOrEmpty(keywords)) &#123; await DoSearch(context); &#125; else &#123; await context.PostAsync(&quot;Sorry, I can not find any keywords, please try again.&quot;); context.Wait(MessageReceived); &#125; &#125; [LuisIntent(&quot;bot.help&quot;)] public async Task Help(IDialogContext context, LuisResult result) &#123; await context.PostAsync(GetHelpMessage()); context.Wait(MessageReceived); &#125; [LuisIntent(&quot;bot.about&quot;)] public async Task About(IDialogContext context, LuisResult result) &#123; await context.PostAsync(GetAboutMessage()); context.Wait(MessageReceived); &#125;&#125; 3. Storage account in AzureChatbot is using Azure storage account (table) for storing end user profiles, such as Full name, email address and subscribed rules codes. It is not a mandatory component for the bot, but simply an user profile database. 4. Luis in Cognitive servicesLuis model is the brain of the chat bot. It is pretty easy to setup and training it without programming skill. Based on the experience, 100 sample inputs can produce a reasonable AI. Tips: Remember to re-publish Luis endpoint once a while during the training process, otherwise the client (bot) won’t pick up the newly trained sample. 5. Search engine in SharePointOnce Luis has identified the search intent and the to-be-search keywords, the information will be passed to the search engine for query. In this case, the search engine is SharePoint. All rules pdf files are indexed by SharePoint search engine - it provides good support for full text search and meta-data enhancement (via Content Enrichment Web Service during the indexing process). Using a search engine is the biggest improvement compares to older version, where the search logic was simply matching words in rules title. I am using SharePoint PnP framework to handle the authentication to the on-premise SharePoint farm, which is protected by ADFS. Also, I have learned from our expert Mikael Svenson to add the freshness boost to the result. Unfortunately built-in date variables do not work in C#&#x2F;KeywordQuery scenario, therefore I have rewrite the code as below: 123456789101112131415161718192021222324252627282930string samlSite = &quot;https://SHAREPOINTURL&quot;;//XRank for boosting fresh contentstring daysAgo580 = DateTime.Now.AddDays(-580).ToString(&quot;yyyy-MM-dd&quot;);string daysAgo365 = DateTime.Now.AddDays(-365).ToString(&quot;yyyy-MM-dd&quot;);string daysAgo180 = DateTime.Now.AddDays(-180).ToString(&quot;yyyy-MM-dd&quot;);string daysAgo90 = DateTime.Now.AddDays(-90).ToString(&quot;yyyy-MM-dd&quot;);string daysAgo60 = DateTime.Now.AddDays(-60).ToString(&quot;yyyy-MM-dd&quot;);string daysAgo30 = DateTime.Now.AddDays(-30).ToString(&quot;yyyy-MM-dd&quot;);string daysAgo16 = DateTime.Now.AddDays(-16).ToString(&quot;yyyy-MM-dd&quot;);string daysAgo7 = DateTime.Now.AddDays(-7).ToString(&quot;yyyy-MM-dd&quot;);string daysAgo1 = DateTime.Now.AddDays(-1).ToString(&quot;yyyy-MM-dd&quot;);string daysAgo0 = DateTime.Now.ToString(&quot;yyyy-MM-dd&quot;);OfficeDevPnP.Core.AuthenticationManager am = new OfficeDevPnP.Core.AuthenticationManager();using (ClientContext ctx = am.GetADFSUserNameMixedAuthenticatedContext(samlSite, &quot;USERNAME&quot;, &quot;PASSWORD&quot;, &quot;DOMAIN&quot;, &quot;STS&quot;, &quot;IDPID&quot;))&#123; KeywordQuery keywordQuery = new KeywordQuery(ctx); keywordQuery.QueryText = string.Format(&quot;(((((((((&#123;0&#125; contentsource:\\&quot;dnvglrules\\&quot; XRANK(cb=1) title:\\&quot;&#123;0&#125;\\&quot;) XRANK(cb=04922713399625873E-17) write&gt;&#123;1&#125;) XRANK(cb=026792479063910794E-18) write&gt;&#123;2&#125;) XRANK(cb=06696008382287308E-17) write&gt;&#123;3&#125;) XRANK(cb=10720794384750529E-17) write&gt;&#123;4&#125;) XRANK(cb=08336806307198713E-17) write&gt;&#123;5&#125;) XRANK(cb=16669442125999617E-17) write&gt;&#123;6&#125;) XRANK(cb=3107141114146401E-16) write&gt;&#123;7&#125;) XRANK(cb=15680891749553738E-17) write&gt;&#123;8&#125;) XRANK(cb=03222684602729131E-17) write&gt;&#123;9&#125;&quot;, keyword, daysAgo580, daysAgo365, daysAgo180, daysAgo90, daysAgo60, daysAgo30, daysAgo7, daysAgo1, daysAgo0); keywordQuery.RowLimit = 5; //return 5 results everytime, since the dialog screen will be small, e.g. from a mobile keywordQuery.StartRow = startRow; SearchExecutor searchExecutor = new SearchExecutor(ctx); ClientResult&lt;ResultTableCollection&gt; results = searchExecutor.ExecuteQuery(keywordQuery); ctx.ExecuteQuery(); return results; &#125; Side notes for developer on Windows 10 Enabling Identity Foundation Framework on windows 10One of the dependency component of the PnP nuget package is Identity Foundation Framework. You cannot download an installer for windows 10, but simply enable the feature in windows 10.Go to Control Panel -&gt; Uninstall a Program on the menu -&gt; Turn Windows Features On or Off -&gt; Check “Windows Identity Framework 3.5” Install Microsoft.IdentityModel.Extensions.dll libraryAnother dependency is Microsoft.IdentityModel.Extensions.dll. Download 32-bit here and 64-bit here. Final notesSecurity is important aspect for any bot service, therefore pay extra attention about how you expose data via your bot service. (Read more at here) It also the same for the privacy of your end users. DisclaimerThis chatbot prototype is a personal project. It is only based on the public information of the DNV GL rules. It has NO connection with DNV GL AS. All information from the chatbot is presented ‘as-is’. No warranties whatsoever for correctness; completeness or usefulness.","categories":[],"tags":[{"name":"Chatbot","slug":"Chatbot","permalink":"https://feng.lu/tags/Chatbot/"},{"name":"Artificial Intelligence","slug":"Artificial-Intelligence","permalink":"https://feng.lu/tags/Artificial-Intelligence/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://feng.lu/tags/Machine-Learning/"},{"name":"Microsoft language understanding intelligent service","slug":"Microsoft-language-understanding-intelligent-service","permalink":"https://feng.lu/tags/Microsoft-language-understanding-intelligent-service/"},{"name":"Prototype","slug":"Prototype","permalink":"https://feng.lu/tags/Prototype/"},{"name":"SharePoint PnP","slug":"SharePoint-PnP","permalink":"https://feng.lu/tags/SharePoint-PnP/"}]},{"title":"No agent pool found with identifier 0 error when setup build agent in VSTS","slug":"No-agent-pool-found-with-identifier-0-error-when-setup-build-agent-in-VSTS","date":"2016-11-19T14:39:32.000Z","updated":"2025-03-28T09:05:16.881Z","comments":true,"path":"2016/11/19/No-agent-pool-found-with-identifier-0-error-when-setup-build-agent-in-VSTS/","permalink":"https://feng.lu/2016/11/19/No-agent-pool-found-with-identifier-0-error-when-setup-build-agent-in-VSTS/","excerpt":"ProblemWe cannot register a private build agent on VSTS by using a service account. This service account has created Personal Access Token with expiration for 1 year and authorization for all scopes. Whenever we run the config.cmd, then connecting to the server, type the Agent Pool and Agent Name, configuration command throws error. No agent pool found with identifier 0. Failed to add the agent. Try again or ctrl-c to quit However, with the same build server and with another developer’s account, it works fine and the build agent is up and running.","text":"ProblemWe cannot register a private build agent on VSTS by using a service account. This service account has created Personal Access Token with expiration for 1 year and authorization for all scopes. Whenever we run the config.cmd, then connecting to the server, type the Agent Pool and Agent Name, configuration command throws error. No agent pool found with identifier 0. Failed to add the agent. Try again or ctrl-c to quit However, with the same build server and with another developer’s account, it works fine and the build agent is up and running. Root CauseIt turns out that the service account is Stakeholder role in VSTS. Users defined as Stakeholder do not have access to build components such as build agents private or hosted. Documentation is at there. It also explained that using the developers account can work fine, since it has MSDN subscription. Regardless, the error message is somewhat misleading. SolutionWe purchased a Basic Account for this service account, and it works perfectly.","categories":[],"tags":[{"name":"Troubleshooting","slug":"Troubleshooting","permalink":"https://feng.lu/tags/Troubleshooting/"},{"name":"VSTS","slug":"VSTS","permalink":"https://feng.lu/tags/VSTS/"},{"name":"Build","slug":"Build","permalink":"https://feng.lu/tags/Build/"}]},{"title":"Work with custom event in Application Insights","slug":"Work-with-custom-event-in-Application-Insights","date":"2016-11-06T18:40:15.000Z","updated":"2025-03-28T09:05:16.912Z","comments":true,"path":"2016/11/06/Work-with-custom-event-in-Application-Insights/","permalink":"https://feng.lu/2016/11/06/Work-with-custom-event-in-Application-Insights/","excerpt":"Application Insights can offer you lots of built-in telemetries such as Page Views and Exceptions. But quite often we need to track some customize&#x2F;business performances. Some examples that we are using now are: SharePoint crawled items from content source X Daily usage of a web application of all users from company ABC Usage and performance of different version of API To meet these challenges, Application Insights offers API for custom event and metrics. In this article I will show how to monitor and analysis API performance by using custom events, including export it into external database for archiving and future analysis.","text":"Application Insights can offer you lots of built-in telemetries such as Page Views and Exceptions. But quite often we need to track some customize&#x2F;business performances. Some examples that we are using now are: SharePoint crawled items from content source X Daily usage of a web application of all users from company ABC Usage and performance of different version of API To meet these challenges, Application Insights offers API for custom event and metrics. In this article I will show how to monitor and analysis API performance by using custom events, including export it into external database for archiving and future analysis. Report eventsOur ASP.Net Web API application is already instrumented with Application Insights. The goal is to also report the performance and usage of API in below format. By following the documentation, we have below code in the server side for sending the performance data of each API request: 12345678910var properties = new Dictionary&lt;string, string&gt;&#123; &#123; &quot;ElapsedMilliseconds&quot;, apiCall.ElapsedMilliseconds.ToString()&#125;, &#123; &quot;ContractName&quot;, apiCall.ContractName&#125;, &#123; &quot;AbsoluteUri&quot;, apiCall.AbsoluteUri&#125;, &#123; &quot;Action&quot;, apiCall.Action &#125;, &#123; &quot;UserId&quot;, apiCall.UserId &#125;, &#123; &quot;AccountId&quot;, apiCall.AccountId &#125;, &#123; &quot;AccountName&quot;, apiCall.AccountName &#125;&#125;;_appInsightTelemetryClient.TrackEvent(apiCall.ContractName + &quot; - &quot; + apiCall.Action, properties); Then you can verify the raw data you got in Application Insights And also in the analytics You can see that all customized attributes in this event are in customDimensions object. Analysis eventsFrom here, you can run analytic queries, such as Find the 5 most slowest API actions in last 24 hours1234567customEvents | where timestamp &gt;= ago(24h) | extend action = tostring(customDimensions.[&quot;Action&quot;])| extend ElapsedSeconds = todouble(customDimensions.[&quot;ElapsedMilliseconds&quot;]) / 1000| summarize avg(ElapsedSeconds) by action| top 5 by avg_ElapsedSeconds desc| render barchart Find the most popular API actions in last 7 days123456customEvents | where timestamp &gt;= ago(7d) | extend action = tostring(customDimensions.[&quot;Action&quot;])| summarize count() by action| order by count_ desc | render barchart Percentile of one action “GetUser” in last 7 days123456customEvents | where timestamp &gt;= ago(7d) | where customDimensions.[&quot;Action&quot;] == &quot;GetUser&quot; | extend ElapsedSeconds = todouble(customDimensions.[&quot;ElapsedMilliseconds&quot;]) / 1000 | summarize percentiles(ElapsedSeconds, 50, 90, 95) by bin(timestamp, 1h) | render timechart Export to SQL databaseAs I described in my previous blog Application Insights Export and PowerBI, you can export custom event telemetry data to a SQL database. Create the database table12345678910111213141516CREATE TABLE [dbo].[mywebapi-ai-events] ( [ID] varchar(50), [TimeStamp] varchar(50), ElapsedMilliseconds varchar(20), AbsoluteUri varchar(300), [UserId] varchar(200), [Action] varchar(100), [AccountId] varchar(200), ContractName varchar(200), AccountName varchar(200), CONSTRAINT [PK_Source] PRIMARY KEY CLUSTERED ( [ID] ASC ) ) Setup input in Streaming Analytics Setup output in Streaming Analytics Setup query in Streaming Analytics12345678910111213SELECT C.internal.data.id as ID , C.context.data.eventTime as [TimeStamp] ,GetRecordPropertyValue(GetArrayElement(C.context.custom.dimensions, 0),&#x27;ElapsedMilliseconds&#x27;) as ElapsedMilliseconds ,GetRecordPropertyValue(GetArrayElement(C.context.custom.dimensions, 1),&#x27;AbsoluteUri&#x27;) as AbsoluteUri ,GetRecordPropertyValue(GetArrayElement(C.context.custom.dimensions, 2),&#x27;UserId&#x27;) as UserId ,GetRecordPropertyValue(GetArrayElement(C.context.custom.dimensions, 3),&#x27;Action&#x27;) as Action ,GetRecordPropertyValue(GetArrayElement(C.context.custom.dimensions, 4),&#x27;AccountId&#x27;) as AccountId ,GetRecordPropertyValue(GetArrayElement(C.context.custom.dimensions, 5),&#x27;ContractName&#x27;) as ContractName ,GetRecordPropertyValue(GetArrayElement(C.context.custom.dimensions, 6),&#x27;AccountName&#x27;) as AccountNameINTO [output-ai-apicustomevents]FROM [input-ai-apicustomevents] C Also see some dicussion here Finally the data will be stored in database","categories":[],"tags":[{"name":"Azure","slug":"Azure","permalink":"https://feng.lu/tags/Azure/"},{"name":"Application Insights","slug":"Application-Insights","permalink":"https://feng.lu/tags/Application-Insights/"},{"name":"Monitoring","slug":"Monitoring","permalink":"https://feng.lu/tags/Monitoring/"},{"name":"PowerBI","slug":"PowerBI","permalink":"https://feng.lu/tags/PowerBI/"},{"name":"DevOps","slug":"DevOps","permalink":"https://feng.lu/tags/DevOps/"}]},{"title":"Web/Load testing against an ADFS federated application","slug":"Web-testing-and-load-testing-against-an-ADFS-federated-application","date":"2016-10-29T18:25:07.000Z","updated":"2025-03-28T09:05:16.902Z","comments":true,"path":"2016/10/29/Web-testing-and-load-testing-against-an-ADFS-federated-application/","permalink":"https://feng.lu/2016/10/29/Web-testing-and-load-testing-against-an-ADFS-federated-application/","excerpt":"In Application Insights, you can create an availability web testing to monitor the availability of a web application. It supports 2 type of testing: URL ping test: a simple test that you can create in the Azure portal. Multi-step web test: which you create in Visual Studio Ultimate or Visual Studio Enterprise and upload to the portal. Normally it is easy to setup ping test and multiple step testing against a public site. However, in order to have a multiple step testing against an ADFS federated application, you will have to do some extra in order to take care of the authentication part.","text":"In Application Insights, you can create an availability web testing to monitor the availability of a web application. It supports 2 type of testing: URL ping test: a simple test that you can create in the Azure portal. Multi-step web test: which you create in Visual Studio Ultimate or Visual Studio Enterprise and upload to the portal. Normally it is easy to setup ping test and multiple step testing against a public site. However, in order to have a multiple step testing against an ADFS federated application, you will have to do some extra in order to take care of the authentication part. The problemBy default, the test scripts that you recorded via Visual studio cannot handle the ADFS authentication. Depends on the version of your Visual Studio, the generated script either has no token, or has a token with a timestamp. If you setup the multiple step web test with this script, after the token with a timestamp expired, you will see error message such as “System.IdentityModel.Tokens.SecurityTokenExpiredException at System.IdentityModel.Tokens.SamlSecurityTokenHandler.ValidateToken”, “The SamlSecurityToken is rejected because the SamlAssertion.NotOnOrAfter condition is not satisfied. NotOnOrAfter: ‘10&#x2F;24&#x2F;2016 2:03:42 PM’ Current time: ‘10&#x2F;27&#x2F;2016 1:20:52 AM’ “ SolutionSteps of ADFS login Redirect to ADFS for login If the current session does not have an valid ADFS token, the end user will be automatically redirected to the ADFS login page. Pay attention to the hyperlink in the response body. In the ADFS login form, user types the account name and password, then submit. The input values can be found in step 3 HTTP request body in below. HTTP POST the user name and password to ADFS URL for verification. If the credential is valid, generate a response HTML which triggers a HTTP POST back to the application URL. The values in the HTTP POST can be found in Step 5 screenshot in below. As the result of the generated HTTP POST to application URL, User got the authentication token to login Setup scriptIn the script, we have following setup: (Please note that the script is accessing a specify application URL: “&#x2F;notifications”, instead of the root URL “&#x2F;“) Conduct a directly POST call to ADFS URL Querystring: values for protocol(wa), application information(wtrealm and wctx) and RedirectToIdentityProvider Form: authenticaiton method, account name and password Once we got the response, extract from context parameter _cert Conduct a directly POST call to application URL , with values that we extracted from previous call to ADFS From now on, you can continue the testing with more URLs of the application, without passed values from hidden fields as we did in above. Successed tests Load TestingAs far as the web testing script is ready, you can quickly load them into the load test cases. RefThere are two similar posts about authentication of ADFS: http://southworks.com/blog/2013/01/03/load-testing-adfs-federated-sharepoint-applications/ https://blogs.msdn.microsoft.com/zwsong/2014/07/23/load-testing-saml-ping-based-sharepoint-2013-sites/","categories":[],"tags":[{"name":"Application Insights","slug":"Application-Insights","permalink":"https://feng.lu/tags/Application-Insights/"},{"name":"ADFS","slug":"ADFS","permalink":"https://feng.lu/tags/ADFS/"},{"name":"Web Testing","slug":"Web-Testing","permalink":"https://feng.lu/tags/Web-Testing/"},{"name":"Load Testing","slug":"Load-Testing","permalink":"https://feng.lu/tags/Load-Testing/"}]},{"title":"Application Insights Export and PowerBI","slug":"Application-Insights-Export-and-PowerBI","date":"2016-10-15T12:36:25.000Z","updated":"2025-03-28T09:05:16.459Z","comments":true,"path":"2016/10/15/Application-Insights-Export-and-PowerBI/","permalink":"https://feng.lu/2016/10/15/Application-Insights-Export-and-PowerBI/","excerpt":"IntroductionApplication Insight is a great tool for monitoring your application. However, there is a limitation regarding historical data: Regardless the different plans, you can only have: 7 days raw data, includes access to all telemetry data points collected by application insights 90 days aggregated data, includes access to telemetry data points aggregated at hourly&#x2F;daily levels This article will demonstrate how to use the continuous data export to overcome this limitation, as well as use PowerBI for future web analytic.","text":"IntroductionApplication Insight is a great tool for monitoring your application. However, there is a limitation regarding historical data: Regardless the different plans, you can only have: 7 days raw data, includes access to all telemetry data points collected by application insights 90 days aggregated data, includes access to telemetry data points aggregated at hourly&#x2F;daily levels This article will demonstrate how to use the continuous data export to overcome this limitation, as well as use PowerBI for future web analytic. Export the data#There is a nice document for enabling the continues export from Application Insights. Create a storage account##Currently, Application Insights can only export to a classic storage account. When you are creating it, I suggest to use “Standard-LRS“ with Cold tier. This is simply because we use storage account as a temporary storage place, before we move everything to a database. Configure the export settingsIn this case, we are exporting two types: Availability Page View. Once it is done and the telemetry data started exporting, you should be able to see folders in the blob container in storage account. One folder is mapping to one data type in above export setting. Take a closer look at the folder content. Below screenshot shows the PageViews data between 12am to 1pm on 28th of Sept. Something keep in mind: The date and time are UTC and are when the telemetry was deposited in the store - not the time it was generated. Each blob is a text file that contains multiple ‘\\n’-separated rows. It contains the telemetry processed over a time period of roughly half a minute. It means that each folder will have about 100 to 120 files. Each row represents a telemetry data point such as a request or page view. Each row is an JSON document. The detailed data structure can be found at here. With the nice Azure Storage Account Explorer, it is pretty easy to check the content of the blob file. Please note that Application Insights also implemented the same logic as IP Anonymization in Google Analytics. For all IP address that is collected by Application Insights, last octagon is anonymized to 0 (you can see the highlighted in above screenshot). Transfer and store the data#They are many ways to transfer the data out from storage account. Here we are going to use Streaming Analytics. Once the streaming analytic instance was created, setup the inputs. We will need 2 inputs for both Availability and PageViews. This is because their data structures are different, and requires different handling. Setup inputsTalk about the Path Pattern firstAccording to the document, the Path Pattern property of input should follow format (yourApplicationInsightName)_(yourkey)&#x2F;(yourTelemtryType)&#x2F;{date}&#x2F;{time}. Firstly, the (yourApplicationInsightName)_(yourkey) part can be simply copied from the storage account page. Secondly, please note that the yourTelemtryType (in this case, PageViews and Availability) is case sensitive! Finally, make sure the date format is “YYYY-MM-DD”, not the default “YYYY&#x2F;MM&#x2F;DD”. Input of PageViews and Availability Test the sample dataYou should always verify the input settings by test the sample data. For example, if we forgot the set data format, you will get below error. ** Keep the downloaded sample data for later usage ** Error message:No events found for ‘ai-pageviews’. Start time: Thursday, September 29, 2016, 1:00:00 AM End time: Friday, September 30, 2016, 1:00:00 AM Last time arrival: Thursday, January 1, 1970, 1:00:00 AM Diagnostics: While sampling data, no data was received from ‘1’ partitions. Setup outputsCreate an Azure SQL databaseLet’s create an Azure SQL database. In this case I have created one with the cheapest plan: 40 NOK&#x2F;Month, and up to 2GB storage.Make sure you setup the firewall that allows Azure service access. It will be used by Streaming Analytic Job and Power BI. Your client IP address. You will need that for creating tables. Create table schemaAccording to the telemetry data structure definition, you can create a DB table that stores the telemetry data which you are interested. AvailabilityNow let’s create a table that stores availability.You can follow above document, or simply have a look at the data you have, and only pickup the fields that you need, as below (Needed fields are highlighted) 12345678910111213CREATE TABLE [dbo].[ai-availability]( [ID] varchar(50), timestamp datetime, name varchar(50), location varchar(50), message varchar(300), duration float, CONSTRAINT [PK_Source_availability] PRIMARY KEY CLUSTERED ( [ID] ASC )) PageViews 12345678910111213141516171819202122CREATE TABLE [dbo].[ai-pageviews]( [ID] varchar(50), timestamp datetime, name varchar(50), url varchar(300), duration float, operation_Name varchar(100), device_type varchar(25), device_OS varchar(25), device_name varchar(25), device_model varchar(25), browser varchar(25), browser_version varchar(25), client_country varchar(25), client_province varchar(25), client_city varchar(25), CONSTRAINT [PK_Source_pageViews] PRIMARY KEY CLUSTERED ( [ID] ASC )) Setup outputs in Streaming Analytic Setup transferringNow we are ready to setup the transferring query. At this moment, the new Azure portal does not support query test (always grayed out), so let’s head to Azure classic portal, and test our query.The detailed input data structure can be found in the previously downloaded sample file. Setup queryBelow query will transfer telemetry data of both Availability and Pageviews. 1234567891011121314151617181920212223242526272829303132SELECT A.internal.data.id as ID ,GetRecordPropertyValue(GetArrayElement(A.[availability], 0), &#x27;testTimestamp&#x27;) as [TimeStamp] ,GetRecordPropertyValue(GetArrayElement(A.[availability], 0), &#x27;testName&#x27;) as name ,GetRecordPropertyValue(GetArrayElement(A.[availability], 0), &#x27;runLocation&#x27;) as location ,SUBSTRING(GetRecordPropertyValue(GetArrayElement(A.[availability], 0), &#x27;message&#x27;), 1, 300) as message ,GetRecordPropertyValue(GetRecordPropertyValue(GetArrayElement(A.[availability], 0), &#x27;durationMetric&#x27;), &#x27;value&#x27;) as [Duration]INTO [output-ai-availability]FROM [input-ai-availability] ASELECT P.internal.data.id as ID , P.context.data.eventTime as [TimeStamp] ,GetRecordPropertyValue(GetArrayElement(P.[view], 0),&#x27;Name&#x27;) as Name ,GetRecordPropertyValue(GetArrayElement(P.[view], 0),&#x27;url&#x27;) as url ,GetRecordPropertyValue(GetRecordPropertyValue(GetArrayElement(P.[view], 0),&#x27;durationMetric&#x27;), &#x27;value&#x27;) as [duration] ,p.context.operation.name as operation_name ,P.context.device.type as device_type ,P.context.device.osVersion as device_OS ,P.context.device.deviceName as device_name ,P.context.device.deviceModel as device_model ,P.context.device.browser as browser ,P.context.device.browserVersion as browser_version ,P.context.location.country as client_country ,P.context.location.province as client_province ,P.context.location.city as client_cityINTO [output-ai-pageviews]FROM [input-ai-pageviews] P Verify the queryUse the test function to ensure the query is OK. Test runNow it’s time to have a test run: start your stream analytic job and pay attend to the input vs output numbers: they should be approximately 1:1. If not, check the logs: As you can see, the problem is the string we got from input is too long. A quick fix is to reduce the length during the transfering as below: 12345678910111213141516171819SELECT P.internal.data.id as ID , P.context.data.eventTime as [TimeStamp] ,SUBSTRING(GetRecordPropertyValue(GetArrayElement(P.[view], 0),&#x27;Name&#x27;), 1, 50) as Name ,SUBSTRING(GetRecordPropertyValue(GetArrayElement(P.[view], 0),&#x27;url&#x27;), 1, 300) as url ,GetRecordPropertyValue(GetRecordPropertyValue(GetArrayElement(P.[view], 0),&#x27;durationMetric&#x27;), &#x27;value&#x27;) as [duration] ,SUBSTRING(p.context.operation.name, 1, 100) as operation_name ,SUBSTRING(P.context.device.type, 1, 15) as device_type ,SUBSTRING(P.context.device.osVersion, 1, 15) as device_OS ,SUBSTRING(P.context.device.deviceName, 1, 15) as device_name ,SUBSTRING(P.context.device.deviceModel, 1, 15) as device_model ,SUBSTRING(P.context.device.browser, 1, 15) as browser ,SUBSTRING(P.context.device.browserVersion, 1, 20) as browser_version ,SUBSTRING(P.context.location.country, 1, 15) as client_country ,SUBSTRING(P.context.location.province, 1, 15) as client_province ,SUBSTRING(P.context.location.city, 1, 15) as client_cityINTO [output-ai-pageviews]FROM [input-ai-pageviews] P Now you are done with the stream analytic part, and all telemetry data will be continuously exported to your Azure SQL database. Visualize your dataPowerBI is a good tool for data visualization and sharing. Before we dive into PowerBI, we will have to do some preparation as below: Creating a time seriesIn order to show the data in time series, we will have to create a temporary dataset, which contains hour-by-hour time series in last 7 days. 12345678910111213141516171819202122232425262728293031DROP FUNCTION PreviousHourSeries;GOCREATE FUNCTION PreviousHourSeries()RETURNS @ret TABLE (HourlySeries datetime)AS BEGINdeclare @EndTime datetime = dateadd(HOUR, datediff(HOUR, 0, getdate()), 0), @StartTime datetime = DATEADD(DAY, -7, dateadd(HOUR, datediff(HOUR, 0, getdate()), 0)) --include previous 7 days ;WITH cSequence AS( SELECT @StartTime AS StartRange, DATEADD(HOUR, 1, @StartTime) AS EndRange UNION ALL SELECT EndRange, DATEADD(HOUR, 1, EndRange) FROM cSequence WHERE EndRange &lt;= @EndTime --WHERE DATEADD(HOUR, 0, EndRange) &lt;= @EndTime)/* insert into tmp_IRange */INSERT INTO @ret SELECT StartRange FROM cSequence OPTION (MAXRECURSION 0);RETURN;END;GO Start working on PowerBI You will connect PowerBI desktop to the Azure SQL database, includes pageviews and availability tables, plus the time series function. Format the timestamp column in a format that can join with time series table. You will have to remove the hours from the timestamp, such as Availability: TimeStampInHourFormat &#x3D; FORMAT(‘ai-availability’[timestamp], “yyyy-mm-dd HH:00”) PageViews: TimeStampInHourFormat &#x3D; FORMAT(‘ai-pageviews’[timestamp], “yyyy-mm-dd HH:00”) PreviousHourSeries: HourlySeriesInFormat &#x3D; FORMAT([HourlySeries], “yyyy-mm-dd HH:00”) Manage the relationship between them Start creating charts. Remember to set HourlySeriesInFormat to “show item without data”, and hide items that HourlySeriesInFormat is Blank (means it is out of the time series scope. It is too old)","categories":[],"tags":[{"name":"Azure","slug":"Azure","permalink":"https://feng.lu/tags/Azure/"},{"name":"Application Insights","slug":"Application-Insights","permalink":"https://feng.lu/tags/Application-Insights/"},{"name":"Monitoring","slug":"Monitoring","permalink":"https://feng.lu/tags/Monitoring/"},{"name":"PowerBI","slug":"PowerBI","permalink":"https://feng.lu/tags/PowerBI/"},{"name":"DevOps","slug":"DevOps","permalink":"https://feng.lu/tags/DevOps/"}]},{"title":"A chatbot prototype - DNV GL rules and standard chatbot","slug":"A-chatbot-prototype-DNV-GL-rules-and-standard-chatbot","date":"2016-09-25T06:06:15.000Z","updated":"2025-03-28T09:05:16.449Z","comments":true,"path":"2016/09/25/A-chatbot-prototype-DNV-GL-rules-and-standard-chatbot/","permalink":"https://feng.lu/2016/09/25/A-chatbot-prototype-DNV-GL-rules-and-standard-chatbot/","excerpt":"(Edited 2017-01-07: The newer version of Chatbot is at here). At the end of March 2016, Microsoft announced the Bot Framework, a platform that helps you to quickly build the high quality bots for your business. In April, after a long weekend happy hacking, I have built a chatbot prototype who can help you to find DNV GL service document.","text":"(Edited 2017-01-07: The newer version of Chatbot is at here). At the end of March 2016, Microsoft announced the Bot Framework, a platform that helps you to quickly build the high quality bots for your business. In April, after a long weekend happy hacking, I have built a chatbot prototype who can help you to find DNV GL service document. You can head to https://dnvgl-rules-bot.azurewebsites.net/ to have a chat with it, or via skype and slack. Here is a live preview: Some facts about it: It understands human language (English), thanks to the integrated Microsoft Language Understanding Intelligent Service. It can be extended to support such as Chinese or Spanish. The business user (such as a Rules and Standard expert) can easily train this bot via the Machine Learning interface, to provide a better intelligence insight. I have only trained it with less than 50 sample inputs. But the more we talk, the more smart it can be. It is hosted in Azure, the Microsoft cloud service. Currently it supports 3 channels: Web Chat, Skype and Slack. All data is from the publicly available information of DNV GL service document. or or DisclaimerThis chatbot prototype is a personal project. It is only based on the public information of the DNV GL documents. It has NO connection with DNV GL AS. All information from the chatbot is presented ‘as-is’. No warranties whatsoever for correctness; completeness or usefulness.","categories":[],"tags":[{"name":"Chatbot","slug":"Chatbot","permalink":"https://feng.lu/tags/Chatbot/"},{"name":"Artificial Intelligence","slug":"Artificial-Intelligence","permalink":"https://feng.lu/tags/Artificial-Intelligence/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://feng.lu/tags/Machine-Learning/"},{"name":"Microsoft language understanding intelligent service","slug":"Microsoft-language-understanding-intelligent-service","permalink":"https://feng.lu/tags/Microsoft-language-understanding-intelligent-service/"},{"name":"Prototype","slug":"Prototype","permalink":"https://feng.lu/tags/Prototype/"}]},{"title":"How to connect VSTS project and Azure subscription","slug":"How-to-connect-VSTS-project-and-Azure-subscription","date":"2016-09-08T18:16:18.000Z","updated":"2025-03-28T09:05:16.741Z","comments":true,"path":"2016/09/08/How-to-connect-VSTS-project-and-Azure-subscription/","permalink":"https://feng.lu/2016/09/08/How-to-connect-VSTS-project-and-Azure-subscription/","excerpt":"IntroductionFor enabling the continues deployment from VSTS to Azure, e.g. provision the environment and deploy code, you must create connection between VSTS project and Azure subscription. It should be done in individual project and not in the VSTS top level. Therefore, each project can connect to different Azure subscription. It contains two main parts, and normally you need both of them Connect Azure Classic (for deploying your application code) Connect Azure Resource Manager (for provisioning your environment)","text":"IntroductionFor enabling the continues deployment from VSTS to Azure, e.g. provision the environment and deploy code, you must create connection between VSTS project and Azure subscription. It should be done in individual project and not in the VSTS top level. Therefore, each project can connect to different Azure subscription. It contains two main parts, and normally you need both of them Connect Azure Classic (for deploying your application code) Connect Azure Resource Manager (for provisioning your environment) How to connect Azure Classic Make sure you are the admin of VSTS project and also the admin of Azure subscription Go to your project management page, such as https://your_instance.visualstudio.com/DefaultCollection/your_project_name/_admin Select “Services” in the tab, then “Azure Classic” Change radio button to “Certificate Based“, then download the setting file by following publish settings file Setting file is downloaded Carefully copy values from that file into the form Done How to connect Azure Resource Manager Make sure you are the admin of VSTS project and also the admin of Azure subscription Go to your project management page, such as https://your_instance.visualstudio.com/DefaultCollection/your_project_name/_admin Select “Services” in the tab, then “Azure Resource Manager” Follow the instruction in the page that link from the picture, to fill up all information that is needed in the form. A key step from the instruction is to run a powershell script. This script can be download from the page. Download and run the script, it will ask for: The name of your Azure Subscription name A password that you would like to set for the Service Principal that is going to be created Copy the name from your Azure subscription, in this case, it is “MSDN Dev&#x2F;Test Pay-As-You-Go” Complete the script execution. It will pop up a window for logging. Log in. Finally you will get values that you can use for the form Copy paste into the form from VSTS project, the click OK Done","categories":[],"tags":[{"name":"Azure","slug":"Azure","permalink":"https://feng.lu/tags/Azure/"},{"name":"VSTS","slug":"VSTS","permalink":"https://feng.lu/tags/VSTS/"},{"name":"Release management","slug":"Release-management","permalink":"https://feng.lu/tags/Release-management/"}]},{"title":"Change Git color","slug":"Change-Git-color","date":"2016-08-27T12:53:04.000Z","updated":"2025-03-28T09:05:16.529Z","comments":true,"path":"2016/08/27/Change-Git-color/","permalink":"https://feng.lu/2016/08/27/Change-Git-color/","excerpt":"Git has a default color schema for showing information. However, sometimes it is difficult to read (especially for color blind people), such as below You can modify the color schema by editing the C:\\Users\\yourname.gitconfig file as below","text":"Git has a default color schema for showing information. However, sometimes it is difficult to read (especially for color blind people), such as below You can modify the color schema by editing the C:\\Users\\yourname.gitconfig file as below 123456[color] ui = true[color &quot;status&quot;] added = yellow changed = green untracked = cyan Now it is much easier to read. Done.","categories":[],"tags":[{"name":"Git","slug":"Git","permalink":"https://feng.lu/tags/Git/"}]},{"title":"How to setup a hexo-based blog: Part 2","slug":"How-to-setup-a-hexo-based-blog-Part-2","date":"2016-08-12T18:01:06.000Z","updated":"2025-03-28T09:05:16.811Z","comments":true,"path":"2016/08/12/How-to-setup-a-hexo-based-blog-Part-2/","permalink":"https://feng.lu/2016/08/12/How-to-setup-a-hexo-based-blog-Part-2/","excerpt":"Previously we have setup and published the hexo-based blog. The article source code and the hexo configuration files are under git version control. Config theme, the wrong way#As most hexo tutorials, the next step is to change the default theme. There are lots of themes that you can choose from. However, most of the theme simply ask you to clone itself under the themes folder. In this case, we will use the popular theme NexT. Its document specified follow below steps: Install 1git clone https://github.com/iissnan/hexo-theme-next themes/next Update the _config.yml to use this theme 1theme: next Pull the theme update when it is needed 12cd theme/nextgit pull The problemFor most of themes (including NexT), we will have to modify the theme’s _config.yml file which is under the theme folder, in order to: change the configuration on the theme level, such change theme style update personal information setup Google Analytic key integrate with 3rd party commenting plugin future customize the theme the way you want Naturally, you would like to have above modifications also under version control, so you check them in and push to Git. Now, if you head to Github source code page, you will find an interesting grayed-out folder named “Next” You also lost the possibility to browse the theme source code there.","text":"Previously we have setup and published the hexo-based blog. The article source code and the hexo configuration files are under git version control. Config theme, the wrong way#As most hexo tutorials, the next step is to change the default theme. There are lots of themes that you can choose from. However, most of the theme simply ask you to clone itself under the themes folder. In this case, we will use the popular theme NexT. Its document specified follow below steps: Install 1git clone https://github.com/iissnan/hexo-theme-next themes/next Update the _config.yml to use this theme 1theme: next Pull the theme update when it is needed 12cd theme/nextgit pull The problemFor most of themes (including NexT), we will have to modify the theme’s _config.yml file which is under the theme folder, in order to: change the configuration on the theme level, such change theme style update personal information setup Google Analytic key integrate with 3rd party commenting plugin future customize the theme the way you want Naturally, you would like to have above modifications also under version control, so you check them in and push to Git. Now, if you head to Github source code page, you will find an interesting grayed-out folder named “Next” You also lost the possibility to browse the theme source code there. Root causeTechnically we created a nested git repository structure: there is a git repository “Next” underneath of git repository “Blog-Hexo”. This git document has a good explanation about this. The right wayAs our customized theme became an independent component component of the blog system, it requires proper version control of it. Fork theme projectBy doing this, you can properly version control your modification of your theme, but still have the possibility to keep it up-to-date. The fork version is https://github.com/linkcd/hexo-theme-next. Create Git submodule which named “next-linkcd” (means it’s our own version of NexT theme), and points to https://github.com/linkcd/hexo-theme-next. 12cd blog-hexogit submodule add https://github.com/linkcd/hexo-theme-next themes/next-linkcd Ask hexo to use our own theme 1theme: next-linkcd Now you have 2 separated repositories, one for your blog-hexo, and one for your own theme. You can run below command to see the relationship. It also works in Github webpage Please note that the “git stamp” is d82e379 for both place. Customize themeNow you are free to modify your theme. To push theme changes back to Github, follow below: Git add, commit and push in next-linkcd folder Git add, commit and push in blog-hexo folder, even you have not modify anything in your blog. Sync between two PC123cd blog-hexogit pullgit submodule update Restore your blog in a new PC Restore source code from Github and install hexo 12345npm install hexo-cli -ggit clone https://github.com/linkcd/blog-hexo.gitcd blog-hexogit submodule update --initnpm install hexo --save The last step is to reinstall hexo program. There is no need to re-install plug-ins. Done","categories":[],"tags":[{"name":"Git","slug":"Git","permalink":"https://feng.lu/tags/Git/"},{"name":"Hexo","slug":"Hexo","permalink":"https://feng.lu/tags/Hexo/"},{"name":"Github","slug":"Github","permalink":"https://feng.lu/tags/Github/"},{"name":"Blog","slug":"Blog","permalink":"https://feng.lu/tags/Blog/"}]},{"title":"How to setup a hexo-based blog: Part 1","slug":"How-to-setup-a-hexo-based-blog-Part-1","date":"2016-08-05T16:59:19.000Z","updated":"2025-03-28T09:05:16.811Z","comments":true,"path":"2016/08/05/How-to-setup-a-hexo-based-blog-Part-1/","permalink":"https://feng.lu/2016/08/05/How-to-setup-a-hexo-based-blog-Part-1/","excerpt":"Hexo is a simple and powerful blog framework that allows you setup your blog quickly and easily. Why Hexo?Instead of create the blog in https://wordpress.com/, I decided to setup and own the whole blog website. You can read a good discussion by Avinash Kaushik. However, own a web site on interent is not easy. Normal maintenance tasks, such as backup database (your content) and apply security patch, are too much to a part-time blogger. Therefore, Hexo became a good solution: Customization: Hexo and its components are open-source. You can customize your blog the way you want. Light-weight: Hexo is a static blog system. It does not require any server-side code, or a database. Git is the best mechanism. Safety: Hexo only publish static files to internet, such as html, css and javascript. It is much less vulnerable compares to any rumtime web application. No compromise on functionality: Even it is a static site, but you can still embed 3rd party services for common blog functionalities such as commenting and web analytic. Cost: Together with Github page, host your blog is free. If you are not a fan of the yourname.github.io domain name, you can also pay a little to have your own domain.","text":"Hexo is a simple and powerful blog framework that allows you setup your blog quickly and easily. Why Hexo?Instead of create the blog in https://wordpress.com/, I decided to setup and own the whole blog website. You can read a good discussion by Avinash Kaushik. However, own a web site on interent is not easy. Normal maintenance tasks, such as backup database (your content) and apply security patch, are too much to a part-time blogger. Therefore, Hexo became a good solution: Customization: Hexo and its components are open-source. You can customize your blog the way you want. Light-weight: Hexo is a static blog system. It does not require any server-side code, or a database. Git is the best mechanism. Safety: Hexo only publish static files to internet, such as html, css and javascript. It is much less vulnerable compares to any rumtime web application. No compromise on functionality: Even it is a static site, but you can still embed 3rd party services for common blog functionalities such as commenting and web analytic. Cost: Together with Github page, host your blog is free. If you are not a fan of the yourname.github.io domain name, you can also pay a little to have your own domain. Setup in nutshellThere are tons of articles on internet show how to setup a Hexo-based blog in Github page. Therefore I will only list necessary steps in below: Installationyou will need to install below applications on your local machine first: Node.js Git Then install Hexo 1npm install hexo-cli -g Create your blogAssuming you will store all your blog files in c:\\private\\blog-hexo. At c:\\private, run below command: 1hexo init blog-hexo Install pluginAt this moment, we need plug-in for feed generation and Git deployment. wWe also install the browser auto fresher for a better writing experience. Inside of the blog folder, run: 123npm install hexo-deployer-git --savenpm install hexo-generator-feed --savenpm install hexo-browsersync --save More plug-in are here. Verify before we move on. In the blog-hexo folder, run: 1hexo s --open Version controlBy now, everything is in your local machine. Let’s use Git and Github to version control it. Create a repo in your Github account, such as https://github.com/linkcd/blog-hexo.git Create a new local Git repo in your blog folder c:\\private\\blog-hexo Push your local Git repo to remote one. Use default branch is fine. If you are using GVIM as me, you can also the last 2 lines in your .gitignore file: 123456789.DS_StoreThumbs.dbdb.json*.lognode_modules/public/.deploy*/*~*.swp Done, from now on you can use the standard Git commands to manage the source code of your articles. You do not need to verson control the public folder, as it only contains the generated html&#x2F;css files. Host in onlineIn order to bring your blog online, we can use Git Pages. Setup Git repositorySimply apply for a special Git repository: .github.io, then turn on the Git Pages feature.Note that you will never need to clone this repository to your local. DeployModify your _config.xml file to specify the git repository address: 1234deploy: type: git repo: github: https://github.com/linkcd/linkcd.github.io.git Then you can run below to publish (as far as you has installed hexo-deployer-git plugin): 1hexo generate --deploy DoneYou can head to https://linkcd.github.io to open your blog. In next article, we will dive into the interesting parts of Hexo: Switching between PC, Theme and Git process.","categories":[],"tags":[{"name":"Git","slug":"Git","permalink":"https://feng.lu/tags/Git/"},{"name":"Hexo","slug":"Hexo","permalink":"https://feng.lu/tags/Hexo/"},{"name":"Github","slug":"Github","permalink":"https://feng.lu/tags/Github/"},{"name":"Blog","slug":"Blog","permalink":"https://feng.lu/tags/Blog/"}]}],"categories":[],"tags":[{"name":"AI","slug":"AI","permalink":"https://feng.lu/tags/AI/"},{"name":"AI Agent","slug":"AI-Agent","permalink":"https://feng.lu/tags/AI-Agent/"},{"name":"Generative AI","slug":"Generative-AI","permalink":"https://feng.lu/tags/Generative-AI/"},{"name":"GenAI","slug":"GenAI","permalink":"https://feng.lu/tags/GenAI/"},{"name":"MCP","slug":"MCP","permalink":"https://feng.lu/tags/MCP/"},{"name":"Model Context Protocol","slug":"Model-Context-Protocol","permalink":"https://feng.lu/tags/Model-Context-Protocol/"},{"name":"Tibber","slug":"Tibber","permalink":"https://feng.lu/tags/Tibber/"},{"name":"Agent","slug":"Agent","permalink":"https://feng.lu/tags/Agent/"},{"name":"LangGraph","slug":"LangGraph","permalink":"https://feng.lu/tags/LangGraph/"},{"name":"CrewAI","slug":"CrewAI","permalink":"https://feng.lu/tags/CrewAI/"},{"name":"OpenAI","slug":"OpenAI","permalink":"https://feng.lu/tags/OpenAI/"},{"name":"Llama","slug":"Llama","permalink":"https://feng.lu/tags/Llama/"},{"name":"Claude","slug":"Claude","permalink":"https://feng.lu/tags/Claude/"},{"name":"LLM","slug":"LLM","permalink":"https://feng.lu/tags/LLM/"},{"name":"Game","slug":"Game","permalink":"https://feng.lu/tags/Game/"},{"name":"AWS","slug":"AWS","permalink":"https://feng.lu/tags/AWS/"},{"name":"Azure","slug":"Azure","permalink":"https://feng.lu/tags/Azure/"},{"name":"Security","slug":"Security","permalink":"https://feng.lu/tags/Security/"},{"name":"Idp","slug":"Idp","permalink":"https://feng.lu/tags/Idp/"},{"name":"IAM","slug":"IAM","permalink":"https://feng.lu/tags/IAM/"},{"name":"Managed Identity","slug":"Managed-Identity","permalink":"https://feng.lu/tags/Managed-Identity/"},{"name":"Entra ID","slug":"Entra-ID","permalink":"https://feng.lu/tags/Entra-ID/"},{"name":"Amazon Bedrock","slug":"Amazon-Bedrock","permalink":"https://feng.lu/tags/Amazon-Bedrock/"},{"name":"AWS Step Functions","slug":"AWS-Step-Functions","permalink":"https://feng.lu/tags/AWS-Step-Functions/"},{"name":"IoT","slug":"IoT","permalink":"https://feng.lu/tags/IoT/"},{"name":"Raspberry Pi","slug":"Raspberry-Pi","permalink":"https://feng.lu/tags/Raspberry-Pi/"},{"name":"AWS IoT Core","slug":"AWS-IoT-Core","permalink":"https://feng.lu/tags/AWS-IoT-Core/"},{"name":"AWS IoT Greengrass","slug":"AWS-IoT-Greengrass","permalink":"https://feng.lu/tags/AWS-IoT-Greengrass/"},{"name":"Device Shadow","slug":"Device-Shadow","permalink":"https://feng.lu/tags/Device-Shadow/"},{"name":"Step Functions","slug":"Step-Functions","permalink":"https://feng.lu/tags/Step-Functions/"},{"name":"ECS Anywhere","slug":"ECS-Anywhere","permalink":"https://feng.lu/tags/ECS-Anywhere/"},{"name":"Nano Pi","slug":"Nano-Pi","permalink":"https://feng.lu/tags/Nano-Pi/"},{"name":"AWS Systems Manager","slug":"AWS-Systems-Manager","permalink":"https://feng.lu/tags/AWS-Systems-Manager/"},{"name":"SSM","slug":"SSM","permalink":"https://feng.lu/tags/SSM/"},{"name":"EC2","slug":"EC2","permalink":"https://feng.lu/tags/EC2/"},{"name":"VPC","slug":"VPC","permalink":"https://feng.lu/tags/VPC/"},{"name":"VPC Endpoint","slug":"VPC-Endpoint","permalink":"https://feng.lu/tags/VPC-Endpoint/"},{"name":"SSH","slug":"SSH","permalink":"https://feng.lu/tags/SSH/"},{"name":"e-paper","slug":"e-paper","permalink":"https://feng.lu/tags/e-paper/"},{"name":"DIY","slug":"DIY","permalink":"https://feng.lu/tags/DIY/"},{"name":"SVMP","slug":"SVMP","permalink":"https://feng.lu/tags/SVMP/"},{"name":"Amazon Lookout for Vision","slug":"Amazon-Lookout-for-Vision","permalink":"https://feng.lu/tags/Amazon-Lookout-for-Vision/"},{"name":"Movie","slug":"Movie","permalink":"https://feng.lu/tags/Movie/"},{"name":"Okta","slug":"Okta","permalink":"https://feng.lu/tags/Okta/"},{"name":"AWS Control Tower","slug":"AWS-Control-Tower","permalink":"https://feng.lu/tags/AWS-Control-Tower/"},{"name":"AWS SSO","slug":"AWS-SSO","permalink":"https://feng.lu/tags/AWS-SSO/"},{"name":"SSO","slug":"SSO","permalink":"https://feng.lu/tags/SSO/"},{"name":"PowerBI","slug":"PowerBI","permalink":"https://feng.lu/tags/PowerBI/"},{"name":"Data Analytics","slug":"Data-Analytics","permalink":"https://feng.lu/tags/Data-Analytics/"},{"name":"Python","slug":"Python","permalink":"https://feng.lu/tags/Python/"},{"name":"Jupyter Notebook","slug":"Jupyter-Notebook","permalink":"https://feng.lu/tags/Jupyter-Notebook/"},{"name":"Visualization","slug":"Visualization","permalink":"https://feng.lu/tags/Visualization/"},{"name":"Car","slug":"Car","permalink":"https://feng.lu/tags/Car/"},{"name":"Databricks","slug":"Databricks","permalink":"https://feng.lu/tags/Databricks/"},{"name":"Time Series Insight (TSI)","slug":"Time-Series-Insight-TSI","permalink":"https://feng.lu/tags/Time-Series-Insight-TSI/"},{"name":"Azure Data Explorer (ADX)","slug":"Azure-Data-Explorer-ADX","permalink":"https://feng.lu/tags/Azure-Data-Explorer-ADX/"},{"name":"Grafana","slug":"Grafana","permalink":"https://feng.lu/tags/Grafana/"},{"name":"Azure IoT Hub","slug":"Azure-IoT-Hub","permalink":"https://feng.lu/tags/Azure-IoT-Hub/"},{"name":"Azure IoT Edge","slug":"Azure-IoT-Edge","permalink":"https://feng.lu/tags/Azure-IoT-Edge/"},{"name":"GPS","slug":"GPS","permalink":"https://feng.lu/tags/GPS/"},{"name":"OBD2","slug":"OBD2","permalink":"https://feng.lu/tags/OBD2/"},{"name":"Hamster","slug":"Hamster","permalink":"https://feng.lu/tags/Hamster/"},{"name":"Home Assistant","slug":"Home-Assistant","permalink":"https://feng.lu/tags/Home-Assistant/"},{"name":"Zigbee","slug":"Zigbee","permalink":"https://feng.lu/tags/Zigbee/"},{"name":"Powershell","slug":"Powershell","permalink":"https://feng.lu/tags/Powershell/"},{"name":"Encoding","slug":"Encoding","permalink":"https://feng.lu/tags/Encoding/"},{"name":"Json","slug":"Json","permalink":"https://feng.lu/tags/Json/"},{"name":"Troubleshooting","slug":"Troubleshooting","permalink":"https://feng.lu/tags/Troubleshooting/"},{"name":"Fiddler","slug":"Fiddler","permalink":"https://feng.lu/tags/Fiddler/"},{"name":"Native App","slug":"Native-App","permalink":"https://feng.lu/tags/Native-App/"},{"name":"HTTPS","slug":"HTTPS","permalink":"https://feng.lu/tags/HTTPS/"},{"name":"Decryption","slug":"Decryption","permalink":"https://feng.lu/tags/Decryption/"},{"name":"Docker","slug":"Docker","permalink":"https://feng.lu/tags/Docker/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://feng.lu/tags/Kubernetes/"},{"name":"Istio","slug":"Istio","permalink":"https://feng.lu/tags/Istio/"},{"name":"Service mesh","slug":"Service-mesh","permalink":"https://feng.lu/tags/Service-mesh/"},{"name":"Kiali","slug":"Kiali","permalink":"https://feng.lu/tags/Kiali/"},{"name":"Jump-start","slug":"Jump-start","permalink":"https://feng.lu/tags/Jump-start/"},{"name":"Linkedin","slug":"Linkedin","permalink":"https://feng.lu/tags/Linkedin/"},{"name":"Social Sharing","slug":"Social-Sharing","permalink":"https://feng.lu/tags/Social-Sharing/"},{"name":"API","slug":"API","permalink":"https://feng.lu/tags/API/"},{"name":"OAuth","slug":"OAuth","permalink":"https://feng.lu/tags/OAuth/"},{"name":"Postman","slug":"Postman","permalink":"https://feng.lu/tags/Postman/"},{"name":"IOTA","slug":"IOTA","permalink":"https://feng.lu/tags/IOTA/"},{"name":"Data Lineage","slug":"Data-Lineage","permalink":"https://feng.lu/tags/Data-Lineage/"},{"name":"Data Integrity","slug":"Data-Integrity","permalink":"https://feng.lu/tags/Data-Integrity/"},{"name":"DLT","slug":"DLT","permalink":"https://feng.lu/tags/DLT/"},{"name":"Distributed Ledger Technology","slug":"Distributed-Ledger-Technology","permalink":"https://feng.lu/tags/Distributed-Ledger-Technology/"},{"name":"Tangle","slug":"Tangle","permalink":"https://feng.lu/tags/Tangle/"},{"name":"Blog","slug":"Blog","permalink":"https://feng.lu/tags/Blog/"},{"name":"Domain","slug":"Domain","permalink":"https://feng.lu/tags/Domain/"},{"name":"DNS","slug":"DNS","permalink":"https://feng.lu/tags/DNS/"},{"name":"Blockchain","slug":"Blockchain","permalink":"https://feng.lu/tags/Blockchain/"},{"name":"Fullnode","slug":"Fullnode","permalink":"https://feng.lu/tags/Fullnode/"},{"name":"DevOps","slug":"DevOps","permalink":"https://feng.lu/tags/DevOps/"},{"name":"VSTS","slug":"VSTS","permalink":"https://feng.lu/tags/VSTS/"},{"name":"Azure Template","slug":"Azure-Template","permalink":"https://feng.lu/tags/Azure-Template/"},{"name":"Infrastructure as Code","slug":"Infrastructure-as-Code","permalink":"https://feng.lu/tags/Infrastructure-as-Code/"},{"name":"Continuous Integration","slug":"Continuous-Integration","permalink":"https://feng.lu/tags/Continuous-Integration/"},{"name":"Continuous Deployment","slug":"Continuous-Deployment","permalink":"https://feng.lu/tags/Continuous-Deployment/"},{"name":"Release Management","slug":"Release-Management","permalink":"https://feng.lu/tags/Release-Management/"},{"name":"Build","slug":"Build","permalink":"https://feng.lu/tags/Build/"},{"name":"OpenID","slug":"OpenID","permalink":"https://feng.lu/tags/OpenID/"},{"name":"Azure AD B2C","slug":"Azure-AD-B2C","permalink":"https://feng.lu/tags/Azure-AD-B2C/"},{"name":"Nodejs","slug":"Nodejs","permalink":"https://feng.lu/tags/Nodejs/"},{"name":"Authentication","slug":"Authentication","permalink":"https://feng.lu/tags/Authentication/"},{"name":"Authorization","slug":"Authorization","permalink":"https://feng.lu/tags/Authorization/"},{"name":"Scripting","slug":"Scripting","permalink":"https://feng.lu/tags/Scripting/"},{"name":"Google Tag Manager","slug":"Google-Tag-Manager","permalink":"https://feng.lu/tags/Google-Tag-Manager/"},{"name":"Google Analytics","slug":"Google-Analytics","permalink":"https://feng.lu/tags/Google-Analytics/"},{"name":"Vim","slug":"Vim","permalink":"https://feng.lu/tags/Vim/"},{"name":"Plugin","slug":"Plugin","permalink":"https://feng.lu/tags/Plugin/"},{"name":"ASP.Net Core","slug":"ASP-Net-Core","permalink":"https://feng.lu/tags/ASP-Net-Core/"},{"name":"Monitoring","slug":"Monitoring","permalink":"https://feng.lu/tags/Monitoring/"},{"name":"KUDU","slug":"KUDU","permalink":"https://feng.lu/tags/KUDU/"},{"name":"Chatbot","slug":"Chatbot","permalink":"https://feng.lu/tags/Chatbot/"},{"name":"Artificial Intelligence","slug":"Artificial-Intelligence","permalink":"https://feng.lu/tags/Artificial-Intelligence/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://feng.lu/tags/Machine-Learning/"},{"name":"Microsoft language understanding intelligent service","slug":"Microsoft-language-understanding-intelligent-service","permalink":"https://feng.lu/tags/Microsoft-language-understanding-intelligent-service/"},{"name":"Prototype","slug":"Prototype","permalink":"https://feng.lu/tags/Prototype/"},{"name":"SharePoint PnP","slug":"SharePoint-PnP","permalink":"https://feng.lu/tags/SharePoint-PnP/"},{"name":"Application Insights","slug":"Application-Insights","permalink":"https://feng.lu/tags/Application-Insights/"},{"name":"ADFS","slug":"ADFS","permalink":"https://feng.lu/tags/ADFS/"},{"name":"Web Testing","slug":"Web-Testing","permalink":"https://feng.lu/tags/Web-Testing/"},{"name":"Load Testing","slug":"Load-Testing","permalink":"https://feng.lu/tags/Load-Testing/"},{"name":"Release management","slug":"Release-management","permalink":"https://feng.lu/tags/Release-management/"},{"name":"Git","slug":"Git","permalink":"https://feng.lu/tags/Git/"},{"name":"Hexo","slug":"Hexo","permalink":"https://feng.lu/tags/Hexo/"},{"name":"Github","slug":"Github","permalink":"https://feng.lu/tags/Github/"}]}